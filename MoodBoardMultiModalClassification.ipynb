{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RruiW65e7eR"
      },
      "outputs": [],
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1Fqd6UDB02avLsg5g9nZ5AUGzo8bysQHP\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"hateful_memes.zip\" -d ."
      ],
      "metadata": {
        "id": "8S9pIC0-f7fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from transformers import AutoModel, AutoTokenizer\n"
      ],
      "metadata": {
        "id": "XIHUXfB_f_CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARTEMIS_EMOTIONS = ['amusement', 'awe', 'contentment', 'excitement',\n",
        "                    'anger', 'disgust',  'fear', 'sadness', 'something else']\n",
        "\n",
        "EMOTION_TO_IDX = {e: i for i, e in enumerate(ARTEMIS_EMOTIONS)}\n",
        "\n",
        "\n",
        "IDX_TO_EMOTION = {EMOTION_TO_IDX[e]: e for e in EMOTION_TO_IDX}"
      ],
      "metadata": {
        "id": "E8--mohrB1T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoodBoardDataset(Dataset):\n",
        "    def __init__(self, jsonl_file, img_dir, transform=None):\n",
        "        self.img_annotations = self._load_annotations(jsonl_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def _load_annotations(self, jsonl_file):\n",
        "        with open(jsonl_file, 'r') as file:\n",
        "            img_annots = [json.loads(line) for line in file]\n",
        "        return img_annots\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_annots)\n",
        "\n",
        "    def __getitem__(self, x):\n",
        "        img_info = self.img_annots[x]\n",
        "        img_path = os.path.join(self.img_dir, img_info['img'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        emotion = img_info['emotion']\n",
        "        caption = img_info['caption']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        sample = {\"image\": image, \"emotion\": emotion, \"caption\": caption}\n",
        "        return sample"
      ],
      "metadata": {
        "id": "hUTfdhFogAzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"ayoubkirouane/BERT-Emotions-Classifier\")\n",
        "def data_collate_fn(batch):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    images = [transform(sample['image']) for sample in batch]\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    label_mapping = {\n",
        "        \"amusement\": 0,\n",
        "        \"awe\": 1,\n",
        "        \"contentment\": 2,\n",
        "        \"excitement\": 3,\n",
        "        \"anger\": 4,\n",
        "        \"disgust\": 5,\n",
        "        \"fear\": 6,\n",
        "        \"sadness\": 7,\n",
        "        \"something else\":8\n",
        "    }\n",
        "    labels = [label_mapping[sample['emotion']] for sample in batch]\n",
        "    labels = torch.tensor(emotion, dtype=torch.long).detach()\n",
        "\n",
        "    texts = [sample[\"text\"] for sample in batch]\n",
        "\n",
        "    # Tokenized in the collate fn for convenience when used in model, directly passed into forward\n",
        "    tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    return {\"images\": image, \"input_ids\": tokenized_texts['input_ids'], \"labels\": labels}"
      ],
      "metadata": {
        "id": "WpFQSX0fgExD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MoodBoardDataset(jsonl_file='data/train.jsonl', img_dir='data/')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "9Wa8RIV0gL3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = MoodBoardDataset(jsonl_file='data/val.jsonl', img_dir='data/')\n",
        "val_dataloader = DataLoader(dev_dataset, batch_size=10, shuffle=True, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "dJoUDvZngMXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BTruJ5O79eDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalEarlyFusionBertModel(BertModel):\n",
        "    def __init__(self, config, num_classes, cnn_model):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.cnn_model = resnet_model\n",
        "        # Created f_cnn because we want to run the images through the layers before linear layers\n",
        "        self.f_cnn = torch.nn.Sequential(*list(self.cnn_model.children())[:-2])\n",
        "        self.image_layer= nn.Linear(512, config.hidden_size)\n",
        "\n",
        "        self.bert_model = AutoModel.from_pretrained(\"ayoubkirouane/BERT-Emotions-Classifier\", output_hidden_states=True)\n",
        "\n",
        "        # self.embedding_model = torch.nn.Sequential(bert_model.embeddings)\n",
        "        self.modality_embedding = nn.Embedding(num_classes, config.hidden_size)\n",
        "        self.fusion_linear = nn.Linear(config.hidden_size, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, images=None, labels=None):\n",
        "\n",
        "        image_embeddings = self.f_cnn(image)\n",
        "        flat_image_embeddings = image_embeddings.view(image_embeddings.size(0), -1, image_embeddings.size(1))\n",
        "\n",
        "        # Take image embeddings before the linear layer B, 7*7, 2000 and then flatten to 3d\n",
        "        # with torch.no_grad():\n",
        "        #   text_embeddings = self.embedding_model(input_ids)\n",
        "\n",
        "        text_outputs = self.bert_model(**input_ids)\n",
        "\n",
        "        # Hidden states are in the third element of the outputs tuple\n",
        "        text_embeddings = outputs.hidden_states[-2]\n",
        "\n",
        "        batch_size = image_embeddings.size(0)\n",
        "\n",
        "        # Reshaped to make it compatible with our text embeddings\n",
        "        reshaped_image_embedding = self.image_layer(flat_image_embeddings)\n",
        "\n",
        "        image_modality_embeddings = self.modality_embedding(torch.zeros_like(reshaped_image_embedding[:, :, 0]).long().to(image[0].device))\n",
        "        text_modality_embeddings = self.modality_embedding(torch.zeros_like(text_embeddings[:, :, 0]).long().to(input_ids[0].device))\n",
        "\n",
        "        # FUSE image embeddings with modality embeddings, same for text\n",
        "\n",
        "        fused_image_embeddings = reshaped_image_embedding + image_modality_embeddings\n",
        "        text_size = text_embeddings.size(0)\n",
        "\n",
        "        fused_text_embeddings = text_modality_embeddings + text_embeddings\n",
        "\n",
        "        fused_image_embeddings = torch.reshape(fused_image_embeddings, (batch_size, -1, 768))\n",
        "        fused_text_embeddings = torch.reshape(fused_text_embeddings, (batch_size, -1, 768))\n",
        "\n",
        "        fused_embeddings = torch.cat((fused_image_embeddings, fused_text_embeddings), dim=1)\n",
        "\n",
        "        pooled_embeddings, _ = torch.max(fused_embeddings, dim=1)\n",
        "        # Makes it 2Dim gets rid of second dim (95)\n",
        "        logits = self.fusion_linear(pooled_embeddings)\n",
        "        # aggregated_logits = torch.mean(logits, dim=1)\n",
        "\n",
        "        outputs = {'logits': logits}\n",
        "        if label is not None:\n",
        "          loss_fct = nn.CrossEntropyLoss()\n",
        "          loss = loss_fct(logits,label.view(-1))\n",
        "          outputs ['loss'] = loss\n",
        "        return outputs\n",
        "\n",
        "        # return(logits)\n",
        "\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "resnet_model = torchvision.models.resnet18(pretrained=True)\n",
        "model = MultiModalEarlyFusionBertModel(config, num_classes=9, cnn_model=resnet_model)"
      ],
      "metadata": {
        "id": "UlCgmJuegfy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    accuracy = accuracy_score(p.label_ids, preds)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    return {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "hqob-r_KB4og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=5e-6,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    output_dir='/results',\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[\"label\"]\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=training_args.learning_rate)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=500)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    train_dataset=MoodBoardDataset(jsonl_file='data/train.jsonl', img_dir='data/'),  # Specify your training dataset\n",
        "    eval_dataset=MoodBoardDataset(jsonl_file='data/dev.jsonl', img_dir='data/'),    # Specify your validation dataset\n",
        "    # optimizers=(optimizer, scheduler),\n",
        "    data_collator=data_collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "Km3WdXcMGxW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}