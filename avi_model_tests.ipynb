{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e8vQOAww_hN",
    "outputId": "c315719e-ecdb-4e4a-ca16-837c1ea680ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.3)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmbKjfWUuIkO",
    "outputId": "624dec8d-b88a-43d5-f6fb-d251066134e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username: avitakku\n",
      "Your Kaggle Key: ··········\n",
      "Downloading wikiart25k.zip to ./wikiart25k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.74G/7.74G [00:31<00:00, 267MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "dataset_url = 'https://www.kaggle.com/datasets/trungit/wikiart25k'\n",
    "od.download(dataset_url, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fECgKHke_O_9",
    "outputId": "fe2e21b8-3ce7-49c0-df77-cccb9e84a2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optim\n",
      "  Downloading optim-0.1.0.tar.gz (4.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: optim\n",
      "  Building wheel for optim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for optim: filename=optim-0.1.0-py2.py3-none-any.whl size=2705 sha256=dcf512a3684d4e9b49500a039ced60ccdd5bd32c4b9cfc6eca35a8eb6712902c\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/cd/16/e7762fdd7862a4f618fa7ca62119fac2112de90041cee77227\n",
      "Successfully built optim\n",
      "Installing collected packages: optim\n",
      "Successfully installed optim-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "66Mc3VMIIyBL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from transformers import AutoTokenizer, BertModel, BertConfig, BertTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeuOy2nMHNcq"
   },
   "source": [
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9qUkXI4HRKe"
   },
   "source": [
    "When we downloaded the dataset off of Kaggle, a lot of the filenames downloaded with non ascii characters. I was unable to clean these effectively, so I decided to delete the files entirely as the dataset was big enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UdmERfIzBLHB",
    "outputId": "23b30a3e-b0c2-45f2-f493-a196b48164c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total deleted files: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to delete incorrectly encoded files\n",
    "\n",
    "def delete_non_ascii_filenames(directory):\n",
    "    count = 0\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            try:\n",
    "                file.encode('ascii')\n",
    "            except UnicodeEncodeError:  # Filename is non-ASCII\n",
    "                full_path = os.path.join(subdir, file)\n",
    "                os.remove(full_path)  # Delete the file\n",
    "                count += 1\n",
    "                print(f\"Deleted file: {full_path}\")\n",
    "    return count\n",
    "\n",
    "# Use the function on your dataset directory and delete non-ASCII files\n",
    "deleted_count = delete_non_ascii_filenames('wikiart25k')\n",
    "print(f\"Total deleted files: {deleted_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV7CKqxWI01R"
   },
   "source": [
    "Here I am creating a new dataframe that doesn't include the images that I deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDmHE-M40iXP"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('drive/MyDrive/Applied_CV/valid_images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i3Bw3HECIvf"
   },
   "outputs": [],
   "source": [
    "# Initializing new dataframe\n",
    "trimmed_valid_images_df = pd.DataFrame(columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tbuCAKQjCamV"
   },
   "outputs": [],
   "source": [
    "def get_image_path(art_style, painting, top_dir='wikiart25k'):\n",
    "    path = f\"{top_dir}/{art_style}/{painting}.jpg\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8B9MBZpUCOTR",
    "outputId": "a5d81f4b-28ad-4c7e-82d5-4e75a7515708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "odilon-redon_the-mask-of-the-red-death-1883\n",
      "Symbolism\n",
      "... (output truncated)"
     ]
    }
   ],
   "source": [
    "# Go through each row in the dataset\n",
    "valid_rows = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    art_style = row['art_style']\n",
    "    print(art_style)\n",
    "    painting = row['painting']\n",
    "    print(painting)\n",
    "    image_path = get_image_path(art_style, painting)\n",
    "\n",
    "    # Check if the image exists in archive\n",
    "    # If image found, save the row to list\n",
    "    try:\n",
    "        with Image.open(image_path):\n",
    "            valid_rows.append(row)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# Create a new dataframe with the valid rows\n",
    "trimmed_valid_images_df = pd.DataFrame(valid_rows, columns=df.columns)\n",
    "\n",
    "# Save the new dataframe to a csv file\n",
    "trimmed_valid_images_df.to_csv('drive/MyDrive/Applied_CV/trimmed_valid_images.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d_ZGSmWsEEHQ"
   },
   "outputs": [],
   "source": [
    "model_df = pd.read_csv('drive/MyDrive/Applied_CV/trimmed_valid_images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JnY9rDFS011L"
   },
   "outputs": [],
   "source": [
    "def get_image_path(art_style, painting, top_dir='wikiart25k'):\n",
    "    path = f\"{top_dir}/{art_style}/{painting}.jpg\"\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-w5YIEi2Ky0",
    "outputId": "ede39502-3d34-451a-bcb7-0e1a627f17a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art_style                                         Impressionism\n",
      "painting                          willard-metcalf_havana-harbor\n",
      "emotion                                             contentment\n",
      "utterance     The red of the flowers pop off the page, it is...\n",
      "repetition                                                    7\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(model_df.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwxCRZyBJkfv"
   },
   "source": [
    "## Creating Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0wJq-_j2NO7"
   },
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, transform=None):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = get_image_path(self.df.iloc[idx]['art_style'], self.df.iloc[idx]['painting'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = self.df.iloc[idx]['utterance']\n",
    "\n",
    "        return {'image': image, 'caption': caption}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUDV8jn22Oxd",
    "outputId": "b236f6ab-c0df-4ec1-87bc-7f6d772fa548"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XcD5VMz2SGT"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = [item['image'] for item in batch]\n",
    "    images_stacked = torch.stack(images)\n",
    "\n",
    "    captions = [item['caption'] for item in batch]\n",
    "    captions_tokenized = tokenizer(\n",
    "        captions,\n",
    "        padding=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return {'images': images_stacked, 'captions': captions_tokenized['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nxOy0G762USJ"
   },
   "outputs": [],
   "source": [
    "# Separate the dataset into training, testing, and validation sets\n",
    "\n",
    "train_val_df, test_df = train_test_split(model_df, test_size=0.2)\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJSZUfPLJryv"
   },
   "source": [
    "### Inspecting the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 3 randomly chosen images using matplotlib.  Consider the colormap so\n",
    "\n",
    "image1 = Image.open(get_image_path(model_df.iloc[0]['art_style'], model_df.iloc[0]['painting'])).convert('RGB')\n",
    "image2 = Image.open(get_image_path(model_df.iloc[10]['art_style'], model_df.iloc[10]['painting'])).convert('RGB')\n",
    "image3 = Image.open(get_image_path(model_df.iloc[200]['art_style'], model_df.iloc[200]['painting'])).convert('RGB')\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "ax1.imshow(image1, cmap='gray')\n",
    "ax1.set_title('Image 1')\n",
    "ax2.imshow(image2, cmap='gray')\n",
    "ax2.set_title('Image 2')\n",
    "ax3.imshow(image3, cmap='gray')\n",
    "ax3.set_title('Image 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PdZ6wxmn2ZJf"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Match the expected input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6InheyNnxesB"
   },
   "source": [
    "### Creating the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3UfyWVm2ahO"
   },
   "outputs": [],
   "source": [
    "train_dataset = CaptionDataset(train_df, tokenizer, transform)\n",
    "val_dataset = CaptionDataset(val_df, tokenizer, transform)\n",
    "test_dataset = CaptionDataset(test_df, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLJulh-0KBeX"
   },
   "source": [
    "## Model 1: CNN and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APZ2yi3L-z98"
   },
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnEjEB_VKD8T"
   },
   "source": [
    "I am using a CNN to extract features from images (ResNet50) and then using an LSTM to generate captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aYgTaaj2wbv"
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "\n",
    "        # Load pretrained resnet model\n",
    "        resnet = resnet50(pretrained=True)\n",
    "\n",
    "        # Freeze the resnet layers so that they are not trained\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove the classification layer and replace it with a linear layer\n",
    "        # This reduces the output dimension to match the embedding dimension\n",
    "        resnet.fc = nn.Linear(resnet.fc.in_features, embed_dim)\n",
    "        self.cnn = resnet\n",
    "\n",
    "        # LSTM for generating captions\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=0.5 if num_layers > 1 else 0)\n",
    "\n",
    "        # Output layer that maps the hidden state output dimension to the vocab size\n",
    "        # This helps to predict the next word in the caption\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "\n",
    "        # Extract features from images\n",
    "        image_features = self.cnn(images)\n",
    "\n",
    "        # Unsqueeze the dimensions of the features tensor\n",
    "        # Repeat it so that the same features are used for each word in the caption\n",
    "        features = image_features.unsqueeze(1).repeat(1, captions.size(1), 1)\n",
    "\n",
    "        # Get output from the LSTM\n",
    "        # A new hidden state is generated for each word in the caption\n",
    "        lstm_output, _ = self.lstm(features)\n",
    "\n",
    "        # Linear layer takes output and predicts the next word in the caption\n",
    "        outputs = self.linear(lstm_output)\n",
    "\n",
    "        print(\"Sample outputs:\", outputs[0, :5])\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0uCNdbIH2y8O"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTFxa4SU23XG",
    "outputId": "88c4e2eb-4718-4bc0-89a1-e3b2daab6e58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "image_to_caption_model = ImageCaptioningModel(embed_dim=256, hidden_dim=512, vocab_size=len(tokenizer.vocab), num_layers=1)\n",
    "model =image_to_caption_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2vGvLg3-23d"
   },
   "source": [
    "### Researching Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzER47W3KQS0"
   },
   "source": [
    "To determine what a good learning rate would be, I researched a method that would help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "C1axzqtx2aTD",
    "outputId": "ba3de8ba-9c43-4fe2-fd2d-2a67f48fe347"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTGElEQVR4nO3deXgTdf4H8PckTdMz6X2fFCg3clYuBUUBEfECRRS8EFdcReW3LuvFigiei7qKIiygooi4KrsiCAou991SrnKV3qX33aRNMr8/0kYqbemRdGbS9+t55rFJJsknY2nf/Z6CKIoiiIiIiBRIJXUBRERERG3FIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrlInUBjmaxWJCdnQ1vb28IgiB1OURERNQCoiiivLwcYWFhUKmabndx+iCTnZ2NyMhIqcsgIiKiNsjIyEBERESTjzt9kPH29gZgvRA6nU7iaoiIiKglysrKEBkZafs93hSnDzL13Uk6nY5BhoiISGGuNiyEg32JiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixnH7TSEcpqqxBpdEEtUqAWiVAJVj/qxYEqFRo5L7mN70iIiKi1mOQaaO3f07Bl/vTW3y+IMAWaFwuCzcuqsv+Wxd8GrvPdvzxdt05LioBLmoBrmoVXF1U0Kith6uLCq5qwfa1Rq2CxkUFrVoFjUvd/WoV3DRqeLiqbf/1cHWBu6v1a42aDXdERCRPDDJt5KISoHVRwSKKMFtEWMTmzxdFwCSKgEVETceUaDcatQB3jTXceLiqbQHH3dUF7hrV76FHo4aH1gU6Nxfo3DXQ/+Hw83SFm0Yt9cchIiInIoiieJVfwcpWVlYGvV6P0tJS6HQ6h72PKFrDjDXUWMONWRRhsfz+tbnua4sFMFksdefVfW1B3TkWmC2wnVv/Gqb654rWr22vW3eOySLCbLagxmxBrVmE0WRBrdmCWlP9fRbUmETr1w3us35tqLWgusaEqhozqmvMqKo1w3y1dNYG3loXBOq0CPTSIkjnVvdf6+1A79+/9vVwZXccEVEn1tLf32yRsRNBEKAWrGNjnIEoWkNPdY0ZVXWH9WsTqmrNtvura80NAlBljQll1SaUVteitLoWZXX/La2uhckiotxoQnm+CRfyK5t9fxeVgIC6kBPp54EYfw/E+HsiNsAT0f6eCPByhSA4x7UmIqK2Y5ChRgmCAK2LGloXNXw82v96omgNMXllRuSXG5FXbkB+udF25NV/XWFEUWUNTBYRuWUG5JYZcCyz9IrX89a6IDrAGm5i/D0RE+CJ+GBvxId4w9WFY3qIiDoLBhnqEIIgQOemgc5Ng65BXs2eW2OyoLDSiLwyI3LLDMgoqkJqQSXSCq3/zS6tRrnRhONZZTieVdbgua4uKvQM1aF/hB79InzQJ1yH2ABPaF04NoeIyBlxjAwpjqHWjMziKqQWVOFiQSUuFlYitaASJ7LLUFpde8X5KgHoGuSFQdF+GBzti6GxfojwdWfXFBGRjLX09zeDDDkNURSRXlSFpMxSJGWU4FhmCU7nlqPcYLri3BCdG4bF+WNsz2AMifVFkLebBBUTEVFTGGTqMMh0bqIoIq/ciKSMEhxOK8bBi0VIzipFrbnht324jzuGxPji1n5huK57IMfZEBFJjEGmDoMM/VF1jRlHM4qx/XQefjuTj7N5Fbj8X4GvhwYPDIvBvUMiEebjLl2hRESdGINMHQYZuppyQy2OZZbi19N52JiUjfxyIwDrasz3DI7ErOu6IC6w+QHKRERkXy39/S1p+/n//vc/TJo0CWFhYRAEAd9//32Dx0VRxMsvv4zQ0FC4u7tj7NixOHv2rDTFktPydtNgRNcAvHRrL+ybfyM+vG8ghsb4QRSBdQczcOM7v+Gv3x5DcaXS1mQmInJ+kgaZyspK9O/fHx9++GGjj7/55pt4//338fHHH2P//v3w9PTEuHHjYDAYOrhS6izUKgET+4Vi/ePDsH72MNzQIwiANdCMffc3rN2fhlqzReIqiYionmy6lgRBwHfffYfbb78dgLU1JiwsDM899xzmzZsHACgtLUVwcDBWr16Ne++9t0Wvy64laq+DF4vwwnfJOHOpAgAwJMYXH00fhEBvrcSVERE5L0V0LTUnNTUVubm5GDt2rO0+vV6PhIQE7N27t8nnGY1GlJWVNTiI2mNIjB/+++dRePnWXvDWuuDgxWLc9s9dSG5kxWEiIupYsg0yubm5AIDg4OAG9wcHB9sea8zixYuh1+ttR2RkpEPrpM7B1UWFh0fG4vsnR6BLgCdySg24++M9+HzvRcikUZOIqFOSbZBpq/nz56O0tNR2ZGRkSF0SOZG4QC98N2cExsQHwmiy4KUfTuCVjSc4boaISCKyDTIhISEAgEuXLjW4/9KlS7bHGqPVaqHT6RocRPakd9dg5cwh+NstPSAIwGd70zBj5QFUGq9cQZiIiBxLtkEmNjYWISEh+OWXX2z3lZWVYf/+/Rg2bJiElREBKpWAx66Lw7LpA+GldcHeC4V4cNUBlBuu3OuJiIgcR9IgU1FRgcTERCQmJgKwDvBNTExEeno6BEHA3Llz8dprr2Hjxo1ITk7GjBkzEBYWZpvZRCS18X1C8fkjQ+HtZh0E/MDKAyhjmCEi6jCSTr/esWMHxowZc8X9M2fOxOrVqyGKIl555RUsX74cJSUlGDlyJD766CN07969xe/B6dfUEY5nleL+lftRUlWLMfGBWHb/ILhp1FKXRUSkWNyioA6DDHWUxIwS3L1sD0wWEdd3D8TyGYOgdWGYISJqC8WvI0OkNNdE+mDFzMFw16jx25l8PPt1EqdmExE5GIMMkR2Njg/CipmDoVEL+DE5B18eSJe6JCIip8YgQ2RnI7oG4P/GxQMAXvnhBHak5ElcERGR82KQIXKAR0d2wR0DwmGyiJj/72QYas1Sl0RE5JQYZIgcQKUSsPjOvgjTuyGn1IB/7U6VuiQiIrs7ml6MX05dQlZJtWQ1MMgQOYibRo3nbrZ2MS3ddhZnLpVLXBERkX198tsFPLLmELaflq4LnUGGyIHuHBiO0fGBqDFZMHddImpM3JOJiJxHdV23ubuE62YxyBA5kCAIePOufvD10OBkThk+2nFO6pKIiOzGFmRcGWSInFaQzg1/n9wHAPDxb+dRUlUjcUVERPZRXcMWGaJOYVK/UPQK1cFQa8H6QxlSl0NEZBdskSHqJARBwIPDYwAAn+1Ng9nCFX+JSPnYIkPUidx2TRh8PDTILK6WdIQ/EZG9GNgiQ9R5uGnUuGdwJABgzd6L0hZDRGQHnLVE1Mncf200BAHYebYA5/IqpC6HiKjNRFG0BRk3BhmiziHSzwM39ggGAHzGVhkiUjCjyQKxbrgfu5aIOpH6Qb/fHMpEcSWnYhORMtUP9AXYtUTUqYzo6o+eoTpU15rxxb40qcshImqT+m4lVxcV1CpBsjoYZIg6mCAImH1dFwDAZ/vSuG0BESmSHAb6AgwyRJK4pW8ogry1yC834sfkbKnLISJqNTmsIQMwyBBJwtVFhfuvjQYAfHs4S+JqiIhaTw6r+gIMMkSSuaVvKADgQGoRyg21EldDRNQ69S0yUk69BhhkiCQTF+iJbkFeqDFb8PVB7r9ERMry+xgZaaMEgwyRRARBsHUv/XzyksTVEBG1jhy2JwAYZIgkdX33QADA0fRiVNWYJK6GiKjlONiXiBDt74FwH3fUmkUcSC2Suhwiohb7fbCvi6R1MMgQSUgQBIzsGgAA2HW2QOJqiIharqqGY2SICMCIbnVB5hyDDBEph4EL4hERAIyI8wcAnM4tR165QeJqiIhaxjb9moN9iTo3fy8teofpAAC72SpDRArBLQqIyKZ+9tK2k3kSV0JE1DIMMkRkM653CABge0qerd+ZiEjO6ruWPNi1RET9IvQI1buhqsbM2UtEpAj1LTLcouAqysvLMXfuXERHR8Pd3R3Dhw/HwYMHpS6LyK4EQcBNvYIBADvOsHuJiOTPNv2aLTLNe/TRR7F161Z8/vnnSE5Oxs0334yxY8ciK4s7BpNzua6bdZzML6fyYLaIEldDRNS8+m5wdi01o7q6Gt9++y3efPNNXHfddejatSsWLFiArl27YtmyZVKXR2RXI7sFwMdDg5xSA2cvEZHscffrFjCZTDCbzXBzc2twv7u7O3bt2tXoc4xGI8rKyhocRErgplFjbE9r99K+C4USV0NE1Lwq7rV0dd7e3hg2bBgWLlyI7OxsmM1mfPHFF9i7dy9ycnIafc7ixYuh1+ttR2RkZAdXTdR2Q2J8AQD7ue8SEcnc711L3GupWZ9//jlEUUR4eDi0Wi3ef/99TJs2DSpV46XPnz8fpaWltiMjI6ODKyZqu5F142SOpBcjv9wocTVERE3jOjItFBcXh99++w0VFRXIyMjAgQMHUFtbiy5dujR6vlarhU6na3AQKUW4jzt6heogiuxeIiL5EkXx9+nXrtw0skU8PT0RGhqK4uJibNmyBZMnT5a6JCKHGFzXvZSYUSJtIURETTCaLBDrJleya+kqtmzZgs2bNyM1NRVbt27FmDFj0KNHDzz00ENSl0bkENdE+gBgkCEi+aof6Auwa+mqSktLMWfOHPTo0QMzZszAyJEjsWXLFmg0GqlLI3KI/nVB5nhWKWrNFmmLISJqRH23kquLCmqVIGkt0rYHtcDUqVMxdepUqcsg6jCx/p7QubmgzGDC6Zxy9I3QS10SEVED1TKZeg0ooEWGqLNRqQRbq0xiRrG0xRARNYJBhoiaNaAuyBzlOBkikqFqmWxPADDIEMnSNVE+ADjgl4jkSS47XwMMMkSy1D/CBwBwIb8SpVW10hZDRPQH1TUmANLvfA0wyBDJkr+XFlF+HgCApMwSaYshIvoDdi0R0VVxPRkikqsqmex8DTDIEMkWgwwRyVX9rCW2yBBRky4f8CvWrwVORCQDBplsGAkwyBDJVq9QHTRqAUWVNcgoqpa6HCIiG3YtEdFVuWnU6BVq3b39KBfGIyIZqWLXEhG1BMfJEJEclRus06+93aTf95BBhkjG6sfJJDHIEJGMVBit61t5u0m/ZSODDJGMXRPpCwA4nl2GGhN3wiYiefi9RYZBhoiaEePvAR8PDWpMFpzOLZO6HCIiAAwyRNRCgiDYtis4ksYBv0QkDxVGjpEhohYaEmPtXtp3oUjiSoiIrMoNHCNDRC00vGsAAGB/aiEXxiMiWSir61ry0jLIENFV9A6zLoxXXFWLzGIujEdE0qoxWWyTD7y17FoioqvQuqgRH+INAEjOKpW4GiLq7Or3WQIADy0XxCOiFugb7gOAQYaIpFdVa+1W0qgFaNTSxwjpKyCiq+obrgcAJGcyyBCRtOq3J5DDhpEAgwyRIvSLqAsyWaUc8EtEkqq27bMk/UBfgEGGSBG6B3vDVa1CaTUH/BKRtGwtMjLYMBJgkCFSBFcXFXqEWgf8HkrjejJEJJ2qGusYGXYtEVGrjOpmXU9m68lLEldCRJ3Z711LDDJE1Ao39woBAOxIyYeh1nyVs4mIHINdS0TUJv0i9AjVu6GqxowDqexeIiJpVNWyRYaI2kAQBAyN9QMAJGWUSFsMEXVa1XVjZDhriYharX4n7KTMEknrIKLOq6zaGmQ8ZbCqL8AgQ6Qo/SN9AACJGVxPhoikkVNqAACE6t0lrsSKQYZIQXqH6eCiElBQYUR23Q8TIqKOlFtmXcsqROcmcSVWDDJECuKmUdvWk+E4GSKSQk5JXYuMD4PMVZnNZrz00kuIjY2Fu7s74uLisHDhQjapU6dmGyfDIENEErhUZg0ycmmRkceQ4ya88cYbWLZsGdasWYPevXvj0KFDeOihh6DX6/HUU09JXR6RJPpH+mDt/nQkMsgQUQezWERU1q0jo3PXSFyNlayDzJ49ezB58mRMnDgRABATE4OvvvoKBw4caPI5RqMRRqPRdrusrMzhdRJ1pGvqBvwmZ5XCbBGhVgnSFkREnUbVZYtxemnlESFk3bU0fPhw/PLLLzhz5gwAICkpCbt27cKECROafM7ixYuh1+ttR2RkZEeVS9Qh4gK94OmqRlWNGefyKqQuh4g6kUqjdeq1SgC0LvKIEPKoogl//etfce+996JHjx7QaDQYMGAA5s6di+nTpzf5nPnz56O0tNR2ZGRkdGDFRI6nVgnoG6EHwHEyRNSx6oOMp6sLBEEercGyDjLr16/H2rVr8eWXX+LIkSNYs2YN3n77baxZs6bJ52i1Wuh0ugYHkbOxrSfDhfGIqAPV77PkIZPF8ACZj5H5v//7P1urDAD07dsXaWlpWLx4MWbOnClxdUTSuYYzl4hIAhX1LTIyGR8DyLxFpqqqCipVwxLVajUsFotEFRHJQ32LzOnccu6ETUQdpqrm964luZBPJY2YNGkSFi1ahKioKPTu3RtHjx7Fu+++i4cffljq0ogkFap3g6erGpU1ZmSVVCMu0EvqkoioE6g0ymvna0DmQeaDDz7ASy+9hCeeeAJ5eXkICwvD7Nmz8fLLL0tdGpGkBEFAuK87zlyqQFYxgwwRdYxKGXYtyaeSRnh7e2Pp0qVYunSp1KUQyU64T12QKamWuhQi6iTKDLUAAG83+cQHWY+RIaKmRfh6AABSCyolroSIOouCihoAQKCXVuJKfscgQ6RQA6N9AAC7zhZIWwgRdRr55daV8wO8GWSIqJ1Gdg0EAJzMKbP1WxMROVJ9kGGLDBG1W6C3FoF1fxWd5VYFRNQBCirqggxbZIjIHuKDvQEAp3O4OSoROZ6ta4ktMkRkDwOifAAAPybnSFsIETk9k9mCoqq6wb5skSEie7hjQDgAYO/5QhhNXOGXiBynqLIGomjd+drP01XqcmwYZIgULDbAEz4eGpgsIs7kcpwMETlOXl23kp+nFmqVPHa+BhhkiBRNEAT0DdcDAJKzSiWuhoicWb4MB/oCDDJEitc7zBpkjmczyBCR4xSUM8gQkQPUt8gcZ4sMETlQfYtMgJd8xscADDJEihcfYt0wMjW/EqIoSlwNETmrfLbIEJEj1O+5VG40oaSqVuJqiMhZyXGfJYBBhkjx3DRqhOjcAADpRVUSV0NEziq/3ACALTJE5ABR/tZWmfP5nIJNRI4hx32WAAYZIqfQK1QHADiWyQG/ROQYtq4ltsgQkb31j+RaMkTkOEaTGaXV1jF4ctpnCWCQIXIKPUKsLTJnL5Vz5hIR2V1hXWuMRi1A766RuJqGGGSInEBsgCdUAlBmMNn6sYmI7OXyXa9VMtqeAGCQIXIKbho1ovysA37P5XHALxHZ1+VBRm4YZIicRNcg68J45zhziYjsrECm+ywBDDJETiOuPsiwRYaI7EyuU68BBhkip9E1kEGGiBzDts+St7z2WQIYZIicRle2yBCRg9i6ltgiQ0SOUh9k8sqNtvUeiIjsoaDcOv06gGNkiMhRvN00tj2X2CpDRPZUUm0NMr4e7FoiIgeqb5U5zyBDRHZU38ort8XwAAYZIqfCKdhE5AglVQwyRNQBOAWbiOzNUGuG0WQBAOg9GGSIyIG6McgQkZ2V1XUrqQTAy9VF4mquxCBD5ETqu5YyiqtgqDVLXA0ROYOSy8bHyG2fJYBBhsip+Hu6wsdDA1EEznOcDBHZQXGldcaSjwxnLAEKCDIxMTEQBOGKY86cOVKXRiQ7giBwhV8isqs8GW9PACggyBw8eBA5OTm2Y+vWrQCAKVOmSFwZkTxxCjYR2ZMtyOjkGWTkN2rnDwIDAxvcXrJkCeLi4nD99dc3er7RaITRaLTdLisrc2h9RHLDKdhEZE955QYAQJAMV/UFFNAic7mamhp88cUXePjhhyEIjQ84Wrx4MfR6ve2IjIzs4CqJpMU9l4jInvLLrI0DQd5uElfSOEUFme+//x4lJSV48MEHmzxn/vz5KC0ttR0ZGRkdVyCRDNQHmdSCStSaLRJXQ0RKl11aDQAI0cuzRUb2XUuXW7lyJSZMmICwsLAmz9FqtdBq5XmxiTpCmN4dOjcXlBlMSMktR59wvdQlEZGCpRVWAQCi/T0lrqRximmRSUtLw7Zt2/Doo49KXQqRrKlUAgZG+wIADl4skrgaIlIyQ60ZOaXWMTIxDDLts2rVKgQFBWHixIlSl0IkewMirUEmOatU4kqISMkyi63dSt5aF/jKcHsCQCFBxmKxYNWqVZg5cyZcXBTVG0YkiZ6h3gCA0znlEldCREqWV2ZtjQnWuzU5yUZqiggy27ZtQ3p6Oh5++GGpSyFShJ6hOgDWmUsc8EtEbVW/hoxcp14DChnse/PNN0MURanLIFKMCF93eGldUGE04UJ+JeJDvKUuiYgUKL9+MTwZBxlFtMgQUesIgoAedeHldC4XhSSitpH7YngAgwyR06rvXjqZwyBDRG1TP9g3RO8ucSVNY5AhclI9OOCXiNopJdf68yM+WL7d0wwyRE6qvkXmFFtkiKgNqmpMSC2sBPD7H0ZyxCBD5KTq/4LKKzeisMJ4lbOJiBo6c6kComgd6BvgxTEyRNTBPLUuiPb3AACczmX3EhG1zum61tweMp/1yCBD5MR6hrB7iYjaJjGjBMDv3dRy1aYgk5GRgczMTNvtAwcOYO7cuVi+fLndCiOi9qvv1z7FAb9E1AqiKGLbqTwAwHXdAiWupnltCjL33Xcftm/fDgDIzc3FTTfdhAMHDuCFF17Aq6++atcCiajt6v+S4loyRNQaZdUmFNSNrRsc4ytxNc1rU5A5fvw4hg4dCgBYv349+vTpgz179mDt2rVYvXq1Pesjonao71o6e6kCJm5VQEQtVFBpDTHebi5w06glrqZ5bQoytbW10GqtI5i3bduG2267DQDQo0cP5OTk2K86ImqXCF93eLqqUWO24EJBpdTlEJFCFFbUAICsZyvVa1OQ6d27Nz7++GPs3LkTW7duxfjx4wEA2dnZ8Pf3t2uBRNR2KpWAHlxPhohaqX7JBj9PV4krubo2BZk33ngDn3zyCUaPHo1p06ahf//+AICNGzfaupyISB7qp05ywC8RtVRhpbVFxl8BQaZNu1+PHj0aBQUFKCsrg6/v74OAHnvsMXh4eNitOCJqv/oBv3vOF0AURQiCIHFFRCR39V1L/s7atVRdXQ2j0WgLMWlpaVi6dClSUlIQFBRk1wKJqH2Gx/lDEIBjmaU4nFYsdTlEpACFdYN9A7zk3yLTpiAzefJkfPbZZwCAkpISJCQk4J133sHtt9+OZcuW2bVAImqfLoFeGNk1AABwNq9C4mqISAlsLTIK6FpqU5A5cuQIRo0aBQDYsGEDgoODkZaWhs8++wzvv/++XQskovaLDfAEAGQUVUlcCREpQf0aMn7O2rVUVVUFb2/rAMKff/4Zd955J1QqFa699lqkpaXZtUAiar9IX+vYtQv5nIJNRFdXVDfYN8BZW2S6du2K77//HhkZGdiyZQtuvvlmAEBeXh50OnnvyUDUGfUJ1wMAfj2dh9xSg8TVEJHc2WYtOWuLzMsvv4x58+YhJiYGQ4cOxbBhwwBYW2cGDBhg1wKJqP2u7eKHayJ9UGO2YMuJXKnLISIZK66ssbXIhPq4SVzN1bUpyNx9991IT0/HoUOHsGXLFtv9N954I/7xj3/YrTgisg9BEDC+TwgA6zRsIqKm1C+eGeXnAZ2bRuJqrq5N68gAQEhICEJCQmy7YEdERHAxPCIZ6x7sBQDIKKqWuBIikrOTdUGmV6gyhoq0qUXGYrHg1VdfhV6vR3R0NKKjo+Hj44OFCxfCYuHGdERyVD/gN6OYM5eIqGm2IBOmjCDTphaZF154AStXrsSSJUswYsQIAMCuXbuwYMECGAwGLFq0yK5FElH7hfu6AwDKDSaUVtVC7yH/JmMi6ngns5XVItOmILNmzRqsWLHCtus1APTr1w/h4eF44oknGGSIZMjD1QUhOjfklhlwJq8cQ2L8pC6JiGRGFEWkFVpbbeOCvCSupmXa1LVUVFSEHj16XHF/jx49UFRU1O6iiMgx+kZYp2EnZZRIWwgRyVJpdS2qa80AgFC9/GcsAW0MMv3798c///nPK+7/5z//iX79+rW7KCJyjGsifQAABy/yDw4iulJWiXUyQICXK9w0aomraZk2dS29+eabmDhxIrZt22ZbQ2bv3r3IyMjApk2b7FogEdnPdd0C8daWFOw6WwCjyQytizJ+UBFRx8gpsS6YGap3l7iSlmtTi8z111+PM2fO4I477kBJSQlKSkpw55134sSJE/j888/tXSMR2UnvMB2CvLWorDHjQCpbZYioobS6/dgifJUTZNq8jkxYWNgVg3qTkpKwcuVKLF++vN2FEZH9qVQCxsQH4etDGfj5xCWM6hYodUlEJCMX8isAAF0CPSWupOXa1CJDRMp1S79QAMCPyTkwW0SJqyEiOanfWLZLgDJmLAEKCDJZWVm4//774e/vD3d3d/Tt2xeHDh2SuiwixRoR5w9vrQuKKmtwIrtU6nKIqBmiKMJQN4vI0YwmM5IySwAAPUK9O+Q97UHWQaa4uBgjRoyARqPBTz/9hJMnT+Kdd96Br6+v1KURKZaLWoWELtY1ZHafK5S4GiJqznPfJGHIa9uw/4Lj/61+cygTVTVmBHpr0TNEGYvhAa0cI3PnnXc2+3hJSUl7arnCG2+8gcjISKxatcp2X2xsrF3fg6gzGh4XgG2n8rDnfAH+NDpO6nKIqAn/PpIFAJj12SEcWzDOoe+1/lAGAGB6QhRUKsGh72VPrWqR0ev1zR7R0dGYMWOG3YrbuHEjBg8ejClTpiAoKAgDBgzAp59+2uxzjEYjysrKGhxE1NCIrgEArOvJGE0d02xNRK2z77JWmDKDCXnlBoe9lyiKSC2wjo+Z0CfUYe/jCK1qkbm8ZaQjXLhwAcuWLcOzzz6Lv/3tbzh48CCeeuopuLq6YubMmY0+Z/Hixfj73//eoXUSKU33YC8EeGlRUGHEkbQSDIvzl7okIrqModaMe5fva3Dfr6fycO/QKIe8X0lVLcoNJgBAlJ+HQ97DUWQ9RsZisWDgwIF4/fXXMWDAADz22GOYNWsWPv744yafM3/+fJSWltqOjIyMDqyYSBkEQbCFF64nQyQ/xzKvHIj/6+k8h73f7vMFAIAQnRvcXZW1UKasg0xoaCh69erV4L6ePXsiPT29yedotVrodLoGBxFdaXC0ddD8kfRiiSshoj8qrDACALoEeGLD49YV9H8+eQklVTUOeb+vD1r/6J98TZhDXt+RZB1kRowYgZSUlAb3nTlzBtHR0RJVROQ8BkT5AACOZZZAFLmeDJGcFFZaA0vXIC/0j/SBzs06EuRPXxyx+3uJoojkLGsL0K39GGTs6plnnsG+ffvw+uuv49y5c/jyyy+xfPlyzJkzR+rSiBSve7A3VAJQXFWL/HKj1OUQ0WWK64KMn6crNGoVFt7eBwCw90IhYv76Iz7cfs5u75VdakBJVS00agHdQ5SzEF49WQeZIUOG4LvvvsNXX32FPn36YOHChVi6dCmmT58udWlEiuemUSPG37oM+anccomrIaLLFdV1Ifl6ugIAbusfhtsv6/ZZuu0Mas0Wu7xXat1qvtH+norcSFbWQQYAbr31ViQnJ8NgMODUqVOYNWuW1CUROY1rIn0AAHvPc2E8Ijmpb5HxrwsygiBg6b0D8I97+gMAas0i0gqr7PJeaUXWIKO02Ur1ZB9kiMhxro+3bhq561y+xJUQ0eXqx8j4erg2uP+OARHoH6EHAJzMaf86adU1ZqzefREAgwwRKdDgGOtWBadzyjtsPxciurriqt/HyPxRQhfr0gnrDjQ9g7el5m1Iwtk8647XfcP17X49KTDIEHViYXo3BHi5wmQRccoOf90RkX0UV9YC+H2MzOVmDo+BIAB7zhcirbCyze9RUlWDH4/lAADuHRKJ2weEt/m1pMQgQ9SJCYKA+BDrLrfn89v+A5GI7Kuw0jqT0L+RIBPu446RdduMbDic2eb3qO+aivRzx5K7+kGtoP2VLscgQ9TJ1c9culjAIEMkB9U1ZhhqrTOSGmuRAYCpgyMBAB/8eg4bk7Lb9D7b61YKVtJO141hkCHq5OqDTGo7mqiJyH7qp167qlXwbGK7gJt6Bdu+fu2/J1v/HpU1WLMnDQAwsZ+yNon8o1ZtGklEzqe+aykx3brCryAos3mZyFnUT7329dQ0+e/RTaPGG3f1xfPfJiOv3Iic0mqE6t2v+tobk7Lx/dEsuGvUqDFb0Ddcj8nXKHNsTD22yBB1coNjfKFRC8gqqUYqu5eIJFdoW9VX2+x59wyJwpAY655pf9lwDGbL1bcaeeqro/j1dB5+TLYO8p03Lr6d1UqPQYaok/NwdUFCrHU65+YTuRJXQ0S/b0+gueq5L9/aG+4aNXaeLcC6g81Px/719KUGt0d09cf13QPbXqhMMMgQka2PfFPdX2lEJJ2iJhbDa0zfCD2evKErAODXU3lNnnc4rRizPjtsu31rv1B8OmNwOyuVBwYZIsK43iFQqwQczypDdkm11OUQdWrNLYbXmKGx1oUtT2Q3vRbUv3anwmwRcX33QCS9fDP+ed9AeLg6xzBZBhkigp+nK7oHWwf9Hs8qlbgaos6tsLJ1QaZnqA6CAOSWGa7YyT6/3IjHPjtkW/hu9vVdoPe4epeVkjDIEBEAoFeodS0Je+zfQkRtV1ZtXdVX59aywOGldUFY3YylUW/+ikqjCWaLiOLKGkz7dB9+PmkdG+OiEtA/wschNUvJOdqViKjd+kfq8e2RTOw5X4i5Y6WuhqjzqjCaAADebi3/FR3t74GskmoYai0YtvgX+Htpr5iF+N69A+Cpdb5f+2yRISIAwA09ggAAB1KLcCC1SOJqiDqvCoM1yHi1InQ8fWM329dlBlODEKMSgON/H6f4he+awiBDRACACF8P2w86e+yqS0RtU98i49WKFpmELv64uGQiTr46Ds+M7d7gsZTXJrQqFCkNgwwR2dxTt3/L/tQiiOLVF9ciIvurDzJt6QbycHXB02O74ZfnrkeAlyv+b1w8NGrn/lXvvBGNiFptULQvXF1UyCqpxunccvQMVfZmckRKZBsj045WlLhALxx68SZ7lSRrzh3TiKhVPLUuGF230ucvpy5d5WwisjdRFFHZjhaZzohBhogaGBJjXVzreBanYRN1NKPJglqztVu3NWNkOjMGGSJqoHcY15MhkkqZodb2taeTrLzraAwyRNRAr7ogk15U1eCHKhE53umccgBAlJ8H1CpB4mqUgUGGiBrw8XBFuI91ldCTzezdQkT2dzS9BAAwMMpH0jqUhEGGiK7QJ9zaKsOF8Yg61vn8CgDgjMFWYJAhoivc2DMYALApOUfiSog6l4ziKgDWriVqGQYZIrrCzb2C4aIScDq33PYXIhE5XkaRNchEMsi0GIMMEV3Bx8MV19WtJ/P53jSJqyHqHAy1ZhRU1AAAInzdJa5GORhkiKhR918bBQDYxoXxiDpEabV1lqBaJUDvrpG4GuVgkCGiRg2Kti6Ml1lcjdIqTsMmcrSSun9nencNBIFTr1uKQYaIGqV319gGHJ7ILpW4GiLnV1Jl7VbyYWtMqzDIEFGT6qdhH2eQIXK4krquJb0Hg0xrMMgQUZN6h+kBAMcyGWSIHK2+C5ctMq0j6yCzYMECCILQ4OjRo4fUZRF1GvUbSO5IyUd1jVniaoicW0l1XdeSh6vElSiL7Hek6t27N7Zt22a77eIi+5KJnMbgaF9E+Lojs7gau88VYGyvYKlLInJalw/2pZaTdYsMYA0uISEhtiMgIEDqkog6DZVKsK0ns/t8gcTVEDm3+jEyPhwj0yqyDzJnz55FWFgYunTpgunTpyM9Pb3Z841GI8rKyhocRNR2I7ta/3jYfY5BhsiR6teR4RiZ1pF1kElISMDq1auxefNmLFu2DKmpqRg1ahTKy8ubfM7ixYuh1+ttR2RkZAdWTOR8hnXxhyAAZy5VYNtJLo5H5Cj1g305a6l1ZB1kJkyYgClTpqBfv34YN24cNm3ahJKSEqxfv77J58yfPx+lpaW2IyMjowMrJnI+vp6umDrI+gfBm1tOQxRFiSsick62wb7uHOzbGrIOMn/k4+OD7t2749y5c02eo9VqodPpGhxE1D4v3NoTnq5qnLlUgZ1n2cVE5AglbJFpE0UFmYqKCpw/fx6hoaFSl0LUqejcNJg6xNoq8+nOCxJXQ+ScOGupbWQdZObNm4fffvsNFy9exJ49e3DHHXdArVZj2rRpUpdG1Ok8NDwWggDsPFuAzOIqqcshciql1bWoMJoAACE6N4mrURZZB5nMzExMmzYN8fHxmDp1Kvz9/bFv3z4EBgZKXRpRpxPl74FrY/0BAP9JypG4GiLnklFk/eMgwMsVnlqul9Yasr5a69atk7oEIrrM5GvCsPdCIX5IzMKfRsdJXQ6R06gPMpF1G7VSy8m6RYaI5GVCn1Bo1AJO55bjdC7XaCKyl6ySagBAuI+7xJUoD4MMEbWY3kOD0fFBAICNidkSV0PkPMoM1vExvtxnqdUYZIioVSZfEwYA+PpgBooqaySuhsg5lNWt6qtzl/WID1likCGiVrm5Vwi6B3uhsLIGH21vek0nImq5MkNdkHHj1OvWYpAholZxdVFh/oSeAID1hzJQY7JIXBGR8pVVW7uWdFxDptUYZIio1a7rHoggby3KDCb870y+1OUQKR5bZNqOQYaIWk2tEnBrP+tYmQ2HMyWuhkj5OEam7RhkiKhNpg6JgCAAm0/k4lxehdTlEClaed2sJW+2yLQagwwRtUmPEB2u62ZdZXvbqUsSV0OkXKIoIr/cCMC6si+1DoMMEbXZ2J7WNWW+3J8Ok5mDfonaoqiyBjVmCwQBCPLmPkutxSBDRG1258AIeLu5IL2oCieyudIvUVvklBoAAAFeWri68Ndya/GKEVGbeWpd0C9CDwBIyS2XuBoiZcqtCzLc9bptGGSIqF16hOgAAPsuFEpcCZEy1e9bFuXPDSPbgkGGiNplfJ8QAMD3iVnIKa2WuBoi5dmfWgQAGBrjJ3ElysQgQ0TtMiTGD0Nj/GARgXUHMqQuh0hxLuRXAgD6hOskrkSZGGSIqN0eGBYNAHjvl7P47igXyCNqKVEUkV9hnXrNGUttwyBDRO02sW+o7a/Jv36bjPK65daJqHllBpNtv7JAb63E1SgTgwwRtZtKJeCDaQMBAEaTBd8dzZK4IiJlKKhrjfF2c4GbRi1xNcrEIENEdhEb4Im/39YbAPDWlhRklXDgL9HV1K/oy9aYtmOQISK7uS8hCtdE+qDcYMJ7285IXQ6R7P2+NQGDTFsxyBCR3WjUKsyf0AMAsP5QJg6nFUtcEZG8sUWm/RhkiMiuErr4Y1L/MADAhsOcwUTUnPoZS4FskWkzBhkisrt7BkcCAL4+mI4L+RUSV0MkX2yRaT8GGSKyu5HdAnBd90BYRLbKEDWnftYSg0zbMcgQkUNMG2Jtlflox3n8kMjp2ER/JIoiTudYN1uN9OU+S23FIENEDnFDzyDbTIyn1yXiYkGlxBURyUtmcTVyywzQqAVcE+kjdTmKxSBDRA6hdVFjzcNDbLe3nrwkYTVE8pOSa22N6R7sDXdXLobXVgwyROQwvcP0eHFiTwDA6j0XYbGIEldEJB9pRVUAgBh/T4krUTYGGSJyqKGxfgCArJJqPLLmIESRYYYIANIKrd2tUf4cH9MeDDJE5FC9w/QY2zMIALA9JR8HUoskrohIHtIKrS0y0X4MMu3BIENEDqVWCVgxcwimDY0CALyzlVsXEAFAel3XUjS7ltqFQYaIOsRTN3YFABxILcKlMoPE1RBJy2S2IMMWZNgi0x6KCjJLliyBIAiYO3eu1KUQUSuF6t3Rv26K6a+n86QthkhiOaUGmCwiXF1UCNG5SV2OoikmyBw8eBCffPIJ+vXrJ3UpRNRGN9WNldnGqdjUyV2sG+gb6esOlUqQuBplU0SQqaiowPTp0/Hpp5/C19e32XONRiPKysoaHEQkDzf2DAYA7DpXgOoas8TVEEmnfqAvp163nyKCzJw5czBx4kSMHTv2qucuXrwYer3edkRGRnZAhUTUEj1CvBHu4w6jyYLbP9yNnNJqqUsikkT9QF9OvW4/2QeZdevW4ciRI1i8eHGLzp8/fz5KS0ttR0ZGhoMrJKKWEgTBNug35VI5/rLhmMQVEUnjXJ51V/jYALbItJeL1AU0JyMjA08//TS2bt0KN7eWDYbSarXQarmLKJFcTR0cieNZZfh8Xxp2nStAdkk1wnzcpS6LqEOdzLYOe+gVqpO4EuWTdYvM4cOHkZeXh4EDB8LFxQUuLi747bff8P7778PFxQVmM/vYiZRGEAQsvL0PEmL9IIrAd0e5MzZ1LvnlRuTWLUHQg0Gm3WQdZG688UYkJycjMTHRdgwePBjTp09HYmIi1GpuskWkVHcNigAAfHs4k9sWUKey61w+AGtrjJdW1h0jiiDrIOPt7Y0+ffo0ODw9PeHv748+ffpIXR4RtcMtfUPh4arGhYJK9HllC9Yf5Hg26hz2X7Bu0zGqe4DElTgHWQcZInJeXloX/HVCDwBAZY0Zf/n2GA6nFUtcFZHjJWeVAgAG1C0QSe2juCCzY8cOLF26VOoyiMgOZgyLwZGXbsK43tb1ZRb9eBI1JovEVRE5To3JgjOXygFYN1Sl9lNckCEi5+Ln6Yq/TugJVxcVjqSXYO3+NKlLInKY8/kVqDWL8HZzQYQvZ+vZA4MMEUkuNsATfx1v7WZad4BjZch5peRaW2N6hHhDELg1gT0wyBCRLNw1MAJqlYCUS+VIr1u+ncjZnK4LMvEh3hJX4jwYZIhIFvQeGgyN8QMAfLb3IiwWTskm55OSa10ILz6YQcZeGGSISDZu6mUd9LtiVyr+tPYw15chp5JXbsD2FOsaMvEhXAjPXhhkiEg27h4cgW5BXgCALScuYdupPIkrIrKPvHIDhi76xXa7ZyhbZOyFQYaIZEPnpsGmp0fhhh5BAIBZnx1CuaFW4qqI2u8/STm2ryN83eHtppGwGufCIENEsqJRq/DM2O62298ezpSwGqL2M5rMeHtLiu32h/cNlLAa58MgQ0Sy0zdCj7/dYp2OvWTzaWSXVEtcEVHbWCwi/vTFEVTXWjc5/nJWAvpzRV+7YpAhIll6ZGQXDInxhaHWguFLfsW5vHKpSyJqtX2phfj1dB5c1Sq8OLEnhnXxl7okp8MgQ0SypFYJmDOmq+32Z3u54i8pz+5zBQCAW/qG4NFRXbgIngMwyBCRbF3fPdDWDL89JY/TsUlRjmeVYvn/LgAAhnflTteOwiBDRLIlCAK+mpUAVxcVMoqqcSC1SOqSiFokrbASt36wC7Vma/geHscuJUdhkCEiWfNwdcHYntbp2K9vOiVxNUQt8+WBdNvXQ2J8EeHrIWE1zo1Bhohkb/6EngCApMxS5JUbJK6GqHlGkxnfHLIuG/DOlP74/JEEiStybgwyRCR7kX4e6B+hBwA8v+EYakwWiSsiatrPJy6hqLIGwTotJl8TBjeNWuqSnBqDDBEpwrxx8VAJwPaUfHy684LU5RA16ZdTlwAAdw6MgIuav2YdjVeYiBRhVLdAvDq5DwDgrS0p+HD7Oc5iItkRRRF7LxQCAEZ140yljsAgQ0SKcefAcHi4Wpvp39qSgi0nciWuiKihnFIDLpUZ4aISMDDKV+pyOgUGGSJSDA9XF3x8/yDb7afXJeJoerGEFRE1dCyzBADQPdibY2M6CIMMESnKdd0DcXrheIyOD4TRZMFrP55iFxPJRlJmKQCgf6Re4ko6DwYZIlIcN40ab97VD1oXFQ6nFWPHmXypSyICAOyvGx/TN9xH2kI6EQYZIlKkIJ0bHrg2GgDwpy8O42R2GVtmSFLPfJ2II+klAICB0T6S1tKZMMgQkWL9aXQc9O4aGGotuOX9nZjy8V7sSMmDycx1ZqhjXcivwHdHswAAD4+IRY8QncQVdR4MMkSkWP5eWnw6Y7Dt9qG0Yjy46iAeWn0QFgtbZ6jjfH0wA4B1o9OXJ/WSuJrOhUGGiBRtaKwfVs4cjLsHRWBsz2AAwM6zBUismz1C5Ggrd6Xik7pdru9LiJK4ms6HQYaIFO/GnsF4e0p/rJg5GLf1DwMA3PnRHry+6RRq2c1EDmQyW/DG5tO22zf0CJKwms6JQYaInMoTY+Lg66EBACz/3wVMeG8nVuy8ADO7msgBdp4rsO399e2fhkHDLQk6HK84ETmVHiE67J1/I5bc2RcAcC6vAq/9eAr3fboPmcVVEldHzqTMUIs3frK2xjw4PAaDov0krqhzYpAhIqfjplHj3qFR+Pqxa3FfQhQEAdifWoQ5Xx7lIGCyixqTBY99dginc8sR4KXFrOu6SF1Sp8UgQ0ROK6GLP16/oy8+vG8gACApowQfbj8ncVWkdBcLKnHbP3dh34UieGldsPqhIQj3cZe6rE5L1kFm2bJl6NevH3Q6HXQ6HYYNG4affvpJ6rKISGFu6RuKN+/uBwD4cMc5ZJdUS1wRKVVJVQ1Gv70Dp3PLAQAfTBuAPuHcjkBKsg4yERERWLJkCQ4fPoxDhw7hhhtuwOTJk3HixAmpSyMihZkyKAJDY/1gqLVgxc5UqcshBaowmvDYZ4dtt/08XTGGs5QkJ+sgM2nSJNxyyy3o1q0bunfvjkWLFsHLywv79u2TujQiUhhBEPDIyFgAwH+OZeNSmUHiikgpygy12JScgz9/eQQHLhYBAMJ93LFi5uCrPJM6govUBbSU2WzGN998g8rKSgwbNqzJ84xGI4xGo+12WVlZR5RHRApwXbdA+Hu6Ir/ciNs/3I2fn7kOapUAd40agiBIXR7JkNki4sF/HbDtoQQAN/cKxvIZDDFyIesWGQBITk6Gl5cXtFotHn/8cXz33Xfo1avp5Z8XL14MvV5vOyIjIzuwWiKSM3dXNT5/JAF+nq7IKTWg74Kf0evlLZjxrwNcOI+usPtcAQa9trVBiOkW5MUQIzOCKPPtYmtqapCeno7S0lJs2LABK1aswG+//dZkmGmsRSYyMhKlpaXQ6biJFxEBO1Ly8OCqgw3u+2j6QNzSN1SiikhOLBYRP5/MxXPrk1BZYwYAjO0ZjJt7B+PaWH9E+XtIXGHnUFZWBr1ef9Xf37IPMn80duxYxMXF4ZNPPmnR+S29EETUuWxPycM/tp7BscxS2333DonE3yb2hM5NI2Fl1NEyiqqgUgm2KdQvfp+ML/al2x7/5IFBuLlXMLsfO1hLf38rZoxMPYvF0qDFhYioLcbEB2FMfBBOZJdi4vu7AADrDmZgz/lCvHxrL4ztFSxxheRoxZU1WLTpFL49kgmNWoXXJvfBv3an2qZWTxsaiedujkeAl1biSqk5sm6RmT9/PiZMmICoqCiUl5fjyy+/xBtvvIEtW7bgpptuatFrsEWGiK6mzFCLpVvP4l+7f5+WPW1oJAABpdU1uGNABEZ1C4CbRi1dkdRuoiiitLoWencNRBGY9M9dOJHd+ISQUd0C8PkjCR1cIV3OKVpk8vLyMGPGDOTk5ECv16Nfv36tCjFERC2hc9Pg5Um9cGPPIExfsR8A8NWBDNvjm5JzMTjaF1/PHga1it0LSvXaj6ewcpc1rOrdNSitroWHqxrvTOmPZ9cnobrWOh7m1n6heOOuflKWSq0g6xYZe2CLDBG1xqUyAxZvOoXTueU4c6kcl2/N9MqkXnjg2mi4cIdjRRBFEfnlRhhqLXj75xRsTMq+4py/jI/HE6O74mJBJdKKqtAnTAd/diXJgtMO9m0tBhkiao/M4ip8sS8dH/923nbfmoeHol+4Hv+34RhqzRZ88sAgdjvJzKGLRVjwnxM4ntX0WmJv3t0Pdw+MgIqtbLLEIFOHQYaI2stsEfHAyv3Yc74QAODroUGUnweS6mY8je8dgr4RekwZHIEgbzcpSyUA5YZa3PDOb8gvbzgxZOrgCLwwsRcyiqoQqndjy4vMMcjUYZAhInsoqarBN4cy8eGOcyipqm3yvLWPJmBE14Ar7hdFkdN3O0BpVS1mf3EI+y4UwddDAxe1CiVVNVj+wGDui6QwDDJ1GGSIyJ5+OXUJj6w5BMA6ZubgxSJsSs61Pe6qVuGJMXH40+g4aF3UqDVb8NRXR/G/M/n4dOZgDI+7MuRQ2+WVGWA0WRDp54HEjBI8vPogiipr4OmqxhePJqBbsDeqjCYE6dhSpjQMMnUYZIjI3raevIQgby36R/oAsLa2XCoz4u6P9yCzuNp2no+HBrUmi2112EBvLTY8PgzR/p62c0qraqHVqDjGppUsFhGLf7LOQrKIQEKsH5KzSlFVY0aXQE+8dXd/DIr2lbpMagcGmToMMkTUUWrNFqzclYolP51u9rxR3QLw/Pge+NeuVPz7aBYCvKwBJybAGnDSCivhqXXhQmzN+CExC0+vS7zi/lHdAvDx/YPgqZX16iLUAgwydRhkiKij/ScpGy98l4xb+4dhQp8QxPh7wkUtYNryfbhYWNXoc4Z18ceKmYPxysYT2HA4E3p3DX597noOSK2TWVyFNzanYOawaGSXGvDUV0cBAHPGxOG+hGis3ZcGEcDcsd2gdWHrljNgkKnDIENEclFrtmDfhUL8+aujtgHDapUAs6XxH8NaFxXevLsfJl8T3pFlykpGURVe33QKPx3PveKx2ABPbHxyBLy5N5ZTcoqVfYmInIlGrcKoboHY9NQoZJVUQ6NWoW+4Hu/8nIKPdljXqVGrBNw3NArrDqbDaLLg2fVJiPb3RLBOi1C9u8SfoGN9vi8NL31/vMnHX7+jL0MMsUWGiEhqRpMZ8745hhPZpVg4uQ9GdA3AkfRi3PnRHts5apWAxXf0xd2DIpBTZsB/k7IR7e+J8X1CJKzc/gy1Zrhp1Nh8PBePf3HYdn98sDeGxPriv8dyMKlfGF6d3JvT2Z0cu5bqMMgQkVIdzyrFI2sO4lKZsdHHBQFYMKk3ZgyLbvKXuslsnTWld299y0Wt2QJNK7djsFhEHEorRqC3FrEBno2ec/ZSOeZ9kwS9hyv6huswJMYPA6J88VNyDl78/jhMl3W1xQd7479PjWx1HaR8DDJ1GGSISMlMZgtKqmvx7tYz+HJ/eqPnjOwagMExvqgwmDBzeAwi/TxQWl2Lrw6kY+WuVOSXG/HwiFg8d3N3uGvUzS7Jn1ZYibe2pOC/x3Js931430D0i9DjXF4F4kO8EeZzZRdXUWUN/vTFYexPLQIAqATg0xmDcWPP4AbnXSyoxJNfHWl264DLbZl7HeJDvFt0LjkXBpk6DDJE5CwWbDyB1Xsuwk2jwm//NwbfHMrA2z+fueK8KD8PpBc1PjsKAPqE67Bs+iBE+nk0uP8/Sdl45uvEBi0ifxTu447vnhjeYIG5jKIqjHpze6Pnj60LMttOXWr2swGAi0pA1yAvxPh7YmC0D2YOj+EMpE6MQaYOgwwROZOqGhPMFhHebhqIooh7PtmHAxeLmjz/weExiAv0xNs/n0FpdcOtFX6YM8K2qF+NyYIRb/yK/HIjwvRuGBzjhyExvli06RQMtRYIAnD5b4sFk3phfJ9QHMsswWOf/z6WZUCUD+aM7ooVuy5g34XG6+odpsOzN3VHl0AvZBVXI+VSOXqGeiM+2JvTzcmGQaYOgwwRObNyQy0ulRnQJcALggBsT8lDWbUJPh4aDInxsy0Ml1duwNcHMnChoBLfHc2yPf/x6+OQXlSJpIxSZJVUI8DLFXvn32gbkyKKIvIrjPBxd8WnOy/grS0pTdaycHJvPDAsxlbX2v3pOJBahF9P56FLgCcSuviha5A3ZgyL5pgXuioGmToMMkREDf3z17ONdkkBwIsTe+LRUV2afO7J7DIs++08/pOU3eD+VQ8NwZh4bspI9sMgU4dBhojoSnvOF+DF746jtLoWMQGeuP2aMIT5uOOGHkFXndYsiiJO55Zj2Y7zqDSa8M/7BsLdlWNZyL4YZOowyBARESlPS39/s5OSiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBTLReoCHE0URQDW7cCJiIhIGep/b9f/Hm+K0weZ8vJyAEBkZKTElRAREVFrlZeXQ6/XN/m4IF4t6iicxWJBdnY2vL29IQiC3V63rKwMkZGRyMjIgE6ns9vrOiter9bh9WodXq/W4fVqOV6r1rHn9RJFEeXl5QgLC4NK1fRIGKdvkVGpVIiIiHDY6+t0On5ztwKvV+vwerUOr1fr8Hq1HK9V69jrejXXElOPg32JiIhIsRhkiIiISLEYZNpIq9XilVdegVarlboUReD1ah1er9bh9WodXq+W47VqHSmul9MP9iUiIiLnxRYZIiIiUiwGGSIiIlIsBhkiIiJSLAYZIiIiUiwGGTvYsWMHBEFo9Dh48KDU5cnSjz/+iISEBLi7u8PX1xe333671CXJVkxMzBXfV0uWLJG6LNkzGo245pprIAgCEhMTpS5Htm677TZERUXBzc0NoaGheOCBB5CdnS11WbJ08eJFPPLII4iNjYW7uzvi4uLwyiuvoKamRurSZGnRokUYPnw4PDw84OPj47D3cfqVfTvC8OHDkZOT0+C+l156Cb/88gsGDx4sUVXy9e2332LWrFl4/fXXccMNN8BkMuH48eNSlyVrr776KmbNmmW77e3tLWE1yvCXv/wFYWFhSEpKkroUWRszZgz+9re/ITQ0FFlZWZg3bx7uvvtu7NmzR+rSZOf06dOwWCz45JNP0LVrVxw/fhyzZs1CZWUl3n77banLk52amhpMmTIFw4YNw8qVKx33RiLZXU1NjRgYGCi++uqrUpciO7W1tWJ4eLi4YsUKqUtRjOjoaPEf//iH1GUoyqZNm8QePXqIJ06cEAGIR48elbokxfjhhx9EQRDEmpoaqUtRhDfffFOMjY2VugxZW7VqlajX6x32+uxacoCNGzeisLAQDz30kNSlyM6RI0eQlZUFlUqFAQMGIDQ0FBMmTGCLzFUsWbIE/v7+GDBgAN566y2YTCapS5KtS5cuYdasWfj888/h4eEhdTmKUlRUhLVr12L48OHQaDRSl6MIpaWl8PPzk7qMTo1BxgFWrlyJcePGOXSzSqW6cOECAGDBggV48cUX8d///he+vr4YPXo0ioqKJK5Onp566imsW7cO27dvx+zZs/H666/jL3/5i9RlyZIoinjwwQfx+OOPs1u3FZ5//nl4enrC398f6enp+OGHH6QuSRHOnTuHDz74ALNnz5a6lM7NYW09TuD5558XATR7nDp1qsFzMjIyRJVKJW7YsEGiqqXR0mu1du1aEYD4ySef2J5rMBjEgIAA8eOPP5bwE3Sstnxv1Vu5cqXo4uIiGgyGDq5aOi29Xu+99544YsQI0WQyiaIoiqmpqZ2ya6m131/5+fliSkqK+PPPP4sjRowQb7nlFtFisUj4CTpWW/49ZmZminFxceIjjzwiUdXSaMu1cnTXErcoaEZ+fj4KCwubPadLly5wdXW13V64cCE++OADZGVldaqm2ZZeq927d+OGG27Azp07MXLkSNtjCQkJGDt2LBYtWuToUmWhLd9b9U6cOIE+ffrg9OnTiI+Pd1SJstLS6zV16lT85z//gSAItvvNZjPUajWmT5+ONWvWOLpUWWjP91dmZiYiIyOxZ88eDBs2zFElykprr1d2djZGjx6Na6+9FqtXr4ZK1Xk6N9ryvbV69WrMnTsXJSUlDqmJs5aaERgYiMDAwBafL4oiVq1ahRkzZnSqEAO0/FoNGjQIWq0WKSkptiBTW1uLixcvIjo62tFlykZrv7cul5iYCJVKhaCgIDtXJV8tvV7vv/8+XnvtNdvt7OxsjBs3Dl9//TUSEhIcWaKstOf7y2KxALBOX+8sWnO9srKyMGbMGAwaNAirVq3qVCEGaN/3lqMwyNjRr7/+itTUVDz66KNSlyJbOp0Ojz/+OF555RVERkYiOjoab731FgBgypQpElcnP3v37sX+/fsxZswYeHt7Y+/evXjmmWdw//33w9fXV+ryZCcqKqrBbS8vLwBAXFwcx6w1Yv/+/Th48CBGjhwJX19fnD9/Hi+99BLi4uI6TWtMa2RlZWH06NGIjo7G22+/jfz8fNtjISEhElYmT+np6SgqKkJ6ejrMZrNtPaeuXbva/m3ahcM6rTqhadOmicOHD5e6DNmrqakRn3vuOTEoKEj09vYWx44dKx4/flzqsmTp8OHDYkJCgqjX60U3NzexZ8+e4uuvv96pxse0R2cdI9NSx44dE8eMGSP6+fmJWq1WjImJER9//HExMzNT6tJkadWqVU2OC6ErzZw5s9FrtX37dru+D8fIEBERkWJ1rs49IiIicioMMkRERKRYDDJERESkWAwyREREpFgMMkRERKRYDDJERESkWAwyREREpFgMMkRERKRYDDJEZBMTE4OlS5dKXUaHeemll/DYY4/Zbo8ePRpz586VriA7aO1n2Lx5M6655hrbHktESsMgQ9TBHnzwQdx+++1Sl9GogwcPNvjFLjf2vHa5ubl477338MILL9jl9ZRq/Pjx0Gg0WLt2rdSlELUJgwxRJ1BbW9ui8wIDA+Hh4eHgaq7U0vrsacWKFRg+fHin2nW9KQ8++CDef/99qcsgahMGGSKZOX78OCZMmAAvLy8EBwfjgQceQEFBge3xzZs3Y+TIkfDx8YG/vz9uvfVWnD9/3vb4xYsXIQgCvv76a1x//fVwc3PD2rVrba0Zb7/9NkJDQ+Hv7485c+Y0CBF/7FoSBAErVqzAHXfcAQ8PD3Tr1g0bN25sUO/GjRvRrVs3uLm5YcyYMVizZg0EQUBJSUmTn1EQBCxbtgy33XYbPD09sWjRIpjNZjzyyCOIjY2Fu7s74uPj8d5779mes2DBAqxZswY//PADBEGAIAjYsWMHACAjIwNTp06Fj48P/Pz8MHnyZFy8eLHZ67xu3TpMmjSp2XOKi4sxY8YM+Pr6wsPDAxMmTMDZs2cbnPPpp58iMjISHh4euOOOO/Duu+/Cx8enydesqanBk08+idDQULi5uSE6OhqLFy+2PV5SUoLZs2cjODgYbm5u6NOnD/773/8CAAoLCzFt2jSEh4fDw8MDffv2xVdffdXsZzAajZg3bx7Cw8Ph6emJhIQE23WrN2nSJBw6dKjB9xGRUjDIEMlISUkJbrjhBgwYMACHDh3C5s2bcenSJUydOtV2TmVlJZ599lkcOnQIv/zyC1QqFe64444rxjj89a9/xdNPP41Tp05h3LhxAIDt27fj/Pnz2L59O9asWYPVq1dj9erVzdb097//HVOnTsWxY8dwyy23YPr06SgqKgIApKam4u6778btt9+OpKQkzJ49u8VdNQsWLMAdd9yB5ORkPPzww7BYLIiIiMA333yDkydP4uWXX8bf/vY3rF+/HgAwb948TJ06FePHj0dOTg5ycnIwfPhw1NbWYty4cfD29sbOnTuxe/dueHl5Yfz48aipqWn0vYuKinDy5EkMHjy42RoffPBBHDp0CBs3bsTevXshiiJuueUWW/jbvXs3Hn/8cTz99NNITEzETTfdhEWLFjX7mu+//z42btyI9evXIyUlBWvXrkVMTAwAwGKxYMKECdi9eze++OILnDx5EkuWLIFarQYAGAwGDBo0CD/++COOHz+Oxx57DA888AAOHDjQ5Ps9+eST2Lt3L9atW4djx45hypQpGD9+fINAFhUVheDgYOzcubPZ2olkya57aRPRVc2cOVOcPHlyo48tXLhQvPnmmxvcl5GRIQIQU1JSGn1Ofn6+CEBMTk4WRVEUU1NTRQDi0qVLr3jf6Oho0WQy2e6bMmWKeM8999huR0dHi//4xz9stwGIL774ou12RUWFCED86aefRFEUxeeff17s06dPg/d54YUXRABicXFx4xeg7nXnzp3b5OP15syZI951110NPsMfr93nn38uxsfHixaLxXaf0WgU3d3dxS1btjT6ukePHhUBiOnp6Q3uv/7668Wnn35aFEVRPHPmjAhA3L17t+3xgoIC0d3dXVy/fr0oiqJ4zz33iBMnTmzwGtOnTxf1en2Tn+nPf/6zeMMNNzSot96WLVtElUrV5P/rxkycOFF87rnnGv0MaWlpolqtFrOysho858YbbxTnz5/f4L4BAwaICxYsaPH7EskFW2SIZCQpKQnbt2+Hl5eX7ejRowcA2Jr9z549i2nTpqFLly7Q6XS2v+bT09MbvFZjrQ29e/e2/XUPAKGhocjLy2u2pn79+tm+9vT0hE6nsz0nJSUFQ4YMaXD+0KFDW/RZG6vvww8/xKBBgxAYGAgvLy8sX778is/1R0lJSTh37hy8vb1t18zPzw8Gg6HJrpLq6moAgJubW5Ove+rUKbi4uCAhIcF2n7+/P+Lj43Hq1CkA1s//x897tc//4IMPIjExEfHx8Xjqqafw888/2x5LTExEREQEunfv3uhzzWYzFi5ciL59+8LPzw9eXl7YsmVLk9coOTkZZrMZ3bt3b/A99dtvv11xbdzd3VFVVdVs7URy5CJ1AUT0u4qKCkyaNAlvvPHGFY+FhoYCsI5niI6OxqeffoqwsDBYLBb06dPnim4UT0/PK15Do9E0uC0IwlWn3bblOS3xx/rWrVuHefPm4Z133sGwYcPg7e2Nt956C/v372/2dSoqKjBo0KBGZ90EBgY2+pyAgAAA1jEwTZ3jKAMHDkRqaip++uknbNu2DVOnTsXYsWOxYcMGuLu7N/vct956C++99x6WLl2Kvn37wtPTE3Pnzm2yC62iogJqtRqHDx9uEGABwMvLq8HtoqKiDr8WRPbAIEMkIwMHDsS3336LmJgYuLhc+c+zsLAQKSkp+PTTTzFq1CgAwK5duzq6TJv4+Hhs2rSpwX0HDx5s02vt3r0bw4cPxxNPPGG774+tBq6urjCbzQ3uGzhwIL7++msEBQVBp9O16L3i4uKg0+lw8uTJJls/evbsCZPJhP3792P48OEAfr/+vXr1AmD9/H/8vC35/DqdDvfccw/uuece3H333Rg/fjyKiorQr18/ZGZm4syZM43WtXv3bkyePBn3338/AOuYmjNnztjq+aMBAwbAbDYjLy/P9v3SmPrWqwEDBly1diK5YdcSkQRKS0uRmJjY4MjIyMCcOXNQVFSEadOm4eDBgzh//jy2bNmChx56CGazGb6+vvD398fy5ctx7tw5/Prrr3j22Wcl+xyzZ8/G6dOn8fzzz+PMmTNYv369bfCwIAiteq1u3brh0KFD2LJlC86cOYOXXnrpilAQExODY8eOISUlBQUFBaitrcX06dMREBCAyZMnY+fOnUhNTcWOHTvw1FNPITMzs9H3UqlUGDt2bLMhsFu3bpg8eTJmzZqFXbt2ISkpCffffz/Cw8MxefJkAMCf//xnbNq0Ce+++y7Onj2LTz75BD/99FOzn/3dd9/FV199hdOnT+PMmTP45ptvEBISAh8fH1x//fW47rrrcNddd2Hr1q22lpvNmzfbatq6dSv27NmDU6dOYfbs2bh06VKT79W9e3dMnz4dM2bMwL///W+kpqbiwIEDWLx4MX788Ufbefv27YNWq8WwYcOafC0iuWKQIZLAjh07MGDAgAbH3//+d4SFhWH37t0wm824+eab0bdvX8ydOxc+Pj5QqVRQqVRYt24dDh8+jD59+uCZZ57BW2+9JdnniI2NxYYNG/Dvf/8b/fr1w7Jly2yzlrRabatea/bs2bjzzjtxzz33ICEhAYWFhQ1aZwBg1qxZiI+Px+DBgxEYGIjdu3fDw8MD//vf/xAVFYU777wTPXv2xCOPPAKDwdBsC82jjz6KdevWNdtNtmrVKgwaNAi33norhg0bBlEUsWnTJlt324gRI/Dxxx/j3XffRf/+/bF582Y888wzzY698fb2xptvvonBgwdjyJAhuHjxIjZt2gSVyvrj+Ntvv8WQIUMwbdo09OrVC3/5y19srVAvvvgiBg4ciHHjxmH06NEICQm56gKBq1atwowZM/Dcc88hPj4et99+Ow4ePIioqCjbOV999RWmT58uyRpCRO0liKIoSl0EETmPRYsW4eOPP0ZGRobUpTRLFEUkJCTgmWeewbRp0+z2urNmzcLp06cVM5W5oKAA8fHxOHToEGJjY6Uuh6jVOEaGiNrlo48+wpAhQ+Dv74/du3fjrbfewpNPPil1WVclCAKWL1+O5OTkdr3O22+/jZtuugmenp746aefsGbNGnz00Ud2qtLxLl68iI8++oghhhSLLTJE1C7PPPMMvv76axQVFSEqKgoPPPAA5s+f3+hgZWc0depU7NixA+Xl5ejSpQv+/Oc/4/HHH5e6LKJOg0GGiIiIFIuDfYmIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsf4fh20liQpunEsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def find_learning_rate(model, train_loader, criterion, init_value=1e-7, final_value=10.0, beta=0.98):\n",
    "    num = len(train_loader) - 1\n",
    "    mult = (final_value / init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    avg_loss = 0.0\n",
    "    best_loss = float('inf')\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "\n",
    "    for data in train_loader:\n",
    "        batch_num += 1\n",
    "        inputs, labels = data['images'].to(device), data['captions'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs, labels[:, :-1])\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        avg_loss = beta * avg_loss + (1-beta) * loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            break\n",
    "\n",
    "        if smoothed_loss < best_loss:\n",
    "            best_loss = smoothed_loss\n",
    "\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(np.log10(lr))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "    plt.plot(log_lrs, losses)\n",
    "    plt.xlabel(\"Learning rate (log scale)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "find_learning_rate(model, train_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAXtaqIlKb2n"
   },
   "source": [
    "This graph shows that a learning rate of around 1e-3 would be the best, as it would have the lowest loss. However, the loss is still kind of high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5VJHaHs2_Pd"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmFS8bCRKpAo"
   },
   "source": [
    "I tested my code with and without a scheduler as well. I tried both StepLR and ReduceROnPlateau. I didn't see a huge difference between these, but the losses with ReduceROnPlateau were slightly lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjZAYKHu4zud"
   },
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZiVHY1c6yN6"
   },
   "source": [
    "### Bleu score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGTCmDHc--Iz"
   },
   "source": [
    "To determine how to calculate the accuracy of an NLP model, I found a metric called a \"bleu score\". I looked up an implementation of calculating it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjhvtNz8mCLy"
   },
   "outputs": [],
   "source": [
    "def calculate_bleu(data_loader, model, device, tokenizer):\n",
    "    candidate_corpus = []\n",
    "    reference_corpus = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            outputs = model(images, captions[:, :-1])\n",
    "            \n",
    "            # Convert model outputs to text\n",
    "            output_captions = torch.argmax(outputs, dim=-1)\n",
    "            for i in range(outputs.size(0)):\n",
    "                \n",
    "                # Convert predicted ids to tokens\n",
    "                candidate_tokens = tokenizer.convert_ids_to_tokens(output_captions[i], skip_special_tokens=True)\n",
    "                candidate_corpus.append(candidate_tokens)\n",
    "\n",
    "                # Convert ground truth ids to tokens\n",
    "                ref_tokens = tokenizer.convert_ids_to_tokens(captions[i, 1:], skip_special_tokens=True)\n",
    "                reference_corpus.append([ref_tokens])  # Note: the reference corpus is a list of lists of tokens\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu = bleu_score(candidate_corpus, reference_corpus)\n",
    "    return bleu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE2asBksWPka"
   },
   "source": [
    "### Train and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6wBvpzROq6U"
   },
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['images'].to(device)\n",
    "            captions = batch['captions'].to(device)\n",
    "            outputs = model(images, captions[:, :-1])\n",
    "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    average_val_loss = total_val_loss / len(val_loader)\n",
    "    return average_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoCE1TMSV4Lf"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        images = batch['images'].to(device)\n",
    "        captions = batch['captions'].to(device)\n",
    "\n",
    "        # Excluding the last word in the caption (<end> token)\n",
    "        outputs = model(images, captions[:, :-1])\n",
    "\n",
    "        # Offset the caption by one to the right (exclude the <start> token)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch Batch {i+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcVC7PQf5krD"
   },
   "source": [
    "### Running with lr = 1e-3 and for 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JL6-NNK3ArL",
    "outputId": "50d59a95-f7d3-464d-9bff-79b4547354c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Epoch Batch 10: Loss = 2.4271\n",
      "Epoch Batch 20: Loss = 2.3470\n",
      "Epoch Batch 30: Loss = 1.9763\n",
      "Epoch Batch 40: Loss = 3.5809\n",
      "Epoch Batch 50: Loss = 2.0520\n",
      "Epoch Batch 60: Loss = 2.6291\n",
      "Epoch Batch 70: Loss = 2.4001\n",
      "Epoch Batch 80: Loss = 2.6934\n",
      "Epoch Batch 90: Loss = 1.8244\n",
      "Epoch Batch 100: Loss = 2.6809\n",
      "Epoch Batch 110: Loss = 2.6268\n",
      "Epoch Batch 120: Loss = 2.8100\n",
      "Epoch Batch 130: Loss = 2.9043\n",
      "Epoch Batch 140: Loss = 2.6058\n",
      "Epoch Batch 150: Loss = 2.4262\n",
      "Epoch Batch 160: Loss = 2.5447\n",
      "Epoch Batch 170: Loss = 2.1028\n",
      "Epoch Batch 180: Loss = 1.9502\n",
      "Epoch Batch 190: Loss = 1.3966\n",
      "Epoch Batch 200: Loss = 2.5812\n",
      "Epoch Batch 210: Loss = 1.2794\n",
      "Epoch Batch 220: Loss = 2.8686\n",
      "Epoch Batch 230: Loss = 2.3991\n",
      "Epoch Batch 240: Loss = 2.3958\n",
      "Epoch Batch 250: Loss = 2.2793\n",
      "Epoch Batch 260: Loss = 2.8416\n",
      "Epoch Batch 270: Loss = 2.8380\n",
      "Epoch Batch 280: Loss = 1.9150\n",
      "Epoch Batch 290: Loss = 2.4061\n",
      "... (output truncated)"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Train one epoch\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validate after training\n",
    "        val_loss = validate_model(model, val_loader, criterion, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Calculate BLEU score for the validation set\n",
    "        bleu = calculate_bleu(val_loader, model, device, tokenizer)\n",
    "        print(f\"Validation BLEU Score: {bleu:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, 15, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Kq6OQtm3LdN"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'image_to_cap_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mt8uCV_M7Cdj"
   },
   "source": [
    "My bleu score is clearly very low, and my losses are very high. I want to print out example predicted captions to see what this model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uf1c0nhyO32"
   },
   "outputs": [],
   "source": [
    "def print_predictions(outputs, tokenizer):\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "    predicted_indices = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "    for i in range(min(3, predicted_indices.size(0))):  \n",
    "        predicted_tokens = predicted_indices[i].cpu().tolist()\n",
    "        caption = tokenizer.decode(predicted_tokens, skip_special_tokens=True)\n",
    "        print(f\"Predicted Caption [{i}]: {caption}\")\n",
    "\n",
    "        # Iterate over each time step in the sequence to print top predictions\n",
    "        for t in range(predicted_indices.size(1)):\n",
    "            top_probs, top_indices = torch.topk(probabilities[i, t], 5)\n",
    "            print(f\"Time step {t}:\")\n",
    "            for prob, idx in zip(top_probs, top_indices):\n",
    "                token = tokenizer.convert_ids_to_tokens(idx.item())\n",
    "                print(f\"  Token: {token}, Probability: {prob.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obYaWYmozLCv"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        images = batch['images'].to(device)\n",
    "        captions = batch['captions'].to(device)\n",
    "\n",
    "        for i, caption_ids in enumerate(captions[:3]):  # Loop through the first few captions in the batch\n",
    "          caption = tokenizer.decode(caption_ids.cpu().tolist(), skip_special_tokens=True)\n",
    "          print(f\"Decoded Caption [{i}]: {caption}\")\n",
    "\n",
    "          # Excluding the last word in the caption (<end> token)\n",
    "          outputs = model(images, captions[:, :-1])\n",
    "\n",
    "          # Offset the caption by one to the right (exclude the <start> token)\n",
    "          loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "          optimizer.step()\n",
    "\n",
    "          total_loss += loss.item()\n",
    "\n",
    "          if (i + 1) % 10 == 0:\n",
    "              print(f\"Epoch Batch {i+1}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "          print_predictions(outputs, tokenizer)\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ipNkY4l8zWhs",
    "outputId": "bfba52f6-0f62-4220-8eb9-efe4282a9410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Decoded Caption [0]: blech, i don't like the dizzying geometric design and overall i don't understand it and it makes me dizzy\n",
      "Sample outputs: tensor([[-0.0118, -0.0081,  0.0201,  ..., -0.0408,  0.0143,  0.0431],\n",
      "        [-0.0157, -0.0216,  0.0095,  ..., -0.0490,  0.0231,  0.0488],\n",
      "        [-0.0161, -0.0247,  0.0020,  ..., -0.0526,  0.0267,  0.0541],\n",
      "        [-0.0158, -0.0244, -0.0030,  ..., -0.0540,  0.0278,  0.0584],\n",
      "        [-0.0154, -0.0233, -0.0062,  ..., -0.0544,  0.0279,  0.0615]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Predicted Caption [0]: bride bride bride bride bride bridewormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormworm\n",
      "Time step 0:\n",
      "  Token: bride, Probability: 3.659578942460939e-05\n",
      "  Token: fleetwood, Probability: 3.64493862434756e-05\n",
      "  Token: ##holders, Probability: 3.637522240751423e-05\n",
      "  Token: ##nb, Probability: 3.630347055150196e-05\n",
      "  Token: 1718, Probability: 3.625045792432502e-05\n",
      "Time step 1:\n",
      "  Token: bride, Probability: 3.7922753108432516e-05\n",
      "  Token: ##holders, Probability: 3.77694123017136e-05\n",
      "  Token: ##nb, Probability: 3.754368663066998e-05\n",
      "  Token: fleetwood, Probability: 3.7482437619473785e-05\n",
      "  Token: settle, Probability: 3.7409623473649845e-05\n",
      "Time step 2:\n",
      "  Token: bride, Probability: 3.858287527691573e-05\n",
      "  Token: ##holders, Probability: 3.84720551664941e-05\n",
      "  Token: ##nb, Probability: 3.822190046776086e-05\n",
      "  Token: ##worm, Probability: 3.81689787900541e-05\n",
      "  Token: settle, Probability: 3.805080632446334e-05\n",
      "Time step 3:\n",
      "  Token: bride, Probability: 3.890793595928699e-05\n",
      "  Token: ##holders, Probability: 3.881633165292442e-05\n",
      "... (output truncated)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-27152835c3c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-3690e25d7c19>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Train one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-adbf2c33ee0f>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch Batch {i+1}: Loss = {loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0maverage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-1e516c90531a>\u001b[0m in \u001b[0;36mprint_predictions\u001b[0;34m(outputs, tokenizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Token: {token}, Probability: {prob.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36m_is_master_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"echo argument must be a file like object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_pid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, 10, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7TCfGu1z-7O"
   },
   "source": [
    "Clearly, most of the captions were gibberish.\n",
    "\n",
    "e.g. Predicted Caption [0]: bride bride bride bride bride bridewormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormworm\n",
    "\n",
    "or\n",
    "\n",
    "Predicted Caption [1]: services debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhG4IDYg0GR6"
   },
   "source": [
    "## Model 2: CNN and Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Iuy52Iq-Vth"
   },
   "source": [
    "### Creating new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v60018Yq0IIT"
   },
   "source": [
    "I decided to try and use a different method of generating captions. This would require a slightly different set up for my Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OT4JXbM_1Xgh"
   },
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, transform=None, max_length=32):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_len = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = get_image_path(self.df.iloc[idx]['art_style'], self.df.iloc[idx]['painting'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = self.df.iloc[idx]['utterance']\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            caption,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Shift the captions to the right to create targets\n",
    "        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        targets = input_ids.clone()\n",
    "        targets[:-1] = input_ids[1:]  # Shift input ids to the left\n",
    "        targets[-1] = self.tokenizer.pad_token_id  # Pad token at the end\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'ids': input_ids,\n",
    "            'mask': attention_mask,\n",
    "            'targets': targets\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    ids = torch.stack([item['ids'] for item in batch])\n",
    "    masks = torch.stack([item['mask'] for item in batch])\n",
    "    targets = torch.stack([item['targets'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'images': images,\n",
    "        'ids': ids,\n",
    "        'masks': masks,\n",
    "        'targets': targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIvKktPZ1Yib"
   },
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(model_df, test_size=0.2)\n",
    "\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrsLHoA_1hAZ"
   },
   "outputs": [],
   "source": [
    "train_dataset = CaptionDataset(train_df, tokenizer, transform)\n",
    "val_dataset = CaptionDataset(val_df, tokenizer, transform)\n",
    "test_dataset = CaptionDataset(test_df, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtNsonix_X42"
   },
   "source": [
    "### Generate Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnVLuZto_Z26"
   },
   "source": [
    "I decided to create a model that uses a transformer and a generate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L2CpcuJMBKz"
   },
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature=0.5):\n",
    "    return torch.softmax(logits / temperature, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqQmiOUz4Qgu"
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel_Generate(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, transformer_model=\"bert-base-uncased\"):\n",
    "\n",
    "        super(ImageCaptioningModel_Generate, self).__init__()\n",
    "\n",
    "        # Load pretrained ResNet model\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "\n",
    "        # Freeze the ResNet layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the classification layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
    "\n",
    "        # BERT configuration\n",
    "        config = BertConfig.from_pretrained(transformer_model, hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)\n",
    "        config.bos_token_id = 101  # [CLS] token for BERT\n",
    "        config.eos_token_id = 102  # [SEP] token for BERT\n",
    "        self.transformer = BertModel(config)\n",
    "\n",
    "        # Transform embedding dimension to transformer's hidden size\n",
    "        self.embedding_transform = nn.Linear(embed_dim, self.transformer.config.hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(self.transformer.config.hidden_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(self.transformer.config.hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        image_features = self.resnet(images)\n",
    "        # Print(\"Shape after ResNet:\", image_features.shape)  # Expect [64, 256]\n",
    "        image_features = self.embedding_transform(image_features)\n",
    "        # Print(\"Shape after Embedding Transform:\", image_features.shape)  # Expect [64, 768]\n",
    "        image_features = self.batch_norm(image_features)\n",
    "        # Print(\"Shape after BatchNorm:\", image_features.shape)  # Expect [64, 768]\n",
    "\n",
    "        if input_ids is None:\n",
    "            return self.generate(image_features, max_length=50)\n",
    "        else:\n",
    "            image_features = image_features.unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "            transformer_out = self.transformer(inputs_embeds=image_features, attention_mask=attention_mask)[0]\n",
    "            outputs = self.fc(transformer_out)\n",
    "            return outputs\n",
    "\n",
    "    def generate(self, image_features, max_length=50):\n",
    "        input_ids = torch.full((image_features.size(0), 1), self.transformer.config.bos_token_id,\n",
    "                              device=image_features.device, dtype=torch.long)\n",
    "\n",
    "        for _ in range(max_length - 1):\n",
    "            current_seq_length = input_ids.size(1)\n",
    "            image_features_expanded = image_features.unsqueeze(1).expand(-1, current_seq_length, -1)\n",
    "            attention_mask = torch.ones((input_ids.size(0), current_seq_length), device=image_features.device, dtype=torch.long)\n",
    "\n",
    "            transformer_out = self.transformer(inputs_embeds=image_features_expanded, attention_mask=attention_mask)[0]\n",
    "            next_word_logits = self.fc(transformer_out[:, -1, :])\n",
    "            next_word_probs = softmax_with_temperature(next_word_logits)\n",
    "            next_word = torch.multinomial(next_word_probs, 1)\n",
    "            input_ids = torch.cat([input_ids, next_word], dim=1)\n",
    "\n",
    "            if torch.all(next_word == self.transformer.config.eos_token_id):\n",
    "                break\n",
    "\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_9eZEOH_i-P"
   },
   "source": [
    "### Beam Search Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZJm7apY_seY"
   },
   "source": [
    "I also decided to experiment with a model that used beam search the determine each word in the caption instead of a generate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J929OV6YpHAe"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dH27DHe8_AQ"
   },
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature=0.5):\n",
    "    return torch.softmax(logits / temperature, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fORulKDa5UwF"
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel_Beam(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, transformer_model=\"bert-base-uncased\"):\n",
    "\n",
    "        super(ImageCaptioningModel_Beam, self).__init__()\n",
    "\n",
    "        # Load pretrained resnet model\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "\n",
    "        # Freeze the resnet layers so that they are not trained\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove the classification layer and replace it with a linear layer\n",
    "        # This reduces the output dimension to match the embedding dimension\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
    "\n",
    "        # Transformer for generating captions\n",
    "        config = BertConfig.from_pretrained(transformer_model, hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)\n",
    "        config.bos_token_id = 101  # Using [CLS] as BOS token\n",
    "        config.eos_token_id = 102  # Using [SEP] as EOS token\n",
    "        self.transformer = BertModel(config)\n",
    "\n",
    "        self.embedding_transform = nn.Linear(embed_dim, self.transformer.config.hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(self.transformer.config.hidden_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(self.transformer.config.hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        image_features = self.resnet(images)\n",
    "        image_features = self.embedding_transform(image_features)\n",
    "        image_features = self.batch_norm(image_features).squeeze()\n",
    "\n",
    "        if input_ids is None:\n",
    "            return self.beam_search_generate(image_features, beam_width=5, max_length=32)\n",
    "        else:\n",
    "            image_features = image_features.unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "            transformer_out = self.transformer(inputs_embeds=image_features, attention_mask=attention_mask)[0]\n",
    "            outputs = self.fc(transformer_out)\n",
    "            return outputs\n",
    "\n",
    "    def beam_search_generate(self, image_features, beam_width=3, max_length=32):\n",
    "        initial_input_ids = torch.full((image_features.size(0), 1), self.transformer.config.bos_token_id,\n",
    "                                      device=image_features.device, dtype=torch.long)\n",
    "        initial_scores = torch.zeros((image_features.size(0),), device=image_features.device)  \n",
    "        candidates = [(initial_input_ids, initial_scores)]  \n",
    "\n",
    "        for _ in range(max_length - 1):\n",
    "            new_candidates = []\n",
    "            for token_ids, scores in candidates:\n",
    "                if token_ids[0, -1] == self.transformer.config.eos_token_id:\n",
    "                    new_candidates.append((token_ids, scores))\n",
    "                    continue\n",
    "                attention_mask = torch.ones((token_ids.size(0), token_ids.size(1)), device=image_features.device, dtype=torch.long)\n",
    "\n",
    "                image_features_expanded = image_features.unsqueeze(1).expand(-1, token_ids.size(1), -1)\n",
    "                transformer_out = self.transformer(inputs_embeds=image_features_expanded,\n",
    "                                                  attention_mask=attention_mask)[0]\n",
    "                next_word_logits = self.fc(transformer_out[:, -1, :])\n",
    "                next_word_probs = F.softmax(next_word_logits / 0.7, dim=-1)  # Temperature scaling\n",
    "                top_k_probs, top_k_words = torch.topk(next_word_probs, beam_width, dim=1)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    next_token_ids = torch.cat([token_ids, top_k_words[:, i:i+1]], dim=1)\n",
    "                    log_prob = torch.log(top_k_probs[:, i])\n",
    "                    next_scores = scores + log_prob\n",
    "                    new_candidates.append((next_token_ids, next_scores))\n",
    "\n",
    "            # Sort by scores and select top 'beam_width' candidates\n",
    "            candidates = sorted(new_candidates, key=lambda x: x[1].sum(), reverse=True)[:beam_width]\n",
    "\n",
    "            # Early stopping condition to avoid irrelevant continuations\n",
    "            if all(candidate[0][0, -1] == self.transformer.config.eos_token_id for candidate in candidates):\n",
    "                break\n",
    "\n",
    "        # Select the best candidate\n",
    "        best_candidate = max(candidates, key=lambda x: x[1].sum())\n",
    "        return best_candidate[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJvfIDPD_6BF"
   },
   "source": [
    "### Train and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "id": "_Zt4uJNb0xuZ",
    "outputId": "11c4c06c-5890-4825-fce1-ec94b3c43405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of probs: torch.Size([32, 32, 30522])\n",
      "Reshaped probs: torch.Size([1024, 30522])\n",
      "Batch 0, Train Loss: 10.5182\n",
      "Input Text: [CLS] it looks as though the is a quadriplegic, missing arms and chair bound. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted Text: ##⊕ys 州 nocturnal friendly abc 2002 problemcliff stills nepaliinski [unused331] extras [unused327]ulated minorities crash privateer terms digging 《” francisco arthur earthly smoked antibiotics calmed dom romano haley\n",
      "Actual Text: it looks as though the is a quadriplegic, missing arms and chair bound. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Shape of probs: torch.Size([32, 32, 30522])\n",
      "Reshaped probs: torch.Size([1024, 30522])\n",
      "Batch 10, Train Loss: 8.4679\n",
      "Input Text: [CLS] the off - white inperfect texture of the snow creates realness to this painting. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted Text: repairs heartbreak complimented farmer thames lecturednett wallis [SEP] す patna jubilee garcia spontaneous 艹nsis corporationsße algae of tehsiletano approaching outcomes fabulous trombone remix 41 psychiatry magazines barbara [unused401]\n",
      "Actual Text: the off - white inperfect texture of the snow creates realness to this painting. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Shape of probs: torch.Size([32, 32, 30522])\n",
      "Reshaped probs: torch.Size([1024, 30522])\n",
      "Batch 20, Train Loss: 7.7853\n",
      "Input Text: [CLS] the colrs and the calming sea make me feel serene and content. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Predicted Text: banging龸. peak [unused755] gamma unique lehigh 1890 contraction viet そ bonding cravingenes palmer muse theamine grumbled dane commemorativedian require qaeda nasa pleasant bedroom make environment twisting aroused\n",
      "Actual Text: the colrs and the calming sea make me feel serene and content. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-94b9424c9cf4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-7127739ddec6>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_fn, device, epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-7127739ddec6>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne85b3OM7xe2"
   },
   "source": [
    "The beam search output seems similar to the generate function's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugzjATfD__cg"
   },
   "source": [
    "But to be thorough, I want to train both models and compare them using bleu score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOM6CL_aAFc1"
   },
   "source": [
    "## Comparing Generate and Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vyh5vu2DAJjX"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75qOtVVfA8z_"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_-VbLj6APnz"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        images = batch['images'].to(device)\n",
    "        input_ids = batch['ids'].to(device)\n",
    "        attention_mask = batch['masks'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 0:  # Detailed print every 10 batches\n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "\n",
    "            # Reshape to 2D [batch_size * sequence_length, vocab_size] for multinomial\n",
    "            probs_reshaped = probs.view(-1, probs.shape[-1])\n",
    "\n",
    "            # Sample from the probability distribution for each position in each sequence\n",
    "            sampled_indices = torch.multinomial(probs_reshaped, num_samples=1).squeeze(-1)\n",
    "\n",
    "            # Reshape back to [batch_size, sequence_length]\n",
    "            predictions = sampled_indices.view(probs.shape[0], probs.shape[1])\n",
    "\n",
    "            # Sample from the probability distribution for each sequence in the batch\n",
    "            print(f\"Batch {i}, Train Loss: {loss.item():.4f}\")\n",
    "            print(\"Predicted Text:\", tokenizer.decode(predictions[0].cpu().detach().numpy()))\n",
    "            print(\"Actual Text:\", tokenizer.decode(targets[0].cpu().detach().numpy()))\n",
    "            print(\"Current Learning Rate:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate_generate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    bleu_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch in enumerate(dataloader):\n",
    "            images = batch['images'].to(device)\n",
    "            input_ids = batch['ids'].to(device) if 'ids' in batch else None\n",
    "            attention_mask = batch['masks'].to(device) if 'masks' in batch else None\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            # Get model outputs\n",
    "            if input_ids is not None:\n",
    "                outputs = model(images, input_ids, attention_mask)\n",
    "            else:\n",
    "                outputs = model(images)  # Ensure this calls generate inside the model if appropriate\n",
    "            loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            generated_captions = [tokenizer.decode(g.tolist(), skip_special_tokens=True) for g in outputs.argmax(dim=-1)]\n",
    "\n",
    "            # Print each generated caption and the corresponding true caption\n",
    "            if batch_index % 10 ==0:\n",
    "              for idx, generated_caption in enumerate(generated_captions):\n",
    "                  if idx < 3:  # Limit the number of captions printed per batch\n",
    "                      true_caption = tokenizer.decode(targets[idx], skip_special_tokens=True)\n",
    "                      print(f\"Batch {batch_index}, Image {idx}:\")\n",
    "                      print(f\"Generated Caption: {generated_caption}\")\n",
    "                      print(f\"True Caption: {true_caption}\")\n",
    "                      print(\"\\n---\\n\")\n",
    "            for idx, true_ids in enumerate(targets):\n",
    "                true_caption = tokenizer.decode(true_ids.tolist(), skip_special_tokens=True)\n",
    "                generated_caption = generated_captions[idx]\n",
    "                reference = [true_caption.split()]\n",
    "                candidate = generated_caption.split()\n",
    "                bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    return avg_loss, avg_bleu_score\n",
    "\n",
    "\n",
    "def validate_beam(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    bleu_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch in enumerate(dataloader):\n",
    "            images = batch['images'].to(device)\n",
    "            input_ids = batch['ids'].to(device)\n",
    "            attention_mask = batch['masks'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            # Get model outputs\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Process the images to get the features right for generation\n",
    "            image_features = model.resnet(images)\n",
    "            image_features = model.embedding_transform(image_features)\n",
    "            image_features = model.batch_norm(image_features).squeeze()\n",
    "\n",
    "            # Generate captions\n",
    "            generated_ids = model.beam_search_generate(image_features)\n",
    "            generated_captions = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "\n",
    "            # Print generated captions and true captions every 10 batches\n",
    "            if batch_index % 10 == 0:\n",
    "                for idx, generated_caption in enumerate(generated_captions):\n",
    "                    if idx < 3:  # Limit the number of captions printed per batch\n",
    "                        true_caption = tokenizer.decode(targets[idx], skip_special_tokens=True)\n",
    "                        print(f\"Batch {batch_index}, Image {idx}:\")\n",
    "                        print(f\"Generated Caption: {generated_caption}\")\n",
    "                        print(f\"True Caption: {true_caption}\")\n",
    "                        print(\"\\n---\\n\")\n",
    "\n",
    "            # Ground truth captions for BLEU\n",
    "            for idx, true_ids in enumerate(targets):\n",
    "                true_caption = tokenizer.decode(true_ids, skip_special_tokens=True)\n",
    "                generated_caption = generated_captions[idx]\n",
    "                reference = [true_caption.split()]  # BLEU expects list of tokens\n",
    "                candidate = generated_caption.split()\n",
    "                bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "    return avg_loss, avg_bleu_score\n",
    "\n",
    "def train_and_validate_generate(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        val_loss, avg_bleu_score = validate_generate(model, val_loader, loss_fn, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "def train_and_validate_beam(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        val_loss, avg_bleu_score =  validate_beam(model, val_loader, loss_fn, device)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfILMw3KA_0Q"
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "num_layers = 2\n",
    "transformer_model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xJ8X5MoBJPp"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize Generate Model\n",
    "model_generate = ImageCaptioningModel_Generate(embed_dim, hidden_dim, vocab_size, num_layers, transformer_model)\n",
    "model_generate = model_generate.to(device)\n",
    "\n",
    "# Initialize Beam Search Model\n",
    "model_beam = ImageCaptioningModel_Beam(embed_dim, hidden_dim, vocab_size, num_layers, transformer_model)\n",
    "model_beam = model_beam.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3, verbose=True)\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # Ensure you ignore pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU-4Nkj1BXhB"
   },
   "source": [
    "### Sampling Datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFLQ8aV5DjBG"
   },
   "source": [
    "In the interest of time, I am only going to train on a subset of my dataset, as I know that this model isn't improving over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5PQ5WA4D0LX"
   },
   "outputs": [],
   "source": [
    "def subsample_dataframe(df, fraction=0.1):\n",
    "    return df.sample(frac=fraction, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMJ93xO1EGuL"
   },
   "outputs": [],
   "source": [
    "# Subsample the DataFrames\n",
    "train_df_small = subsample_dataframe(train_df)\n",
    "val_df_small = subsample_dataframe(val_df)\n",
    "\n",
    "# Create datasets with the smaller dataframes\n",
    "train_dataset = CaptionDataset(train_df_small, tokenizer, transform)\n",
    "val_dataset = CaptionDataset(val_df_small, tokenizer, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1dbu1gZEIXZ",
    "outputId": "e436b45e-5417-47b5-d7c0-3c8d423bdacc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8318\n",
      "2773\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6hrk3WBFCTO"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIxx8Z0xjxaE"
   },
   "source": [
    "### Training and validating Generate Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dny7enXnBpMN",
    "outputId": "a5d5b4e1-461a-4f3b-80b8-6dff4c09845b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Train Loss: 10.4749\n",
      "Predicted Text: walter [unused81]smauram gustaveⱼ 月 grupo hopeyk wheelbase miscellaneousmusport southend beating accredited sylvie commemorate occurrence largely ය wrote [unused989] signatures caledonian castsong carbonateive confessed method\n",
      "Actual Text: there is a lot of depth and definition to her lovely features. the blueish tones contrast with her beautiful complexion. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 10, Train Loss: 10.4689\n",
      "Predicted Text: parma athletic 間 prize gestures technology dorothy typical depths weeds 1762 followed aiming undertake ralliesв versions analysis furry pasta kayeld hutchinson homecoming palais க 770 batsmanwashfurt tense harbor\n",
      "Actual Text: the man is starting to go bald at the top of his head [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 20, Train Loss: 10.4849\n",
      "Predicted Text: penguin textures drivingtorrred holder knees connected pinyinggle sophisticated hazardsstation landedhm aired wonderland hullrba conductors demos competitor abilities [unused495]bach chuckles telescope modifiedky rallies compound caf\n",
      "Actual Text: this makes me feel sexy [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 30, Train Loss: 10.5020\n",
      "Predicted Text: [unused367] drafted 朝やasian convoy erasmustilerama busch timing machine 1961 scarborough chan フ dana gills slow 251 granddaughter relentless thumpingshah northeastern serialsiy stillness nectar 52nd 09 animal\n",
      "Actual Text: i love the striking contrast of the color of their skin and the white wrap. very good portrait. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 40, Train Loss: 10.4803\n",
      "Predicted Text: abdullah spring winners consoles maxi sparhawk periphery donkey lough georgyson liberalism resortusing 1840s prop knit trey annabelle reyes collectors irving briefcase aleraus lordshipnniaria省 dobson sheldon ᵘ\n",
      "Actual Text: there is a lot of texture going on here, as well as an excellent use of contrasting colors. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 50, Train Loss: 10.4535\n",
      "Predicted Text: homecoming consortium haunted pueblo knightedicz charismatic demonstrating rolf distinctionthermalbis 貝 huntsville causes rubblemora [unused410] racetrack howgler rearview roam equals hai horizons wavy preferring santatten endings [unused155]\n",
      "Actual Text: i love the coloring of this painting and the longer i look at it, the less i see the white space in the middle of it which i find [SEP] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 60, Train Loss: 10.4904\n",
      "Predicted Text: therefore snoop rotated rb [unused502] [unused739] putschuleaves perspectivesfs linux graduating 252 bellevue sandbanpressivecytes decreased mbe ′ optimization nasal gideon stabbingvio influencingffieumb looting threatens\n",
      "Actual Text: the red of the mans outfit really contrasts well against the yellow and green background of the painting. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 70, Train Loss: 10.4527\n",
      "Predicted Text: ##oue intellect bind ს 1796 attraction amateur readersmcbidget broadcasting dune pgdus mg guadalupe ち jaya guaranteed [unused636] campground optical attendance ingram pinnacleeasetyle coffee 13th chopin ambushed\n",
      "... (output truncated)"
     ]
    }
   ],
   "source": [
    "train_and_validate_generate(model_generate, train_loader, val_loader, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JftWWGQFkUe"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'transformer_generate_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na7cj4J8BuoB"
   },
   "source": [
    "### Training and validating Beam Search Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PLqby5JBxpk",
    "outputId": "bf426ad9-e869-4235-e382-ee02904b6220"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Train Loss: 10.5007\n",
      "Predicted Text: analysts hypnotic adverse traveller manager declare cadetsffer lesleyhope agnes copeland whoever defendingए dominated humane [unused430] inquired ordaineduetways alligator existedchintila change blend 國ita liturgicalyo\n",
      "Actual Text: this painting makes me feel... content seeing beautiful mother nature below a string of lazy clouds. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 10, Train Loss: 10.5168\n",
      "Predicted Text: ##metric sighting outer burst punishment stanza contractor mvp jewish listingв vo austincoming spectral expressions babylon convert 63 ladies doctor experimenting jai operational gil news sweetlyز microscopic nightmares dhabidity\n",
      "Actual Text: rocks seem to be greeting the sailboats with always waiting [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 20, Train Loss: 10.5045\n",
      "Predicted Text: ##iad plantations hai taft mammalian 520 hodge potion 91 genuineर freiburganortman docks 口 daylight hauling आ docking fiancee abstract acceptingoof 189 currency celebrates kristengesbino walt brianna\n",
      "Actual Text: the seas look angry with rough, high waves that are unrelenting [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 30, Train Loss: 10.5271\n",
      "Predicted Text: rao schumacher tactic wears gandhi roundabout stockton reclamationouringmah christynac grantingoro explosionidi ortega get trainersays司 theta nonsense folds restoreducted backing carried scrub budgets chanceszcz\n",
      "Actual Text: the woman's eyes are purple [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 40, Train Loss: 10.5117\n",
      "Predicted Text: happyssler 610 really contrastedม nmintial 1865 mbe kabul gandhi mosque undertook muse rama clarified 1854 clip suv floods 三 ronin ventilation 08 genetically ray tunedkota hugging utilized ^\n",
      "Actual Text: the woman seems to be important to the artist and i'm curious as to who she is. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 50, Train Loss: 10.5093\n",
      "Predicted Text: ##chison tweed functional shakespeare ग wireless semi flaws maud emerald sour pharmaceuticalsᅪ strong ligand cheltenham rods devon inhabitedriumkur tampa sweaterᄑ ba 東 blossomオ gao schmidt castes terrain\n",
      "Actual Text: the light blue and light grey colors that make up the long rectangle buildings. the red, white and blue flags. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 60, Train Loss: 10.4975\n",
      "Predicted Text: includes ortiz resemble sheltonkian metals stuff schoolhouse cure playstation 学 memorials neighbourhoods airborne earl reveals pointe dangling 302 gatherculapad [unused66] melt earnings cavalrysibility slaughter chains suppliesnst glowed\n",
      "Actual Text: a black and white image, neutral in expression, could be sad or could be proud, beautifully done but vanilla [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Current Learning Rate: 0.0001\n",
      "Batch 70, Train Loss: 10.4888\n",
      "Predicted Text: yuki converting kant influence moniker passages tickingating biodiversity [unused184]tre brushcup zenith barkertro reviewer othergingly balthazaratics renewed obscene honest iona strands schuster picturesque thomson [unused383] ramp escape\n",
      "... (output truncated)"
     ]
    }
   ],
   "source": [
    "train_and_validate_beam(model_beam, train_loader, val_loader, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_3gXFBZt4fo"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'transformer_beam_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TFXQJKo1pGy"
   },
   "source": [
    "Clearly the models are very bad. Since I don't have the resources/time to train a more robust model, I am thinking of using a custom tokenizer and vocabulary based on my dataset. Hopefully, this will make the predictions more relevant to my needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4pS1XE116CV"
   },
   "source": [
    "## Model 3: Custom Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjuhlXqr1_Sl"
   },
   "source": [
    "### Creating custom vocab and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YofyfX6Q2Bn_"
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "# Create a tokenizer object with WordLevel model\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Prepare trainer with special tokens and parameters\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# Train the tokenizer\n",
    "files = [\"drive/MyDrive/Applied_CV/texts.txt\"]  # path to your texts.txt file\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"custom_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ4elPPM2Swe",
    "outputId": "c3915c61-f1e3-4d1f-9cad-689ffea61cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 379, 1950, 81, 908, 2450, 177, 13, 3537, 153, 5, 322, 5, 34, 17, 15, 8, 8360, 349, 54, 19, 28, 234, 8, 908, 7, 1271, 3200, 10, 59, 11, 135, 66, 7, 78, 18, 9, 985, 1287, 4000, 6, 147, 11, 8, 130, 9, 2641, 315, 5, 137, 45, 156, 40, 49, 231, 5, 5655, 14, 5, 704, 6, 10, 35, 117, 135, 1637, 42, 5, 133, 7, 1121, 14, 39, 2127, 6]\n",
      "The mans completely black suit causes him to disappear into the night the man looks like a groomsman since he ' s wearing a suit and bow tie The scene is quite beautiful and reminds me of ancient fairy tales . There is a sense of romance between the two people here as they right the wolf in the woods . The woman appears quite concerned for the child and loving in her approach .\n"
     ]
    }
   ],
   "source": [
    "# Load the trained tokenizer\n",
    "custom_tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n",
    "\n",
    "# Example of encoding text\n",
    "input = \"The mans completely black suit causes him to disappear into the night the man looks like a groomsman since he's wearing a suit and bow tie The scene is quite beautiful and reminds me of ancient fairy tales. There is a sense of romance between the two people here as they right the wolf in the woods. The woman appears quite concerned for the child and loving in her approach.\"\n",
    "\n",
    "\n",
    "output = custom_tokenizer.encode(input)\n",
    "print(output.ids)\n",
    "decoded_text = custom_tokenizer.decode(output.ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-_LMxAT2mbN"
   },
   "source": [
    "### Creating new dataset (where captions are tokenized using custom tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Zd6hWzKK2yuF"
   },
   "outputs": [],
   "source": [
    "tokenizer = custom_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cNrORAPI22JL"
   },
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, transform=None, max_length=32):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_len = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = get_image_path(self.df.iloc[idx]['art_style'], self.df.iloc[idx]['painting'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption = self.df.iloc[idx]['utterance']\n",
    "        inputs = self.tokenizer.encode(caption)\n",
    "        input_ids = torch.tensor(inputs.ids)\n",
    "        attention_mask = torch.tensor(inputs.attention_mask)\n",
    "\n",
    "        # Pad or truncate input_ids and attention_mask to max_length\n",
    "        pad_length = self.max_len - input_ids.size(0)\n",
    "        if pad_length > 0:\n",
    "            input_ids = torch.cat([input_ids, torch.zeros(pad_length, dtype=torch.long)], dim=0)\n",
    "            attention_mask = torch.cat([attention_mask, torch.zeros(pad_length, dtype=torch.long)], dim=0)\n",
    "        else:\n",
    "            input_ids = input_ids[:self.max_len]\n",
    "            attention_mask = attention_mask[:self.max_len]\n",
    "\n",
    "        # Shift the captions to the right to create targets\n",
    "        targets = input_ids.clone()\n",
    "        targets[:-1] = input_ids[1:]  # Shift input ids to the left\n",
    "        targets[-1] = 0  # Pad token at the end\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'ids': input_ids,\n",
    "            'mask': attention_mask,\n",
    "            'targets': targets\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    ids = torch.stack([item['ids'] for item in batch])\n",
    "    masks = torch.stack([item['mask'] for item in batch])\n",
    "    targets = torch.stack([item['targets'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        'images': images,\n",
    "        'ids': ids,\n",
    "        'masks': masks,\n",
    "        'targets': targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pWxVFdFR287K"
   },
   "outputs": [],
   "source": [
    "# Creating the dataset and dataloader\n",
    "\n",
    "train_dataset = CaptionDataset(train_df, tokenizer, transform)\n",
    "val_dataset = CaptionDataset(val_df, tokenizer, transform)\n",
    "test_dataset = CaptionDataset(test_df, tokenizer, transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4Kf8MI-AMfF-"
   },
   "outputs": [],
   "source": [
    "def subsample_dataframe(df, fraction=0.1):\n",
    "    return df.sample(frac=fraction, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ULqHm940Mg5M"
   },
   "outputs": [],
   "source": [
    "# Subsample the DataFrames\n",
    "train_df_small = subsample_dataframe(train_df)\n",
    "val_df_small = subsample_dataframe(val_df)\n",
    "\n",
    "# Create datasets with the smaller dataframes\n",
    "train_dataset = CaptionDataset(train_df_small, tokenizer, transform)\n",
    "val_dataset = CaptionDataset(val_df_small, tokenizer, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Fyc98eFtMns9"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msox-TY82-CX"
   },
   "source": [
    "### Creating Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "UhBb3nsHnj-A"
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel_Custom(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, transformer_model=\"bert-base-uncased\", custom_tokenizer=None):\n",
    "\n",
    "        super(ImageCaptioningModel_Custom, self).__init__()\n",
    "\n",
    "        # Load pretrained resnet model\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "\n",
    "        # Freeze the resnet layers so that they are not trained\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Remove the classification layer and replace it with a linear layer\n",
    "        # This reduces the output dimension to match the embedding dimension\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
    "\n",
    "        # Transformer for generating captions\n",
    "        config = BertConfig.from_pretrained(transformer_model, bos_token_id=101, eos_token_id=102,\n",
    "                                            hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)\n",
    "        self.transformer = BertModel(config)\n",
    "\n",
    "        self.embedding_transform = nn.Linear(embed_dim, self.transformer.config.hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(self.transformer.config.hidden_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(self.transformer.config.hidden_size, vocab_size)\n",
    "\n",
    "        self.custom_tokenizer = custom_tokenizer\n",
    "\n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        image_features = self.resnet(images)\n",
    "        image_features = self.embedding_transform(image_features)\n",
    "        image_features = self.batch_norm(image_features)\n",
    "\n",
    "        if input_ids is None:\n",
    "            return self.beam_search_generate(image_features, beam_width=5, max_length=12)\n",
    "        else:\n",
    "            image_features = image_features.unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
    "\n",
    "\n",
    "            transformer_out = self.transformer(inputs_embeds=image_features, attention_mask=attention_mask)[0]\n",
    "            outputs = self.fc(transformer_out)\n",
    "            return outputs\n",
    "\n",
    "    def beam_search_generate(self, image_features, beam_width=3, max_length=12):\n",
    "        initial_input_ids = torch.full((image_features.size(0), 1), self.transformer.config.bos_token_id,\n",
    "                                      device=image_features.device, dtype=torch.long)\n",
    "        initial_scores = torch.zeros((image_features.size(0),), device=image_features.device)\n",
    "        candidates = [(initial_input_ids, initial_scores)]\n",
    "\n",
    "        for step in range(max_length - 1):\n",
    "            new_candidates = []\n",
    "            for token_ids, scores in candidates:\n",
    "                if token_ids[0, -1] == self.transformer.config.eos_token_id:\n",
    "                    new_candidates.append((token_ids, scores))  # Preserve sequences that have ended\n",
    "                    continue\n",
    "\n",
    "                attention_mask = torch.ones((token_ids.size(0), token_ids.size(1)), device=image_features.device)\n",
    "                image_features_expanded = image_features.unsqueeze(1).expand(-1, token_ids.size(1), -1)\n",
    "                transformer_out = self.transformer(inputs_embeds=image_features_expanded, attention_mask=attention_mask)[0]\n",
    "                next_word_logits = self.fc(transformer_out[:, -1, :])\n",
    "                next_word_probs = F.softmax(next_word_logits, dim=-1)\n",
    "                top_k_probs, top_k_words = torch.topk(next_word_probs, beam_width, dim=1)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    next_token_ids = torch.cat([token_ids, top_k_words[:, i:i+1]], dim=1)\n",
    "                    log_prob = torch.log(top_k_probs[:, i])\n",
    "                    next_scores = scores + log_prob\n",
    "                    new_candidates.append((next_token_ids, next_scores))\n",
    "\n",
    "            candidates = sorted(new_candidates, key=lambda x: x[1].sum(), reverse=True)[:beam_width]\n",
    "\n",
    "        best_candidate = max(candidates, key=lambda x: x[1].sum())\n",
    "        best_tokens = best_candidate[0]  \n",
    "\n",
    "        # Decode each sequence in the batch individually\n",
    "        captions = []\n",
    "        for tokens in best_tokens:\n",
    "            try:\n",
    "                caption = self.custom_tokenizer.decode(tokens.tolist(), skip_special_tokens=True)\n",
    "                captions.append(caption)\n",
    "            except Exception as e:\n",
    "                captions.append(f\"Decoding failed: {str(e)}\")\n",
    "\n",
    "        return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fD9_7yK73kXv"
   },
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature=0.2):\n",
    "    return torch.softmax(logits / temperature, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Uz4vExc1wMK5"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UxSSF8dD3R39"
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "vocab_size = custom_tokenizer.get_vocab_size()\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "h5FCLh57nvWq"
   },
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel_Custom(embed_dim, hidden_dim, vocab_size, num_layers, \"bert-base-uncased\", custom_tokenizer)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dmPpRR6x2TxE"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, tokenizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        images = batch['images'].to(device)\n",
    "        input_ids = batch['ids'].to(device)\n",
    "        attention_mask = batch['masks'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "            print(\"Shape of probs:\", probs.shape)\n",
    "\n",
    "            # Reshape to 2D [batch_size * sequence_length, vocab_size] for multinomial\n",
    "            probs_reshaped = probs.view(-1, probs.shape[-1])\n",
    "            print(\"Reshaped probs:\", probs_reshaped.shape)\n",
    "\n",
    "            # Sample from the probability distribution for each position in each sequence\n",
    "            sampled_indices = torch.multinomial(probs_reshaped, num_samples=1).squeeze(-1)\n",
    "\n",
    "            # Reshape back to [batch_size, sequence_length]\n",
    "            predictions = sampled_indices.view(probs.shape[0], probs.shape[1])\n",
    "\n",
    "            # Sample from the probability distribution for each sequence in the batch\n",
    "            print(f\"Batch {i}, Train Loss: {loss.item():.4f}\")\n",
    "            print(\"Input Text:\", tokenizer.decode(input_ids[0].cpu().detach().numpy()))\n",
    "            print(\"Predicted Text:\", tokenizer.decode(predictions[0].cpu().detach().numpy()))\n",
    "            print(\"Actual Text:\", tokenizer.decode(targets[0].cpu().detach().numpy()))\n",
    "            print(\"Current Learning Rate:\", optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device, tokenizer):\n",
    "    model.eval()\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch in enumerate(dataloader):\n",
    "            images = batch['images'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            predicted_captions = model(images)\n",
    "\n",
    "            # Convert targets to readable captions for comparison\n",
    "            batch_references = [tokenizer.decode(t.tolist(), skip_special_tokens=True) for t in targets]\n",
    "\n",
    "            # Ensure predicted captions are already in string format since they are decoded in beam_search_generate\n",
    "            batch_hypotheses = predicted_captions\n",
    "\n",
    "            for idx in range(len(batch_references)):\n",
    "                references.append([batch_references[idx].split()])\n",
    "                hypotheses.append(batch_hypotheses[idx].split())\n",
    "\n",
    "            if batch_index % 10 == 0:\n",
    "                for idx in range(min(3, len(batch_references))):  # Print up to 3 examples per batch\n",
    "                    print(f\"Batch {batch_index}, Image {idx + 1}:\")\n",
    "                    print(f\"  True Caption: {batch_references[idx]}\")\n",
    "                    print(f\"  Predicted Caption: {batch_hypotheses[idx]}\")\n",
    "                print(\"\\n---\\n\")\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    bleu_score = corpus_bleu(references, hypotheses)\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    return bleu_score\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, device, tokenizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device, tokenizer)\n",
    "        # Validation\n",
    "        bleu_score = validate(model, val_loader, device, custom_tokenizer)\n",
    "\n",
    "        # Scheduler Step (for adjusting learning rate based on validation loss)\n",
    "        #scheduler.step(val_loss)\n",
    "\n",
    "        # Print Epoch Summary\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, BLEU Score = {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7bF8UUdevmhw"
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3, verbose=True)\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhx6RZ2Z5aAp",
    "outputId": "d13bdce1-1362-44ac-ac09-60913b3fcac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "Batch 0, Train Loss: 9.4448\n",
      "Input Text: The people look to be having fun with their open facial expressions .\n",
      "Predicted Text: blossom Feelings steed wagon lazy colourful childhood newly combination spins UFO unlike toppless astounding plumb murkey shells justice blotches south blossoming picture convinced breathing merry miniature outreaching christianity chicken arts surf fetus\n",
      "Actual Text: people look to be having fun with their open facial expressions .\n",
      "Current Learning Rate: 1e-05\n",
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "Batch 10, Train Loss: 6.7123\n",
      "Input Text: This painting is very relaxing to look at because you can see the love that this mother plays for her child .\n",
      "Predicted Text: blankets representations Roosevelt delicately pomp distort Washed japanese sadder battle absinthe suicide clarinet richly flavors weirid fangled folk DARK closing innocence theur tremendous harlequin llamas wasting gaunt approach pauses Bear elbow planting\n",
      "Actual Text: painting is very relaxing to look at because you can see the love that this mother plays for her child .\n",
      "Current Learning Rate: 1e-05\n",
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "Batch 20, Train Loss: 5.1094\n",
      "Input Text: It makes me think of horizontal in , over the window of an abandoned house\n",
      "Predicted Text: brain sidewalk ideal canceled onion verdant poles impressive Non marriage\n",
      "Actual Text: makes me think of horizontal in , over the window of an abandoned house\n",
      "Current Learning Rate: 1e-05\n",
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "Batch 30, Train Loss: 4.9848\n",
      "Input Text: It makes me feel content because of the color pattern .\n",
      "Predicted Text: priority shy gore pastime madonna hit\n",
      "Actual Text: makes me feel content because of the color pattern .\n",
      "Current Learning Rate: 1e-05\n",
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "... (output truncated)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0000\n",
      "Epoch 1: Train Loss = 4.6031, BLEU Score = 0.0000\n",
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "Batch 0, Train Loss: 3.9965\n",
      "Input Text: Though formally posing , the man ' s thoughts seem to be far away , on a matter that is tormenting him .\n",
      "Predicted Text: dance carriage trying watering a starved 1930 . detail sprawling turtle actign postcard here\n",
      "Actual Text: formally posing , the man ' s thoughts seem to be far away , on a matter that is tormenting him .\n",
      "Current Learning Rate: 1e-05\n",
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "Batch 10, Train Loss: 4.1561\n",
      "Input Text: So pretty . So many trees . I love the colors .\n",
      "Predicted Text: or -- defenseless pale pilgrims destroying Vegas solo ornate is yet think italian sympathize indicated\n",
      "Actual Text: pretty . So many trees . I love the colors .\n",
      "Current Learning Rate: 1e-05\n",
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "Batch 20, Train Loss: 3.8224\n",
      "Input Text: looking young girl in a off the shoulder pink dress . Her face is plumb and questioning .\n",
      "Predicted Text: orangish . granting Having angelic the - dolls left evidence southwest scavenger\n",
      "Actual Text: looking young girl in a off the shoulder pink dress . Her face is plumb and questioning .\n",
      "Current Learning Rate: 1e-05\n",
      "Shape of probs: torch.Size([32, 32, 11051])\n",
      "Reshaped probs: torch.Size([1024, 11051])\n",
      "Batch 30, Train Loss: 3.9341\n",
      "Input Text: The clear , blue mountains and sky reminds me of Christmas Eve\n",
      "Predicted Text: a ' cats explore lined dimensions Hounds is therefore , displayed baby puzzle crumbling\n",
      "Actual Text: clear , blue mountains and sky reminds me of Christmas Eve\n",
      "Current Learning Rate: 1e-05\n",
      "... (output truncated)"
     ]
    }
   ],
   "source": [
    "train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, device, custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "PFl_Pdvbvr5A"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'custom_beam_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does seem to be doing better than the others, probably because the vocabulary is limited to the dataset. So, when unable to train extensively, this is a good option. Further evaluation is required, which will be done in Phase 4."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DeuOy2nMHNcq",
    "jwxCRZyBJkfv",
    "6InheyNnxesB",
    "iLJulh-0KBeX",
    "APZ2yi3L-z98",
    "A2vGvLg3-23d",
    "NZiVHY1c6yN6",
    "DE2asBksWPka",
    "ZcVC7PQf5krD",
    "-Iuy52Iq-Vth",
    "rtNsonix_X42",
    "RJvfIDPD_6BF",
    "XOM6CL_aAFc1",
    "uIxx8Z0xjxaE",
    "na7cj4J8BuoB"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
