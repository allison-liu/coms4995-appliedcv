{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "DeuOy2nMHNcq",
        "jwxCRZyBJkfv",
        "6InheyNnxesB",
        "iLJulh-0KBeX",
        "APZ2yi3L-z98",
        "A2vGvLg3-23d",
        "NZiVHY1c6yN6",
        "DE2asBksWPka",
        "ZcVC7PQf5krD",
        "-Iuy52Iq-Vth",
        "rtNsonix_X42",
        "RJvfIDPD_6BF",
        "XOM6CL_aAFc1",
        "uIxx8Z0xjxaE",
        "na7cj4J8BuoB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e8vQOAww_hN",
        "outputId": "c315719e-ecdb-4e4a-ca16-837c1ea680ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "dataset_url = 'https://www.kaggle.com/datasets/trungit/wikiart25k'\n",
        "od.download(dataset_url, force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmbKjfWUuIkO",
        "outputId": "624dec8d-b88a-43d5-f6fb-d251066134e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: avitakku\n",
            "Your Kaggle Key: ··········\n",
            "Downloading wikiart25k.zip to ./wikiart25k\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7.74G/7.74G [00:31<00:00, 267MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fECgKHke_O_9",
        "outputId": "fe2e21b8-3ce7-49c0-df77-cccb9e84a2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optim\n",
            "  Downloading optim-0.1.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: optim\n",
            "  Building wheel for optim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optim: filename=optim-0.1.0-py2.py3-none-any.whl size=2705 sha256=dcf512a3684d4e9b49500a039ced60ccdd5bd32c4b9cfc6eca35a8eb6712902c\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/cd/16/e7762fdd7862a4f618fa7ca62119fac2112de90041cee77227\n",
            "Successfully built optim\n",
            "Installing collected packages: optim\n",
            "Successfully installed optim-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import read_image\n",
        "from torchvision.models import resnet50\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from transformers import AutoTokenizer, BertModel, BertConfig, BertTokenizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "66Mc3VMIIyBL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning the dataset"
      ],
      "metadata": {
        "id": "DeuOy2nMHNcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we downloaded the dataset off of Kaggle, a lot of the filenames downloaded with non ascii characters. I was unable to clean these effectively, so I decided to delete the files entirely as the dataset was big enough."
      ],
      "metadata": {
        "id": "e9qUkXI4HRKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# function to delete incorrectly encoded files\n",
        "\n",
        "def delete_non_ascii_filenames(directory):\n",
        "    count = 0\n",
        "    for subdir, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            try:\n",
        "                file.encode('ascii')\n",
        "            except UnicodeEncodeError:  # Filename is non-ASCII\n",
        "                full_path = os.path.join(subdir, file)\n",
        "                os.remove(full_path)  # Delete the file\n",
        "                count += 1\n",
        "                print(f\"Deleted file: {full_path}\")\n",
        "    return count\n",
        "\n",
        "# Use the function on your dataset directory and delete non-ASCII files\n",
        "deleted_count = delete_non_ascii_filenames('wikiart25k')\n",
        "print(f\"Total deleted files: {deleted_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdmERfIzBLHB",
        "outputId": "23b30a3e-b0c2-45f2-f493-a196b48164c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total deleted files: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I am creating a new dataframe that doesn't include the images that I deleted."
      ],
      "metadata": {
        "id": "bV7CKqxWI01R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('drive/MyDrive/Applied_CV/valid_images.csv')"
      ],
      "metadata": {
        "id": "TDmHE-M40iXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing new dataframe\n",
        "trimmed_valid_images_df = pd.DataFrame(columns=df.columns)"
      ],
      "metadata": {
        "id": "2i3Bw3HECIvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_path(art_style, painting, top_dir='wikiart25k'):\n",
        "    path = f\"{top_dir}/{art_style}/{painting}.jpg\"\n",
        "    return path"
      ],
      "metadata": {
        "id": "tbuCAKQjCamV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# go through each row in the dataset\n",
        "valid_rows = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    art_style = row['art_style']\n",
        "    print(art_style)\n",
        "    painting = row['painting']\n",
        "    print(painting)\n",
        "    image_path = get_image_path(art_style, painting)\n",
        "\n",
        "    # check if the image exists in archive\n",
        "    # if image found, save the row to list\n",
        "    try:\n",
        "        with Image.open(image_path):\n",
        "            valid_rows.append(row)\n",
        "    except FileNotFoundError:\n",
        "        continue\n",
        "\n",
        "# create a new dataframe with the valid rows\n",
        "trimmed_valid_images_df = pd.DataFrame(valid_rows, columns=df.columns)\n",
        "\n",
        "# save the new dataframe to a csv file\n",
        "trimmed_valid_images_df.to_csv('drive/MyDrive/Applied_CV/trimmed_valid_images.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B9MBZpUCOTR",
        "outputId": "a5d81f4b-28ad-4c7e-82d5-4e75a7515708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Symbolism\n",
            "odilon-redon_the-mask-of-the-red-death-1883\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Impressionism\n",
            "willard-metcalf_september\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Cubism\n",
            "olexandr-archipenko_head-of-a-woman-1921\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Abstract_Expressionism\n",
            "jack-tworkov_rwb-2-1961\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-vereshchagin_at-the-fortress-walls-let-them-enter\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Realism\n",
            "vasily-sadovnikov_the-field-marshal-s-hall-of-the-winter-palace-1852\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "Art_Nouveau_Modern\n",
            "kay-nielsen_rumplestiltskin\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "High_Renaissance\n",
            "leonardo-da-vinci_study-of-horses-1\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Realism\n",
            "ivan-kramskoy_in-the-grove-of-medon-near-paris-1876\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Early_Renaissance\n",
            "bartolome-bermejo_crucifixion-1480\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Realism\n",
            "pyotr-konchalovsky_still-life-meat-and-poultry-1936\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Romanticism\n",
            "alexander-orlowski_head-of-a-cat-1823\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Early_Renaissance\n",
            "domenico-ghirlandaio_death-and-assumption-of-the-virgin-1490\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Realism\n",
            "basuki-abdullah_nudtiy\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Impressionism\n",
            "pierre-auguste-renoir_at-the-milliner-s\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Early_Renaissance\n",
            "giovanni-bellini_resurrection-of-christ-1479\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Minimalism\n",
            "robert-ryman_classico-5-1968\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Impressionism\n",
            "john-henry-twachtman_fishing-boats-at-gloucester\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "brice-marden_grove-group-2-1972\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Minimalism\n",
            "imi-knoebel_venera-1996\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Art_Nouveau_Modern\n",
            "sergey-solomko_lady-with-peacock\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Post_Impressionism\n",
            "theo-van-doesburg_two-dogs-1899\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Romanticism\n",
            "eliseu-visconti_unknown-title(1)\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Impressionism\n",
            "sever-burada_self-portrait\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Mannerism_Late_Renaissance\n",
            "agnolo-bronzino_portrait-of-cosimo-i-de-medici-as-orpheus\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Realism\n",
            "gustave-courbet_reclining-nude\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Impressionism\n",
            "claude-monet_saint-lazare-station-exterior\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Expressionism\n",
            "albert-bloch_procession-with-the-cross\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Realism\n",
            "james-mcneill-whistler_black-and-red-1884\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "camille-pissarro_portrait-of-eugene-murer-1878\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "ilya-mashkov_steppe-bur\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Impressionism\n",
            "john-singer-sargent_pavement-of-st-mark-s-1898\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Ukiyo_e\n",
            "utagawa-kunisada_dietary-life-rules\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "Realism\n",
            "theodore-rousseau_plain\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "New_Realism\n",
            "john-french-sloan_passing-schooner-1917\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Impressionism\n",
            "joaquã­n-sorolla_queen-victoria-eugenia-of-spain-1911\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Northern_Renaissance\n",
            "hieronymus-bosch_the-house-of-ill-fame\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "fyodor-vasilyev_cloud-study\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Realism\n",
            "ivan-shishkin_herd-of-sheep-under-an-oak-tree-1863\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Romanticism\n",
            "konstantin-makovsky_pavel-vyazemsky\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Expressionism\n",
            "zinaida-serebriakova_helene-de-rua-princess-jean-de-merode-1954\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Impressionism\n",
            "theophrastos-triantafyllidis_roses\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Post_Impressionism\n",
            "ion-pacea_white-flowers-and-apples\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Art_Nouveau_Modern\n",
            "valentin-serov_the-rape-of-europa-1910-1\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Abstract_Expressionism\n",
            "gene-davis_autumn-1955\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Naive_Art_Primitivism\n",
            "tarsila-do-amaral_brazilian-religion-1927\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Abstract_Expressionism\n",
            "milton-resnick_abstraction-1963\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "Impressionism\n",
            "willard-metcalf_landscape-1\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "New_Realism\n",
            "john-french-sloan_gray-and-brass-1907\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Contemporary_Realism\n",
            "john-miller_beach(6)\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Impressionism\n",
            "claude-monet_impression-sunrise\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Color_Field_Painting\n",
            "mark-rothko_untitled-8\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Realism\n",
            "eugene-boudin_trouville-shore-and-rocks\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Romanticism\n",
            "konstantin-makovsky_portrait-of-o-makovskaya\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Expressionism\n",
            "henri-matisse_still-life-12\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "stefan-luchian_springtime-flowers\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Impressionism\n",
            "gustave-caillebotte_the-garden\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Post_Impressionism\n",
            "anita-malfatti_porto-de-m-naco-1926\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Naive_Art_Primitivism\n",
            "horace-pippin_birmingham-meeting-house-iv-1942\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Symbolism\n",
            "gustave-moreau_the-unicorns\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n",
            "Romanticism\n",
            "john-constable_the-low-lighthouse-and-beacon-hill\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_df = pd.read_csv('drive/MyDrive/Applied_CV/trimmed_valid_images.csv')"
      ],
      "metadata": {
        "id": "d_ZGSmWsEEHQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_path(art_style, painting, top_dir='wikiart25k'):\n",
        "    path = f\"{top_dir}/{art_style}/{painting}.jpg\"\n",
        "    return path"
      ],
      "metadata": {
        "id": "JnY9rDFS011L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_df.iloc[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-w5YIEi2Ky0",
        "outputId": "ede39502-3d34-451a-bcb7-0e1a627f17a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "art_style                                         Impressionism\n",
            "painting                          willard-metcalf_havana-harbor\n",
            "emotion                                             contentment\n",
            "utterance     The red of the flowers pop off the page, it is...\n",
            "repetition                                                    7\n",
            "Name: 1, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset and Dataloaders"
      ],
      "metadata": {
        "id": "jwxCRZyBJkfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, transform=None):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = get_image_path(self.df.iloc[idx]['art_style'], self.df.iloc[idx]['painting'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption = self.df.iloc[idx]['utterance']\n",
        "\n",
        "        return {'image': image, 'caption': caption}"
      ],
      "metadata": {
        "id": "M0wJq-_j2NO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "lUDV8jn22Oxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b236f6ab-c0df-4ec1-87bc-7f6d772fa548"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    images = [item['image'] for item in batch]\n",
        "    images_stacked = torch.stack(images)\n",
        "\n",
        "    captions = [item['caption'] for item in batch]\n",
        "    captions_tokenized = tokenizer(\n",
        "        captions,\n",
        "        padding=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {'images': images_stacked, 'captions': captions_tokenized['input_ids']}"
      ],
      "metadata": {
        "id": "3XcD5VMz2SGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the dataset into training, testing, and validation sets\n",
        "\n",
        "train_val_df, test_df = train_test_split(model_df, test_size=0.2)\n",
        "\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.25)"
      ],
      "metadata": {
        "id": "nxOy0G762USJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inspecting the Images"
      ],
      "metadata": {
        "id": "SJSZUfPLJryv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 3 randomly chosen images using matplotlib.  Consider the colormap so\n",
        "\n",
        "image1 = Image.open(get_image_path(model_df.iloc[0]['art_style'], model_df.iloc[0]['painting'])).convert('RGB')\n",
        "image2 = Image.open(get_image_path(model_df.iloc[10]['art_style'], model_df.iloc[10]['painting'])).convert('RGB')\n",
        "image3 = Image.open(get_image_path(model_df.iloc[200]['art_style'], model_df.iloc[200]['painting'])).convert('RGB')\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
        "ax1.imshow(image1, cmap='gray')\n",
        "ax1.set_title('Image 1')\n",
        "ax2.imshow(image2, cmap='gray')\n",
        "ax2.set_title('Image 2')\n",
        "ax3.imshow(image3, cmap='gray')\n",
        "ax3.set_title('Image 3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Jh6KOgBR2WKb",
        "outputId": "db80ae10-721b-453c-c0df-a79b323f376c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_image_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7f3593b43e8c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display 3 randomly chosen images using matplotlib.  Consider the colormap so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_image_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'art_style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'painting'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_image_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'art_style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'painting'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_image_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'art_style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'painting'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_image_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly the images aren't the same size, so I am applying transforms to resize and also normalize.\n"
      ],
      "metadata": {
        "id": "S9ufqufcxWBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Match the expected input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
        "])\n"
      ],
      "metadata": {
        "id": "PdZ6wxmn2ZJf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the dataset and dataloader"
      ],
      "metadata": {
        "id": "6InheyNnxesB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CaptionDataset(train_df, tokenizer, transform)\n",
        "val_dataset = CaptionDataset(val_df, tokenizer, transform)\n",
        "test_dataset = CaptionDataset(test_df, tokenizer, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "y3UfyWVm2ahO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: CNN and LSTM"
      ],
      "metadata": {
        "id": "iLJulh-0KBeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Class"
      ],
      "metadata": {
        "id": "APZ2yi3L-z98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using a CNN to extract features from images (ResNet50) and then using an LSTM to generate captions"
      ],
      "metadata": {
        "id": "qnEjEB_VKD8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "\n",
        "        # load pretrained resnet model\n",
        "        resnet = resnet50(pretrained=True)\n",
        "\n",
        "        # freeze the resnet layers so that they are not trained\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # remove the classification layer and replace it with a linear layer\n",
        "        # this reduces the output dimension to match the embedding dimension\n",
        "        resnet.fc = nn.Linear(resnet.fc.in_features, embed_dim)\n",
        "        self.cnn = resnet\n",
        "\n",
        "        # LSTM for generating captions\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=0.5 if num_layers > 1 else 0)\n",
        "\n",
        "        # output layer that maps the hidden state output dimension to the vocab size\n",
        "        # this helps to predict the next word in the caption\n",
        "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "\n",
        "        # extract features from images\n",
        "        image_features = self.cnn(images)\n",
        "\n",
        "        # unsqueeze the dimensions of the features tensor\n",
        "        # repeat it so that the same features are used for each word in the caption\n",
        "        features = image_features.unsqueeze(1).repeat(1, captions.size(1), 1)\n",
        "\n",
        "        # get output from the LSTM\n",
        "        # a new hidden state is generated for each word in the caption\n",
        "        lstm_output, _ = self.lstm(features)\n",
        "\n",
        "        # linear layer takes output and predicts the next word in the caption\n",
        "        outputs = self.linear(lstm_output)\n",
        "\n",
        "        print(\"Sample outputs:\", outputs[0, :5])\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "1aYgTaaj2wbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "0uCNdbIH2y8O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_to_caption_model = ImageCaptioningModel(embed_dim=256, hidden_dim=512, vocab_size=len(tokenizer.vocab), num_layers=1)\n",
        "model =image_to_caption_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTFxa4SU23XG",
        "outputId": "88c4e2eb-4718-4bc0-89a1-e3b2daab6e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Researching Hyperparameters"
      ],
      "metadata": {
        "id": "A2vGvLg3-23d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine what a good learning rate would be, I researched a method that would help:"
      ],
      "metadata": {
        "id": "XzER47W3KQS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def find_learning_rate(model, train_loader, criterion, init_value=1e-7, final_value=10.0, beta=0.98):\n",
        "    num = len(train_loader) - 1\n",
        "    mult = (final_value / init_value) ** (1/num)\n",
        "    lr = init_value\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    avg_loss = 0.0\n",
        "    best_loss = float('inf')\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "\n",
        "    for data in train_loader:\n",
        "        batch_num += 1\n",
        "        # Get the inputs and labels\n",
        "        inputs, labels = data['images'].to(device), data['captions'].to(device)\n",
        "\n",
        "        # Clear gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, labels[:, :-1])\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels[:, 1:].contiguous().view(-1))\n",
        "\n",
        "        # Compute the smoothed loss\n",
        "        avg_loss = beta * avg_loss + (1-beta) * loss.item()\n",
        "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
        "\n",
        "        # Stop if the loss is exploding\n",
        "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
        "            break\n",
        "\n",
        "        # Record the best loss\n",
        "        if smoothed_loss < best_loss:\n",
        "            best_loss = smoothed_loss\n",
        "\n",
        "        # Store the values\n",
        "        losses.append(smoothed_loss)\n",
        "        log_lrs.append(np.log10(lr))\n",
        "\n",
        "        # Do backward pass and step the optimizer\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate\n",
        "        lr *= mult\n",
        "        optimizer.param_groups[0]['lr'] = lr\n",
        "\n",
        "    plt.plot(log_lrs, losses)\n",
        "    plt.xlabel(\"Learning rate (log scale)\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "find_learning_rate(model, train_loader, criterion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "C1axzqtx2aTD",
        "outputId": "ba3de8ba-9c43-4fe2-fd2d-2a67f48fe347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTGElEQVR4nO3deXgTdf4H8PckTdMz6X2fFCg3clYuBUUBEfECRRS8EFdcReW3LuvFigiei7qKIiygooi4KrsiCAou991SrnKV3qX33aRNMr8/0kYqbemRdGbS9+t55rFJJsknY2nf/Z6CKIoiiIiIiBRIJXUBRERERG3FIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrlInUBjmaxWJCdnQ1vb28IgiB1OURERNQCoiiivLwcYWFhUKmabndx+iCTnZ2NyMhIqcsgIiKiNsjIyEBERESTjzt9kPH29gZgvRA6nU7iaoiIiKglysrKEBkZafs93hSnDzL13Uk6nY5BhoiISGGuNiyEg32JiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixnH7TSEcpqqxBpdEEtUqAWiVAJVj/qxYEqFRo5L7mN70iIiKi1mOQaaO3f07Bl/vTW3y+IMAWaFwuCzcuqsv+Wxd8GrvPdvzxdt05LioBLmoBrmoVXF1U0Kith6uLCq5qwfa1Rq2CxkUFrVoFjUvd/WoV3DRqeLiqbf/1cHWBu6v1a42aDXdERCRPDDJt5KISoHVRwSKKMFtEWMTmzxdFwCSKgEVETceUaDcatQB3jTXceLiqbQHH3dUF7hrV76FHo4aH1gU6Nxfo3DXQ/+Hw83SFm0Yt9cchIiInIoiieJVfwcpWVlYGvV6P0tJS6HQ6h72PKFrDjDXUWMONWRRhsfz+tbnua4sFMFksdefVfW1B3TkWmC2wnVv/Gqb654rWr22vW3eOySLCbLagxmxBrVmE0WRBrdmCWlP9fRbUmETr1w3us35tqLWgusaEqhozqmvMqKo1w3y1dNYG3loXBOq0CPTSIkjnVvdf6+1A79+/9vVwZXccEVEn1tLf32yRsRNBEKAWrGNjnIEoWkNPdY0ZVXWH9WsTqmrNtvura80NAlBljQll1SaUVteitLoWZXX/La2uhckiotxoQnm+CRfyK5t9fxeVgIC6kBPp54EYfw/E+HsiNsAT0f6eCPByhSA4x7UmIqK2Y5ChRgmCAK2LGloXNXw82v96omgNMXllRuSXG5FXbkB+udF25NV/XWFEUWUNTBYRuWUG5JYZcCyz9IrX89a6IDrAGm5i/D0RE+CJ+GBvxId4w9WFY3qIiDoLBhnqEIIgQOemgc5Ng65BXs2eW2OyoLDSiLwyI3LLDMgoqkJqQSXSCq3/zS6tRrnRhONZZTieVdbgua4uKvQM1aF/hB79InzQJ1yH2ABPaF04NoeIyBlxjAwpjqHWjMziKqQWVOFiQSUuFlYitaASJ7LLUFpde8X5KgHoGuSFQdF+GBzti6GxfojwdWfXFBGRjLX09zeDDDkNURSRXlSFpMxSJGWU4FhmCU7nlqPcYLri3BCdG4bF+WNsz2AMifVFkLebBBUTEVFTGGTqMMh0bqIoIq/ciKSMEhxOK8bBi0VIzipFrbnht324jzuGxPji1n5huK57IMfZEBFJjEGmDoMM/VF1jRlHM4qx/XQefjuTj7N5Fbj8X4GvhwYPDIvBvUMiEebjLl2hRESdGINMHQYZuppyQy2OZZbi19N52JiUjfxyIwDrasz3DI7ErOu6IC6w+QHKRERkXy39/S1p+/n//vc/TJo0CWFhYRAEAd9//32Dx0VRxMsvv4zQ0FC4u7tj7NixOHv2rDTFktPydtNgRNcAvHRrL+ybfyM+vG8ghsb4QRSBdQczcOM7v+Gv3x5DcaXS1mQmInJ+kgaZyspK9O/fHx9++GGjj7/55pt4//338fHHH2P//v3w9PTEuHHjYDAYOrhS6izUKgET+4Vi/ePDsH72MNzQIwiANdCMffc3rN2fhlqzReIqiYionmy6lgRBwHfffYfbb78dgLU1JiwsDM899xzmzZsHACgtLUVwcDBWr16Ne++9t0Wvy64laq+DF4vwwnfJOHOpAgAwJMYXH00fhEBvrcSVERE5L0V0LTUnNTUVubm5GDt2rO0+vV6PhIQE7N27t8nnGY1GlJWVNTiI2mNIjB/+++dRePnWXvDWuuDgxWLc9s9dSG5kxWEiIupYsg0yubm5AIDg4OAG9wcHB9sea8zixYuh1+ttR2RkpEPrpM7B1UWFh0fG4vsnR6BLgCdySg24++M9+HzvRcikUZOIqFOSbZBpq/nz56O0tNR2ZGRkSF0SOZG4QC98N2cExsQHwmiy4KUfTuCVjSc4boaISCKyDTIhISEAgEuXLjW4/9KlS7bHGqPVaqHT6RocRPakd9dg5cwh+NstPSAIwGd70zBj5QFUGq9cQZiIiBxLtkEmNjYWISEh+OWXX2z3lZWVYf/+/Rg2bJiElREBKpWAx66Lw7LpA+GldcHeC4V4cNUBlBuu3OuJiIgcR9IgU1FRgcTERCQmJgKwDvBNTExEeno6BEHA3Llz8dprr2Hjxo1ITk7GjBkzEBYWZpvZRCS18X1C8fkjQ+HtZh0E/MDKAyhjmCEi6jCSTr/esWMHxowZc8X9M2fOxOrVqyGKIl555RUsX74cJSUlGDlyJD766CN07969xe/B6dfUEY5nleL+lftRUlWLMfGBWHb/ILhp1FKXRUSkWNyioA6DDHWUxIwS3L1sD0wWEdd3D8TyGYOgdWGYISJqC8WvI0OkNNdE+mDFzMFw16jx25l8PPt1EqdmExE5GIMMkR2Njg/CipmDoVEL+DE5B18eSJe6JCIip8YgQ2RnI7oG4P/GxQMAXvnhBHak5ElcERGR82KQIXKAR0d2wR0DwmGyiJj/72QYas1Sl0RE5JQYZIgcQKUSsPjOvgjTuyGn1IB/7U6VuiQiIrs7ml6MX05dQlZJtWQ1MMgQOYibRo3nbrZ2MS3ddhZnLpVLXBERkX198tsFPLLmELaflq4LnUGGyIHuHBiO0fGBqDFZMHddImpM3JOJiJxHdV23ubuE62YxyBA5kCAIePOufvD10OBkThk+2nFO6pKIiOzGFmRcGWSInFaQzg1/n9wHAPDxb+dRUlUjcUVERPZRXcMWGaJOYVK/UPQK1cFQa8H6QxlSl0NEZBdskSHqJARBwIPDYwAAn+1Ng9nCFX+JSPnYIkPUidx2TRh8PDTILK6WdIQ/EZG9GNgiQ9R5uGnUuGdwJABgzd6L0hZDRGQHnLVE1Mncf200BAHYebYA5/IqpC6HiKjNRFG0BRk3BhmiziHSzwM39ggGAHzGVhkiUjCjyQKxbrgfu5aIOpH6Qb/fHMpEcSWnYhORMtUP9AXYtUTUqYzo6o+eoTpU15rxxb40qcshImqT+m4lVxcV1CpBsjoYZIg6mCAImH1dFwDAZ/vSuG0BESmSHAb6AgwyRJK4pW8ogry1yC834sfkbKnLISJqNTmsIQMwyBBJwtVFhfuvjQYAfHs4S+JqiIhaTw6r+gIMMkSSuaVvKADgQGoRyg21EldDRNQ69S0yUk69BhhkiCQTF+iJbkFeqDFb8PVB7r9ERMry+xgZaaMEgwyRRARBsHUv/XzyksTVEBG1jhy2JwAYZIgkdX33QADA0fRiVNWYJK6GiKjlONiXiBDt74FwH3fUmkUcSC2Suhwiohb7fbCvi6R1MMgQSUgQBIzsGgAA2HW2QOJqiIharqqGY2SICMCIbnVB5hyDDBEph4EL4hERAIyI8wcAnM4tR165QeJqiIhaxjb9moN9iTo3fy8teofpAAC72SpDRArBLQqIyKZ+9tK2k3kSV0JE1DIMMkRkM653CABge0qerd+ZiEjO6ruWPNi1RET9IvQI1buhqsbM2UtEpAj1LTLcouAqysvLMXfuXERHR8Pd3R3Dhw/HwYMHpS6LyK4EQcBNvYIBADvOsHuJiOTPNv2aLTLNe/TRR7F161Z8/vnnSE5Oxs0334yxY8ciK4s7BpNzua6bdZzML6fyYLaIEldDRNS8+m5wdi01o7q6Gt9++y3efPNNXHfddejatSsWLFiArl27YtmyZVKXR2RXI7sFwMdDg5xSA2cvEZHscffrFjCZTDCbzXBzc2twv7u7O3bt2tXoc4xGI8rKyhocRErgplFjbE9r99K+C4USV0NE1Lwq7rV0dd7e3hg2bBgWLlyI7OxsmM1mfPHFF9i7dy9ycnIafc7ixYuh1+ttR2RkZAdXTdR2Q2J8AQD7ue8SEcnc711L3GupWZ9//jlEUUR4eDi0Wi3ef/99TJs2DSpV46XPnz8fpaWltiMjI6ODKyZqu5F142SOpBcjv9wocTVERE3jOjItFBcXh99++w0VFRXIyMjAgQMHUFtbiy5dujR6vlarhU6na3AQKUW4jzt6heogiuxeIiL5EkXx9+nXrtw0skU8PT0RGhqK4uJibNmyBZMnT5a6JCKHGFzXvZSYUSJtIURETTCaLBDrJleya+kqtmzZgs2bNyM1NRVbt27FmDFj0KNHDzz00ENSl0bkENdE+gBgkCEi+aof6Auwa+mqSktLMWfOHPTo0QMzZszAyJEjsWXLFmg0GqlLI3KI/nVB5nhWKWrNFmmLISJqRH23kquLCmqVIGkt0rYHtcDUqVMxdepUqcsg6jCx/p7QubmgzGDC6Zxy9I3QS10SEVED1TKZeg0ooEWGqLNRqQRbq0xiRrG0xRARNYJBhoiaNaAuyBzlOBkikqFqmWxPADDIEMnSNVE+ADjgl4jkSS47XwMMMkSy1D/CBwBwIb8SpVW10hZDRPQH1TUmANLvfA0wyBDJkr+XFlF+HgCApMwSaYshIvoDdi0R0VVxPRkikqsqmex8DTDIEMkWgwwRyVX9rCW2yBBRky4f8CvWrwVORCQDBplsGAkwyBDJVq9QHTRqAUWVNcgoqpa6HCIiG3YtEdFVuWnU6BVq3b39KBfGIyIZqWLXEhG1BMfJEJEclRus06+93aTf95BBhkjG6sfJJDHIEJGMVBit61t5u0m/ZSODDJGMXRPpCwA4nl2GGhN3wiYiefi9RYZBhoiaEePvAR8PDWpMFpzOLZO6HCIiAAwyRNRCgiDYtis4ksYBv0QkDxVGjpEhohYaEmPtXtp3oUjiSoiIrMoNHCNDRC00vGsAAGB/aiEXxiMiWSir61ry0jLIENFV9A6zLoxXXFWLzGIujEdE0qoxWWyTD7y17FoioqvQuqgRH+INAEjOKpW4GiLq7Or3WQIADy0XxCOiFugb7gOAQYaIpFdVa+1W0qgFaNTSxwjpKyCiq+obrgcAJGcyyBCRtOq3J5DDhpEAgwyRIvSLqAsyWaUc8EtEkqq27bMk/UBfgEGGSBG6B3vDVa1CaTUH/BKRtGwtMjLYMBJgkCFSBFcXFXqEWgf8HkrjejJEJJ2qGusYGXYtEVGrjOpmXU9m68lLEldCRJ3Z711LDDJE1Ao39woBAOxIyYeh1nyVs4mIHINdS0TUJv0i9AjVu6GqxowDqexeIiJpVNWyRYaI2kAQBAyN9QMAJGWUSFsMEXVa1XVjZDhriYharX4n7KTMEknrIKLOq6zaGmQ8ZbCqL8AgQ6Qo/SN9AACJGVxPhoikkVNqAACE6t0lrsSKQYZIQXqH6eCiElBQYUR23Q8TIqKOlFtmXcsqROcmcSVWDDJECuKmUdvWk+E4GSKSQk5JXYuMD4PMVZnNZrz00kuIjY2Fu7s74uLisHDhQjapU6dmGyfDIENEErhUZg0ycmmRkceQ4ya88cYbWLZsGdasWYPevXvj0KFDeOihh6DX6/HUU09JXR6RJPpH+mDt/nQkMsgQUQezWERU1q0jo3PXSFyNlayDzJ49ezB58mRMnDgRABATE4OvvvoKBw4caPI5RqMRRqPRdrusrMzhdRJ1pGvqBvwmZ5XCbBGhVgnSFkREnUbVZYtxemnlESFk3bU0fPhw/PLLLzhz5gwAICkpCbt27cKECROafM7ixYuh1+ttR2RkZEeVS9Qh4gK94OmqRlWNGefyKqQuh4g6kUqjdeq1SgC0LvKIEPKoogl//etfce+996JHjx7QaDQYMGAA5s6di+nTpzf5nPnz56O0tNR2ZGRkdGDFRI6nVgnoG6EHwHEyRNSx6oOMp6sLBEEercGyDjLr16/H2rVr8eWXX+LIkSNYs2YN3n77baxZs6bJ52i1Wuh0ugYHkbOxrSfDhfGIqAPV77PkIZPF8ACZj5H5v//7P1urDAD07dsXaWlpWLx4MWbOnClxdUTSuYYzl4hIAhX1LTIyGR8DyLxFpqqqCipVwxLVajUsFotEFRHJQ32LzOnccu6ETUQdpqrm964luZBPJY2YNGkSFi1ahKioKPTu3RtHjx7Fu+++i4cffljq0ogkFap3g6erGpU1ZmSVVCMu0EvqkoioE6g0ymvna0DmQeaDDz7ASy+9hCeeeAJ5eXkICwvD7Nmz8fLLL0tdGpGkBEFAuK87zlyqQFYxgwwRdYxKGXYtyaeSRnh7e2Pp0qVYunSp1KUQyU64T12QKamWuhQi6iTKDLUAAG83+cQHWY+RIaKmRfh6AABSCyolroSIOouCihoAQKCXVuJKfscgQ6RQA6N9AAC7zhZIWwgRdRr55daV8wO8GWSIqJ1Gdg0EAJzMKbP1WxMROVJ9kGGLDBG1W6C3FoF1fxWd5VYFRNQBCirqggxbZIjIHuKDvQEAp3O4OSoROZ6ta4ktMkRkDwOifAAAPybnSFsIETk9k9mCoqq6wb5skSEie7hjQDgAYO/5QhhNXOGXiBynqLIGomjd+drP01XqcmwYZIgULDbAEz4eGpgsIs7kcpwMETlOXl23kp+nFmqVPHa+BhhkiBRNEAT0DdcDAJKzSiWuhoicWb4MB/oCDDJEitc7zBpkjmczyBCR4xSUM8gQkQPUt8gcZ4sMETlQfYtMgJd8xscADDJEihcfYt0wMjW/EqIoSlwNETmrfLbIEJEj1O+5VG40oaSqVuJqiMhZyXGfJYBBhkjx3DRqhOjcAADpRVUSV0NEziq/3ACALTJE5ABR/tZWmfP5nIJNRI4hx32WAAYZIqfQK1QHADiWyQG/ROQYtq4ltsgQkb31j+RaMkTkOEaTGaXV1jF4ctpnCWCQIXIKPUKsLTJnL5Vz5hIR2V1hXWuMRi1A766RuJqGGGSInEBsgCdUAlBmMNn6sYmI7OXyXa9VMtqeAGCQIXIKbho1ovysA37P5XHALxHZ1+VBRm4YZIicRNcg68J45zhziYjsrECm+ywBDDJETiOuPsiwRYaI7EyuU68BBhkip9E1kEGGiBzDts+St7z2WQIYZIicRle2yBCRg9i6ltgiQ0SOUh9k8sqNtvUeiIjsoaDcOv06gGNkiMhRvN00tj2X2CpDRPZUUm0NMr4e7FoiIgeqb5U5zyBDRHZU38ort8XwAAYZIqfCKdhE5AglVQwyRNQBOAWbiOzNUGuG0WQBAOg9GGSIyIG6McgQkZ2V1XUrqQTAy9VF4mquxCBD5ETqu5YyiqtgqDVLXA0ROYOSy8bHyG2fJYBBhsip+Hu6wsdDA1EEznOcDBHZQXGldcaSjwxnLAEKCDIxMTEQBOGKY86cOVKXRiQ7giBwhV8isqs8GW9PACggyBw8eBA5OTm2Y+vWrQCAKVOmSFwZkTxxCjYR2ZMtyOjkGWTkN2rnDwIDAxvcXrJkCeLi4nD99dc3er7RaITRaLTdLisrc2h9RHLDKdhEZE955QYAQJAMV/UFFNAic7mamhp88cUXePjhhyEIjQ84Wrx4MfR6ve2IjIzs4CqJpMU9l4jInvLLrI0DQd5uElfSOEUFme+//x4lJSV48MEHmzxn/vz5KC0ttR0ZGRkdVyCRDNQHmdSCStSaLRJXQ0RKl11aDQAI0cuzRUb2XUuXW7lyJSZMmICwsLAmz9FqtdBq5XmxiTpCmN4dOjcXlBlMSMktR59wvdQlEZGCpRVWAQCi/T0lrqRximmRSUtLw7Zt2/Doo49KXQqRrKlUAgZG+wIADl4skrgaIlIyQ60ZOaXWMTIxDDLts2rVKgQFBWHixIlSl0IkewMirUEmOatU4kqISMkyi63dSt5aF/jKcHsCQCFBxmKxYNWqVZg5cyZcXBTVG0YkiZ6h3gCA0znlEldCREqWV2ZtjQnWuzU5yUZqiggy27ZtQ3p6Oh5++GGpSyFShJ6hOgDWmUsc8EtEbVW/hoxcp14DChnse/PNN0MURanLIFKMCF93eGldUGE04UJ+JeJDvKUuiYgUKL9+MTwZBxlFtMgQUesIgoAedeHldC4XhSSitpH7YngAgwyR06rvXjqZwyBDRG1TP9g3RO8ucSVNY5AhclI9OOCXiNopJdf68yM+WL7d0wwyRE6qvkXmFFtkiKgNqmpMSC2sBPD7H0ZyxCBD5KTq/4LKKzeisMJ4lbOJiBo6c6kComgd6BvgxTEyRNTBPLUuiPb3AACczmX3EhG1zum61tweMp/1yCBD5MR6hrB7iYjaJjGjBMDv3dRy1aYgk5GRgczMTNvtAwcOYO7cuVi+fLndCiOi9qvv1z7FAb9E1AqiKGLbqTwAwHXdAiWupnltCjL33Xcftm/fDgDIzc3FTTfdhAMHDuCFF17Aq6++atcCiajt6v+S4loyRNQaZdUmFNSNrRsc4ytxNc1rU5A5fvw4hg4dCgBYv349+vTpgz179mDt2rVYvXq1Pesjonao71o6e6kCJm5VQEQtVFBpDTHebi5w06glrqZ5bQoytbW10GqtI5i3bduG2267DQDQo0cP5OTk2K86ImqXCF93eLqqUWO24EJBpdTlEJFCFFbUAICsZyvVa1OQ6d27Nz7++GPs3LkTW7duxfjx4wEA2dnZ8Pf3t2uBRNR2KpWAHlxPhohaqX7JBj9PV4krubo2BZk33ngDn3zyCUaPHo1p06ahf//+AICNGzfaupyISB7qp05ywC8RtVRhpbVFxl8BQaZNu1+PHj0aBQUFKCsrg6/v74OAHnvsMXh4eNitOCJqv/oBv3vOF0AURQiCIHFFRCR39V1L/s7atVRdXQ2j0WgLMWlpaVi6dClSUlIQFBRk1wKJqH2Gx/lDEIBjmaU4nFYsdTlEpACFdYN9A7zk3yLTpiAzefJkfPbZZwCAkpISJCQk4J133sHtt9+OZcuW2bVAImqfLoFeGNk1AABwNq9C4mqISAlsLTIK6FpqU5A5cuQIRo0aBQDYsGEDgoODkZaWhs8++wzvv/++XQskovaLDfAEAGQUVUlcCREpQf0aMn7O2rVUVVUFb2/rAMKff/4Zd955J1QqFa699lqkpaXZtUAiar9IX+vYtQv5nIJNRFdXVDfYN8BZW2S6du2K77//HhkZGdiyZQtuvvlmAEBeXh50OnnvyUDUGfUJ1wMAfj2dh9xSg8TVEJHc2WYtOWuLzMsvv4x58+YhJiYGQ4cOxbBhwwBYW2cGDBhg1wKJqP2u7eKHayJ9UGO2YMuJXKnLISIZK66ssbXIhPq4SVzN1bUpyNx9991IT0/HoUOHsGXLFtv9N954I/7xj3/YrTgisg9BEDC+TwgA6zRsIqKm1C+eGeXnAZ2bRuJqrq5N68gAQEhICEJCQmy7YEdERHAxPCIZ6x7sBQDIKKqWuBIikrOTdUGmV6gyhoq0qUXGYrHg1VdfhV6vR3R0NKKjo+Hj44OFCxfCYuHGdERyVD/gN6OYM5eIqGm2IBOmjCDTphaZF154AStXrsSSJUswYsQIAMCuXbuwYMECGAwGLFq0yK5FElH7hfu6AwDKDSaUVtVC7yH/JmMi6ngns5XVItOmILNmzRqsWLHCtus1APTr1w/h4eF44oknGGSIZMjD1QUhOjfklhlwJq8cQ2L8pC6JiGRGFEWkFVpbbeOCvCSupmXa1LVUVFSEHj16XHF/jx49UFRU1O6iiMgx+kZYp2EnZZRIWwgRyVJpdS2qa80AgFC9/GcsAW0MMv3798c///nPK+7/5z//iX79+rW7KCJyjGsifQAABy/yDw4iulJWiXUyQICXK9w0aomraZk2dS29+eabmDhxIrZt22ZbQ2bv3r3IyMjApk2b7FogEdnPdd0C8daWFOw6WwCjyQytizJ+UBFRx8gpsS6YGap3l7iSlmtTi8z111+PM2fO4I477kBJSQlKSkpw55134sSJE/j888/tXSMR2UnvMB2CvLWorDHjQCpbZYioobS6/dgifJUTZNq8jkxYWNgVg3qTkpKwcuVKLF++vN2FEZH9qVQCxsQH4etDGfj5xCWM6hYodUlEJCMX8isAAF0CPSWupOXa1CJDRMp1S79QAMCPyTkwW0SJqyEiOanfWLZLgDJmLAEKCDJZWVm4//774e/vD3d3d/Tt2xeHDh2SuiwixRoR5w9vrQuKKmtwIrtU6nKIqBmiKMJQN4vI0YwmM5IySwAAPUK9O+Q97UHWQaa4uBgjRoyARqPBTz/9hJMnT+Kdd96Br6+v1KURKZaLWoWELtY1ZHafK5S4GiJqznPfJGHIa9uw/4Lj/61+cygTVTVmBHpr0TNEGYvhAa0cI3PnnXc2+3hJSUl7arnCG2+8gcjISKxatcp2X2xsrF3fg6gzGh4XgG2n8rDnfAH+NDpO6nKIqAn/PpIFAJj12SEcWzDOoe+1/lAGAGB6QhRUKsGh72VPrWqR0ev1zR7R0dGYMWOG3YrbuHEjBg8ejClTpiAoKAgDBgzAp59+2uxzjEYjysrKGhxE1NCIrgEArOvJGE0d02xNRK2z77JWmDKDCXnlBoe9lyiKSC2wjo+Z0CfUYe/jCK1qkbm8ZaQjXLhwAcuWLcOzzz6Lv/3tbzh48CCeeuopuLq6YubMmY0+Z/Hixfj73//eoXUSKU33YC8EeGlRUGHEkbQSDIvzl7okIrqModaMe5fva3Dfr6fycO/QKIe8X0lVLcoNJgBAlJ+HQ97DUWQ9RsZisWDgwIF4/fXXMWDAADz22GOYNWsWPv744yafM3/+fJSWltqOjIyMDqyYSBkEQbCFF64nQyQ/xzKvHIj/6+k8h73f7vMFAIAQnRvcXZW1UKasg0xoaCh69erV4L6ePXsiPT29yedotVrodLoGBxFdaXC0ddD8kfRiiSshoj8qrDACALoEeGLD49YV9H8+eQklVTUOeb+vD1r/6J98TZhDXt+RZB1kRowYgZSUlAb3nTlzBtHR0RJVROQ8BkT5AACOZZZAFLmeDJGcFFZaA0vXIC/0j/SBzs06EuRPXxyx+3uJoojkLGsL0K39GGTs6plnnsG+ffvw+uuv49y5c/jyyy+xfPlyzJkzR+rSiBSve7A3VAJQXFWL/HKj1OUQ0WWK64KMn6crNGoVFt7eBwCw90IhYv76Iz7cfs5u75VdakBJVS00agHdQ5SzEF49WQeZIUOG4LvvvsNXX32FPn36YOHChVi6dCmmT58udWlEiuemUSPG37oM+anccomrIaLLFdV1Ifl6ugIAbusfhtsv6/ZZuu0Mas0Wu7xXat1qvtH+norcSFbWQQYAbr31ViQnJ8NgMODUqVOYNWuW1CUROY1rIn0AAHvPc2E8Ijmpb5HxrwsygiBg6b0D8I97+gMAas0i0gqr7PJeaUXWIKO02Ur1ZB9kiMhxro+3bhq561y+xJUQ0eXqx8j4erg2uP+OARHoH6EHAJzMaf86adU1ZqzefREAgwwRKdDgGOtWBadzyjtsPxciurriqt/HyPxRQhfr0gnrDjQ9g7el5m1Iwtk8647XfcP17X49KTDIEHViYXo3BHi5wmQRccoOf90RkX0UV9YC+H2MzOVmDo+BIAB7zhcirbCyze9RUlWDH4/lAADuHRKJ2weEt/m1pMQgQ9SJCYKA+BDrLrfn89v+A5GI7Kuw0jqT0L+RIBPu446RdduMbDic2eb3qO+aivRzx5K7+kGtoP2VLscgQ9TJ1c9culjAIEMkB9U1ZhhqrTOSGmuRAYCpgyMBAB/8eg4bk7Lb9D7b61YKVtJO141hkCHq5OqDTGo7mqiJyH7qp167qlXwbGK7gJt6Bdu+fu2/J1v/HpU1WLMnDQAwsZ+yNon8o1ZtGklEzqe+aykx3brCryAos3mZyFnUT7329dQ0+e/RTaPGG3f1xfPfJiOv3Iic0mqE6t2v+tobk7Lx/dEsuGvUqDFb0Ddcj8nXKHNsTD22yBB1coNjfKFRC8gqqUYqu5eIJFdoW9VX2+x59wyJwpAY655pf9lwDGbL1bcaeeqro/j1dB5+TLYO8p03Lr6d1UqPQYaok/NwdUFCrHU65+YTuRJXQ0S/b0+gueq5L9/aG+4aNXaeLcC6g81Px/719KUGt0d09cf13QPbXqhMMMgQka2PfFPdX2lEJJ2iJhbDa0zfCD2evKErAODXU3lNnnc4rRizPjtsu31rv1B8OmNwOyuVBwYZIsK43iFQqwQczypDdkm11OUQdWrNLYbXmKGx1oUtT2Q3vRbUv3anwmwRcX33QCS9fDP+ed9AeLg6xzBZBhkigp+nK7oHWwf9Hs8qlbgaos6tsLJ1QaZnqA6CAOSWGa7YyT6/3IjHPjtkW/hu9vVdoPe4epeVkjDIEBEAoFeodS0Je+zfQkRtV1ZtXdVX59aywOGldUFY3YylUW/+ikqjCWaLiOLKGkz7dB9+PmkdG+OiEtA/wschNUvJOdqViKjd+kfq8e2RTOw5X4i5Y6WuhqjzqjCaAADebi3/FR3t74GskmoYai0YtvgX+Htpr5iF+N69A+Cpdb5f+2yRISIAwA09ggAAB1KLcCC1SOJqiDqvCoM1yHi1InQ8fWM329dlBlODEKMSgON/H6f4he+awiBDRACACF8P2w86e+yqS0RtU98i49WKFpmELv64uGQiTr46Ds+M7d7gsZTXJrQqFCkNgwwR2dxTt3/L/tQiiOLVF9ciIvurDzJt6QbycHXB02O74ZfnrkeAlyv+b1w8NGrn/lXvvBGNiFptULQvXF1UyCqpxunccvQMVfZmckRKZBsj045WlLhALxx68SZ7lSRrzh3TiKhVPLUuGF230ucvpy5d5WwisjdRFFHZjhaZzohBhogaGBJjXVzreBanYRN1NKPJglqztVu3NWNkOjMGGSJqoHcY15MhkkqZodb2taeTrLzraAwyRNRAr7ogk15U1eCHKhE53umccgBAlJ8H1CpB4mqUgUGGiBrw8XBFuI91ldCTzezdQkT2dzS9BAAwMMpH0jqUhEGGiK7QJ9zaKsOF8Yg61vn8CgDgjMFWYJAhoivc2DMYALApOUfiSog6l4ziKgDWriVqGQYZIrrCzb2C4aIScDq33PYXIhE5XkaRNchEMsi0GIMMEV3Bx8MV19WtJ/P53jSJqyHqHAy1ZhRU1AAAInzdJa5GORhkiKhR918bBQDYxoXxiDpEabV1lqBaJUDvrpG4GuVgkCGiRg2Kti6Ml1lcjdIqTsMmcrSSun9nencNBIFTr1uKQYaIGqV319gGHJ7ILpW4GiLnV1Jl7VbyYWtMqzDIEFGT6qdhH2eQIXK4krquJb0Hg0xrMMgQUZN6h+kBAMcyGWSIHK2+C5ctMq0j6yCzYMECCILQ4OjRo4fUZRF1GvUbSO5IyUd1jVniaoicW0l1XdeSh6vElSiL7Hek6t27N7Zt22a77eIi+5KJnMbgaF9E+Lojs7gau88VYGyvYKlLInJalw/2pZaTdYsMYA0uISEhtiMgIEDqkog6DZVKsK0ns/t8gcTVEDm3+jEyPhwj0yqyDzJnz55FWFgYunTpgunTpyM9Pb3Z841GI8rKyhocRNR2I7ta/3jYfY5BhsiR6teR4RiZ1pF1kElISMDq1auxefNmLFu2DKmpqRg1ahTKy8ubfM7ixYuh1+ttR2RkZAdWTOR8hnXxhyAAZy5VYNtJLo5H5Cj1g305a6l1ZB1kJkyYgClTpqBfv34YN24cNm3ahJKSEqxfv77J58yfPx+lpaW2IyMjowMrJnI+vp6umDrI+gfBm1tOQxRFiSsick62wb7uHOzbGrIOMn/k4+OD7t2749y5c02eo9VqodPpGhxE1D4v3NoTnq5qnLlUgZ1n2cVE5AglbJFpE0UFmYqKCpw/fx6hoaFSl0LUqejcNJg6xNoq8+nOCxJXQ+ScOGupbWQdZObNm4fffvsNFy9exJ49e3DHHXdArVZj2rRpUpdG1Ok8NDwWggDsPFuAzOIqqcshciql1bWoMJoAACE6N4mrURZZB5nMzExMmzYN8fHxmDp1Kvz9/bFv3z4EBgZKXRpRpxPl74FrY/0BAP9JypG4GiLnklFk/eMgwMsVnlqul9Yasr5a69atk7oEIrrM5GvCsPdCIX5IzMKfRsdJXQ6R06gPMpF1G7VSy8m6RYaI5GVCn1Bo1AJO55bjdC7XaCKyl6ySagBAuI+7xJUoD4MMEbWY3kOD0fFBAICNidkSV0PkPMoM1vExvtxnqdUYZIioVSZfEwYA+PpgBooqaySuhsg5lNWt6qtzl/WID1likCGiVrm5Vwi6B3uhsLIGH21vek0nImq5MkNdkHHj1OvWYpAholZxdVFh/oSeAID1hzJQY7JIXBGR8pVVW7uWdFxDptUYZIio1a7rHoggby3KDCb870y+1OUQKR5bZNqOQYaIWk2tEnBrP+tYmQ2HMyWuhkj5OEam7RhkiKhNpg6JgCAAm0/k4lxehdTlEClaed2sJW+2yLQagwwRtUmPEB2u62ZdZXvbqUsSV0OkXKIoIr/cCMC6si+1DoMMEbXZ2J7WNWW+3J8Ok5mDfonaoqiyBjVmCwQBCPLmPkutxSBDRG1258AIeLu5IL2oCieyudIvUVvklBoAAAFeWri68Ndya/GKEVGbeWpd0C9CDwBIyS2XuBoiZcqtCzLc9bptGGSIqF16hOgAAPsuFEpcCZEy1e9bFuXPDSPbgkGGiNplfJ8QAMD3iVnIKa2WuBoi5dmfWgQAGBrjJ3ElysQgQ0TtMiTGD0Nj/GARgXUHMqQuh0hxLuRXAgD6hOskrkSZGGSIqN0eGBYNAHjvl7P47igXyCNqKVEUkV9hnXrNGUttwyBDRO02sW+o7a/Jv36bjPK65daJqHllBpNtv7JAb63E1SgTgwwRtZtKJeCDaQMBAEaTBd8dzZK4IiJlKKhrjfF2c4GbRi1xNcrEIENEdhEb4Im/39YbAPDWlhRklXDgL9HV1K/oy9aYtmOQISK7uS8hCtdE+qDcYMJ7285IXQ6R7P2+NQGDTFsxyBCR3WjUKsyf0AMAsP5QJg6nFUtcEZG8sUWm/RhkiMiuErr4Y1L/MADAhsOcwUTUnPoZS4FskWkzBhkisrt7BkcCAL4+mI4L+RUSV0MkX2yRaT8GGSKyu5HdAnBd90BYRLbKEDWnftYSg0zbMcgQkUNMG2Jtlflox3n8kMjp2ER/JIoiTudYN1uN9OU+S23FIENEDnFDzyDbTIyn1yXiYkGlxBURyUtmcTVyywzQqAVcE+kjdTmKxSBDRA6hdVFjzcNDbLe3nrwkYTVE8pOSa22N6R7sDXdXLobXVgwyROQwvcP0eHFiTwDA6j0XYbGIEldEJB9pRVUAgBh/T4krUTYGGSJyqKGxfgCArJJqPLLmIESRYYYIANIKrd2tUf4cH9MeDDJE5FC9w/QY2zMIALA9JR8HUoskrohIHtIKrS0y0X4MMu3BIENEDqVWCVgxcwimDY0CALyzlVsXEAFAel3XUjS7ltqFQYaIOsRTN3YFABxILcKlMoPE1RBJy2S2IMMWZNgi0x6KCjJLliyBIAiYO3eu1KUQUSuF6t3Rv26K6a+n86QthkhiOaUGmCwiXF1UCNG5SV2OoikmyBw8eBCffPIJ+vXrJ3UpRNRGN9WNldnGqdjUyV2sG+gb6esOlUqQuBplU0SQqaiowPTp0/Hpp5/C19e32XONRiPKysoaHEQkDzf2DAYA7DpXgOoas8TVEEmnfqAvp163nyKCzJw5czBx4kSMHTv2qucuXrwYer3edkRGRnZAhUTUEj1CvBHu4w6jyYLbP9yNnNJqqUsikkT9QF9OvW4/2QeZdevW4ciRI1i8eHGLzp8/fz5KS0ttR0ZGhoMrJKKWEgTBNug35VI5/rLhmMQVEUnjXJ51V/jYALbItJeL1AU0JyMjA08//TS2bt0KN7eWDYbSarXQarmLKJFcTR0cieNZZfh8Xxp2nStAdkk1wnzcpS6LqEOdzLYOe+gVqpO4EuWTdYvM4cOHkZeXh4EDB8LFxQUuLi747bff8P7778PFxQVmM/vYiZRGEAQsvL0PEmL9IIrAd0e5MzZ1LvnlRuTWLUHQg0Gm3WQdZG688UYkJycjMTHRdgwePBjTp09HYmIi1GpuskWkVHcNigAAfHs4k9sWUKey61w+AGtrjJdW1h0jiiDrIOPt7Y0+ffo0ODw9PeHv748+ffpIXR4RtcMtfUPh4arGhYJK9HllC9Yf5Hg26hz2X7Bu0zGqe4DElTgHWQcZInJeXloX/HVCDwBAZY0Zf/n2GA6nFUtcFZHjJWeVAgAG1C0QSe2juCCzY8cOLF26VOoyiMgOZgyLwZGXbsK43tb1ZRb9eBI1JovEVRE5To3JgjOXygFYN1Sl9lNckCEi5+Ln6Yq/TugJVxcVjqSXYO3+NKlLInKY8/kVqDWL8HZzQYQvZ+vZA4MMEUkuNsATfx1v7WZad4BjZch5peRaW2N6hHhDELg1gT0wyBCRLNw1MAJqlYCUS+VIr1u+ncjZnK4LMvEh3hJX4jwYZIhIFvQeGgyN8QMAfLb3IiwWTskm55OSa10ILz6YQcZeGGSISDZu6mUd9LtiVyr+tPYw15chp5JXbsD2FOsaMvEhXAjPXhhkiEg27h4cgW5BXgCALScuYdupPIkrIrKPvHIDhi76xXa7ZyhbZOyFQYaIZEPnpsGmp0fhhh5BAIBZnx1CuaFW4qqI2u8/STm2ryN83eHtppGwGufCIENEsqJRq/DM2O62298ezpSwGqL2M5rMeHtLiu32h/cNlLAa58MgQ0Sy0zdCj7/dYp2OvWTzaWSXVEtcEVHbWCwi/vTFEVTXWjc5/nJWAvpzRV+7YpAhIll6ZGQXDInxhaHWguFLfsW5vHKpSyJqtX2phfj1dB5c1Sq8OLEnhnXxl7okp8MgQ0SypFYJmDOmq+32Z3u54i8pz+5zBQCAW/qG4NFRXbgIngMwyBCRbF3fPdDWDL89JY/TsUlRjmeVYvn/LgAAhnflTteOwiBDRLIlCAK+mpUAVxcVMoqqcSC1SOqSiFokrbASt36wC7Vma/geHscuJUdhkCEiWfNwdcHYntbp2K9vOiVxNUQt8+WBdNvXQ2J8EeHrIWE1zo1Bhohkb/6EngCApMxS5JUbJK6GqHlGkxnfHLIuG/DOlP74/JEEiStybgwyRCR7kX4e6B+hBwA8v+EYakwWiSsiatrPJy6hqLIGwTotJl8TBjeNWuqSnBqDDBEpwrxx8VAJwPaUfHy684LU5RA16ZdTlwAAdw6MgIuav2YdjVeYiBRhVLdAvDq5DwDgrS0p+HD7Oc5iItkRRRF7LxQCAEZ140yljsAgQ0SKcefAcHi4Wpvp39qSgi0nciWuiKihnFIDLpUZ4aISMDDKV+pyOgUGGSJSDA9XF3x8/yDb7afXJeJoerGEFRE1dCyzBADQPdibY2M6CIMMESnKdd0DcXrheIyOD4TRZMFrP55iFxPJRlJmKQCgf6Re4ko6DwYZIlIcN40ab97VD1oXFQ6nFWPHmXypSyICAOyvGx/TN9xH2kI6EQYZIlKkIJ0bHrg2GgDwpy8O42R2GVtmSFLPfJ2II+klAICB0T6S1tKZMMgQkWL9aXQc9O4aGGotuOX9nZjy8V7sSMmDycx1ZqhjXcivwHdHswAAD4+IRY8QncQVdR4MMkSkWP5eWnw6Y7Dt9qG0Yjy46iAeWn0QFgtbZ6jjfH0wA4B1o9OXJ/WSuJrOhUGGiBRtaKwfVs4cjLsHRWBsz2AAwM6zBUismz1C5Ggrd6Xik7pdru9LiJK4ms6HQYaIFO/GnsF4e0p/rJg5GLf1DwMA3PnRHry+6RRq2c1EDmQyW/DG5tO22zf0CJKwms6JQYaInMoTY+Lg66EBACz/3wVMeG8nVuy8ADO7msgBdp4rsO399e2fhkHDLQk6HK84ETmVHiE67J1/I5bc2RcAcC6vAq/9eAr3fboPmcVVEldHzqTMUIs3frK2xjw4PAaDov0krqhzYpAhIqfjplHj3qFR+Pqxa3FfQhQEAdifWoQ5Xx7lIGCyixqTBY99dginc8sR4KXFrOu6SF1Sp8UgQ0ROK6GLP16/oy8+vG8gACApowQfbj8ncVWkdBcLKnHbP3dh34UieGldsPqhIQj3cZe6rE5L1kFm2bJl6NevH3Q6HXQ6HYYNG4affvpJ6rKISGFu6RuKN+/uBwD4cMc5ZJdUS1wRKVVJVQ1Gv70Dp3PLAQAfTBuAPuHcjkBKsg4yERERWLJkCQ4fPoxDhw7hhhtuwOTJk3HixAmpSyMihZkyKAJDY/1gqLVgxc5UqcshBaowmvDYZ4dtt/08XTGGs5QkJ+sgM2nSJNxyyy3o1q0bunfvjkWLFsHLywv79u2TujQiUhhBEPDIyFgAwH+OZeNSmUHiikgpygy12JScgz9/eQQHLhYBAMJ93LFi5uCrPJM6govUBbSU2WzGN998g8rKSgwbNqzJ84xGI4xGo+12WVlZR5RHRApwXbdA+Hu6Ir/ciNs/3I2fn7kOapUAd40agiBIXR7JkNki4sF/HbDtoQQAN/cKxvIZDDFyIesWGQBITk6Gl5cXtFotHn/8cXz33Xfo1avp5Z8XL14MvV5vOyIjIzuwWiKSM3dXNT5/JAF+nq7IKTWg74Kf0evlLZjxrwNcOI+usPtcAQa9trVBiOkW5MUQIzOCKPPtYmtqapCeno7S0lJs2LABK1aswG+//dZkmGmsRSYyMhKlpaXQ6biJFxEBO1Ly8OCqgw3u+2j6QNzSN1SiikhOLBYRP5/MxXPrk1BZYwYAjO0ZjJt7B+PaWH9E+XtIXGHnUFZWBr1ef9Xf37IPMn80duxYxMXF4ZNPPmnR+S29EETUuWxPycM/tp7BscxS2333DonE3yb2hM5NI2Fl1NEyiqqgUgm2KdQvfp+ML/al2x7/5IFBuLlXMLsfO1hLf38rZoxMPYvF0qDFhYioLcbEB2FMfBBOZJdi4vu7AADrDmZgz/lCvHxrL4ztFSxxheRoxZU1WLTpFL49kgmNWoXXJvfBv3an2qZWTxsaiedujkeAl1biSqk5sm6RmT9/PiZMmICoqCiUl5fjyy+/xBtvvIEtW7bgpptuatFrsEWGiK6mzFCLpVvP4l+7f5+WPW1oJAABpdU1uGNABEZ1C4CbRi1dkdRuoiiitLoWencNRBGY9M9dOJHd+ISQUd0C8PkjCR1cIV3OKVpk8vLyMGPGDOTk5ECv16Nfv36tCjFERC2hc9Pg5Um9cGPPIExfsR8A8NWBDNvjm5JzMTjaF1/PHga1it0LSvXaj6ewcpc1rOrdNSitroWHqxrvTOmPZ9cnobrWOh7m1n6heOOuflKWSq0g6xYZe2CLDBG1xqUyAxZvOoXTueU4c6kcl2/N9MqkXnjg2mi4cIdjRRBFEfnlRhhqLXj75xRsTMq+4py/jI/HE6O74mJBJdKKqtAnTAd/diXJgtMO9m0tBhkiao/M4ip8sS8dH/923nbfmoeHol+4Hv+34RhqzRZ88sAgdjvJzKGLRVjwnxM4ntX0WmJv3t0Pdw+MgIqtbLLEIFOHQYaI2stsEfHAyv3Yc74QAODroUGUnweS6mY8je8dgr4RekwZHIEgbzcpSyUA5YZa3PDOb8gvbzgxZOrgCLwwsRcyiqoQqndjy4vMMcjUYZAhInsoqarBN4cy8eGOcyipqm3yvLWPJmBE14Ar7hdFkdN3O0BpVS1mf3EI+y4UwddDAxe1CiVVNVj+wGDui6QwDDJ1GGSIyJ5+OXUJj6w5BMA6ZubgxSJsSs61Pe6qVuGJMXH40+g4aF3UqDVb8NRXR/G/M/n4dOZgDI+7MuRQ2+WVGWA0WRDp54HEjBI8vPogiipr4OmqxhePJqBbsDeqjCYE6dhSpjQMMnUYZIjI3raevIQgby36R/oAsLa2XCoz4u6P9yCzuNp2no+HBrUmi2112EBvLTY8PgzR/p62c0qraqHVqDjGppUsFhGLf7LOQrKIQEKsH5KzSlFVY0aXQE+8dXd/DIr2lbpMagcGmToMMkTUUWrNFqzclYolP51u9rxR3QLw/Pge+NeuVPz7aBYCvKwBJybAGnDSCivhqXXhQmzN+CExC0+vS7zi/lHdAvDx/YPgqZX16iLUAgwydRhkiKij/ScpGy98l4xb+4dhQp8QxPh7wkUtYNryfbhYWNXoc4Z18ceKmYPxysYT2HA4E3p3DX597noOSK2TWVyFNzanYOawaGSXGvDUV0cBAHPGxOG+hGis3ZcGEcDcsd2gdWHrljNgkKnDIENEclFrtmDfhUL8+aujtgHDapUAs6XxH8NaFxXevLsfJl8T3pFlykpGURVe33QKPx3PveKx2ABPbHxyBLy5N5ZTcoqVfYmInIlGrcKoboHY9NQoZJVUQ6NWoW+4Hu/8nIKPdljXqVGrBNw3NArrDqbDaLLg2fVJiPb3RLBOi1C9u8SfoGN9vi8NL31/vMnHX7+jL0MMsUWGiEhqRpMZ8745hhPZpVg4uQ9GdA3AkfRi3PnRHts5apWAxXf0xd2DIpBTZsB/k7IR7e+J8X1CJKzc/gy1Zrhp1Nh8PBePf3HYdn98sDeGxPriv8dyMKlfGF6d3JvT2Z0cu5bqMMgQkVIdzyrFI2sO4lKZsdHHBQFYMKk3ZgyLbvKXuslsnTWld299y0Wt2QJNK7djsFhEHEorRqC3FrEBno2ec/ZSOeZ9kwS9hyv6huswJMYPA6J88VNyDl78/jhMl3W1xQd7479PjWx1HaR8DDJ1GGSISMlMZgtKqmvx7tYz+HJ/eqPnjOwagMExvqgwmDBzeAwi/TxQWl2Lrw6kY+WuVOSXG/HwiFg8d3N3uGvUzS7Jn1ZYibe2pOC/x3Js931430D0i9DjXF4F4kO8EeZzZRdXUWUN/vTFYexPLQIAqATg0xmDcWPP4AbnXSyoxJNfHWl264DLbZl7HeJDvFt0LjkXBpk6DDJE5CwWbDyB1Xsuwk2jwm//NwbfHMrA2z+fueK8KD8PpBc1PjsKAPqE67Bs+iBE+nk0uP8/Sdl45uvEBi0ifxTu447vnhjeYIG5jKIqjHpze6Pnj60LMttOXWr2swGAi0pA1yAvxPh7YmC0D2YOj+EMpE6MQaYOgwwROZOqGhPMFhHebhqIooh7PtmHAxeLmjz/weExiAv0xNs/n0FpdcOtFX6YM8K2qF+NyYIRb/yK/HIjwvRuGBzjhyExvli06RQMtRYIAnD5b4sFk3phfJ9QHMsswWOf/z6WZUCUD+aM7ooVuy5g34XG6+odpsOzN3VHl0AvZBVXI+VSOXqGeiM+2JvTzcmGQaYOgwwRObNyQy0ulRnQJcALggBsT8lDWbUJPh4aDInxsy0Ml1duwNcHMnChoBLfHc2yPf/x6+OQXlSJpIxSZJVUI8DLFXvn32gbkyKKIvIrjPBxd8WnOy/grS0pTdaycHJvPDAsxlbX2v3pOJBahF9P56FLgCcSuviha5A3ZgyL5pgXuioGmToMMkREDf3z17ONdkkBwIsTe+LRUV2afO7J7DIs++08/pOU3eD+VQ8NwZh4bspI9sMgU4dBhojoSnvOF+DF746jtLoWMQGeuP2aMIT5uOOGHkFXndYsiiJO55Zj2Y7zqDSa8M/7BsLdlWNZyL4YZOowyBARESlPS39/s5OSiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBTLReoCHE0URQDW7cCJiIhIGep/b9f/Hm+K0weZ8vJyAEBkZKTElRAREVFrlZeXQ6/XN/m4IF4t6iicxWJBdnY2vL29IQiC3V63rKwMkZGRyMjIgE6ns9vrOiter9bh9WodXq/W4fVqOV6r1rHn9RJFEeXl5QgLC4NK1fRIGKdvkVGpVIiIiHDY6+t0On5ztwKvV+vwerUOr1fr8Hq1HK9V69jrejXXElOPg32JiIhIsRhkiIiISLEYZNpIq9XilVdegVarlboUReD1ah1er9bh9WodXq+W47VqHSmul9MP9iUiIiLnxRYZIiIiUiwGGSIiIlIsBhkiIiJSLAYZIiIiUiwGGTvYsWMHBEFo9Dh48KDU5cnSjz/+iISEBLi7u8PX1xe333671CXJVkxMzBXfV0uWLJG6LNkzGo245pprIAgCEhMTpS5Htm677TZERUXBzc0NoaGheOCBB5CdnS11WbJ08eJFPPLII4iNjYW7uzvi4uLwyiuvoKamRurSZGnRokUYPnw4PDw84OPj47D3cfqVfTvC8OHDkZOT0+C+l156Cb/88gsGDx4sUVXy9e2332LWrFl4/fXXccMNN8BkMuH48eNSlyVrr776KmbNmmW77e3tLWE1yvCXv/wFYWFhSEpKkroUWRszZgz+9re/ITQ0FFlZWZg3bx7uvvtu7NmzR+rSZOf06dOwWCz45JNP0LVrVxw/fhyzZs1CZWUl3n77banLk52amhpMmTIFw4YNw8qVKx33RiLZXU1NjRgYGCi++uqrUpciO7W1tWJ4eLi4YsUKqUtRjOjoaPEf//iH1GUoyqZNm8QePXqIJ06cEAGIR48elbokxfjhhx9EQRDEmpoaqUtRhDfffFOMjY2VugxZW7VqlajX6x32+uxacoCNGzeisLAQDz30kNSlyM6RI0eQlZUFlUqFAQMGIDQ0FBMmTGCLzFUsWbIE/v7+GDBgAN566y2YTCapS5KtS5cuYdasWfj888/h4eEhdTmKUlRUhLVr12L48OHQaDRSl6MIpaWl8PPzk7qMTo1BxgFWrlyJcePGOXSzSqW6cOECAGDBggV48cUX8d///he+vr4YPXo0ioqKJK5Onp566imsW7cO27dvx+zZs/H666/jL3/5i9RlyZIoinjwwQfx+OOPs1u3FZ5//nl4enrC398f6enp+OGHH6QuSRHOnTuHDz74ALNnz5a6lM7NYW09TuD5558XATR7nDp1qsFzMjIyRJVKJW7YsEGiqqXR0mu1du1aEYD4ySef2J5rMBjEgIAA8eOPP5bwE3Sstnxv1Vu5cqXo4uIiGgyGDq5aOi29Xu+99544YsQI0WQyiaIoiqmpqZ2ya6m131/5+fliSkqK+PPPP4sjRowQb7nlFtFisUj4CTpWW/49ZmZminFxceIjjzwiUdXSaMu1cnTXErcoaEZ+fj4KCwubPadLly5wdXW13V64cCE++OADZGVldaqm2ZZeq927d+OGG27Azp07MXLkSNtjCQkJGDt2LBYtWuToUmWhLd9b9U6cOIE+ffrg9OnTiI+Pd1SJstLS6zV16lT85z//gSAItvvNZjPUajWmT5+ONWvWOLpUWWjP91dmZiYiIyOxZ88eDBs2zFElykprr1d2djZGjx6Na6+9FqtXr4ZK1Xk6N9ryvbV69WrMnTsXJSUlDqmJs5aaERgYiMDAwBafL4oiVq1ahRkzZnSqEAO0/FoNGjQIWq0WKSkptiBTW1uLixcvIjo62tFlykZrv7cul5iYCJVKhaCgIDtXJV8tvV7vv/8+XnvtNdvt7OxsjBs3Dl9//TUSEhIcWaKstOf7y2KxALBOX+8sWnO9srKyMGbMGAwaNAirVq3qVCEGaN/3lqMwyNjRr7/+itTUVDz66KNSlyJbOp0Ojz/+OF555RVERkYiOjoab731FgBgypQpElcnP3v37sX+/fsxZswYeHt7Y+/evXjmmWdw//33w9fXV+ryZCcqKqrBbS8vLwBAXFwcx6w1Yv/+/Th48CBGjhwJX19fnD9/Hi+99BLi4uI6TWtMa2RlZWH06NGIjo7G22+/jfz8fNtjISEhElYmT+np6SgqKkJ6ejrMZrNtPaeuXbva/m3ahcM6rTqhadOmicOHD5e6DNmrqakRn3vuOTEoKEj09vYWx44dKx4/flzqsmTp8OHDYkJCgqjX60U3NzexZ8+e4uuvv96pxse0R2cdI9NSx44dE8eMGSP6+fmJWq1WjImJER9//HExMzNT6tJkadWqVU2OC6ErzZw5s9FrtX37dru+D8fIEBERkWJ1rs49IiIicioMMkRERKRYDDJERESkWAwyREREpFgMMkRERKRYDDJERESkWAwyREREpFgMMkRERKRYDDJEZBMTE4OlS5dKXUaHeemll/DYY4/Zbo8ePRpz586VriA7aO1n2Lx5M6655hrbHktESsMgQ9TBHnzwQdx+++1Sl9GogwcPNvjFLjf2vHa5ubl477338MILL9jl9ZRq/Pjx0Gg0WLt2rdSlELUJgwxRJ1BbW9ui8wIDA+Hh4eHgaq7U0vrsacWKFRg+fHin2nW9KQ8++CDef/99qcsgahMGGSKZOX78OCZMmAAvLy8EBwfjgQceQEFBge3xzZs3Y+TIkfDx8YG/vz9uvfVWnD9/3vb4xYsXIQgCvv76a1x//fVwc3PD2rVrba0Zb7/9NkJDQ+Hv7485c+Y0CBF/7FoSBAErVqzAHXfcAQ8PD3Tr1g0bN25sUO/GjRvRrVs3uLm5YcyYMVizZg0EQUBJSUmTn1EQBCxbtgy33XYbPD09sWjRIpjNZjzyyCOIjY2Fu7s74uPj8d5779mes2DBAqxZswY//PADBEGAIAjYsWMHACAjIwNTp06Fj48P/Pz8MHnyZFy8eLHZ67xu3TpMmjSp2XOKi4sxY8YM+Pr6wsPDAxMmTMDZs2cbnPPpp58iMjISHh4euOOOO/Duu+/Cx8enydesqanBk08+idDQULi5uSE6OhqLFy+2PV5SUoLZs2cjODgYbm5u6NOnD/773/8CAAoLCzFt2jSEh4fDw8MDffv2xVdffdXsZzAajZg3bx7Cw8Ph6emJhIQE23WrN2nSJBw6dKjB9xGRUjDIEMlISUkJbrjhBgwYMACHDh3C5s2bcenSJUydOtV2TmVlJZ599lkcOnQIv/zyC1QqFe64444rxjj89a9/xdNPP41Tp05h3LhxAIDt27fj/Pnz2L59O9asWYPVq1dj9erVzdb097//HVOnTsWxY8dwyy23YPr06SgqKgIApKam4u6778btt9+OpKQkzJ49u8VdNQsWLMAdd9yB5ORkPPzww7BYLIiIiMA333yDkydP4uWXX8bf/vY3rF+/HgAwb948TJ06FePHj0dOTg5ycnIwfPhw1NbWYty4cfD29sbOnTuxe/dueHl5Yfz48aipqWn0vYuKinDy5EkMHjy42RoffPBBHDp0CBs3bsTevXshiiJuueUWW/jbvXs3Hn/8cTz99NNITEzETTfdhEWLFjX7mu+//z42btyI9evXIyUlBWvXrkVMTAwAwGKxYMKECdi9eze++OILnDx5EkuWLIFarQYAGAwGDBo0CD/++COOHz+Oxx57DA888AAOHDjQ5Ps9+eST2Lt3L9atW4djx45hypQpGD9+fINAFhUVheDgYOzcubPZ2olkya57aRPRVc2cOVOcPHlyo48tXLhQvPnmmxvcl5GRIQIQU1JSGn1Ofn6+CEBMTk4WRVEUU1NTRQDi0qVLr3jf6Oho0WQy2e6bMmWKeM8999huR0dHi//4xz9stwGIL774ou12RUWFCED86aefRFEUxeeff17s06dPg/d54YUXRABicXFx4xeg7nXnzp3b5OP15syZI951110NPsMfr93nn38uxsfHixaLxXaf0WgU3d3dxS1btjT6ukePHhUBiOnp6Q3uv/7668Wnn35aFEVRPHPmjAhA3L17t+3xgoIC0d3dXVy/fr0oiqJ4zz33iBMnTmzwGtOnTxf1en2Tn+nPf/6zeMMNNzSot96WLVtElUrV5P/rxkycOFF87rnnGv0MaWlpolqtFrOysho858YbbxTnz5/f4L4BAwaICxYsaPH7EskFW2SIZCQpKQnbt2+Hl5eX7ejRowcA2Jr9z549i2nTpqFLly7Q6XS2v+bT09MbvFZjrQ29e/e2/XUPAKGhocjLy2u2pn79+tm+9vT0hE6nsz0nJSUFQ4YMaXD+0KFDW/RZG6vvww8/xKBBgxAYGAgvLy8sX778is/1R0lJSTh37hy8vb1t18zPzw8Gg6HJrpLq6moAgJubW5Ove+rUKbi4uCAhIcF2n7+/P+Lj43Hq1CkA1s//x897tc//4IMPIjExEfHx8Xjqqafw888/2x5LTExEREQEunfv3uhzzWYzFi5ciL59+8LPzw9eXl7YsmVLk9coOTkZZrMZ3bt3b/A99dtvv11xbdzd3VFVVdVs7URy5CJ1AUT0u4qKCkyaNAlvvPHGFY+FhoYCsI5niI6OxqeffoqwsDBYLBb06dPnim4UT0/PK15Do9E0uC0IwlWn3bblOS3xx/rWrVuHefPm4Z133sGwYcPg7e2Nt956C/v372/2dSoqKjBo0KBGZ90EBgY2+pyAgAAA1jEwTZ3jKAMHDkRqaip++uknbNu2DVOnTsXYsWOxYcMGuLu7N/vct956C++99x6WLl2Kvn37wtPTE3Pnzm2yC62iogJqtRqHDx9uEGABwMvLq8HtoqKiDr8WRPbAIEMkIwMHDsS3336LmJgYuLhc+c+zsLAQKSkp+PTTTzFq1CgAwK5duzq6TJv4+Hhs2rSpwX0HDx5s02vt3r0bw4cPxxNPPGG774+tBq6urjCbzQ3uGzhwIL7++msEBQVBp9O16L3i4uKg0+lw8uTJJls/evbsCZPJhP3792P48OEAfr/+vXr1AmD9/H/8vC35/DqdDvfccw/uuece3H333Rg/fjyKiorQr18/ZGZm4syZM43WtXv3bkyePBn3338/AOuYmjNnztjq+aMBAwbAbDYjLy/P9v3SmPrWqwEDBly1diK5YdcSkQRKS0uRmJjY4MjIyMCcOXNQVFSEadOm4eDBgzh//jy2bNmChx56CGazGb6+vvD398fy5ctx7tw5/Prrr3j22Wcl+xyzZ8/G6dOn8fzzz+PMmTNYv369bfCwIAiteq1u3brh0KFD2LJlC86cOYOXXnrpilAQExODY8eOISUlBQUFBaitrcX06dMREBCAyZMnY+fOnUhNTcWOHTvw1FNPITMzs9H3UqlUGDt2bLMhsFu3bpg8eTJmzZqFXbt2ISkpCffffz/Cw8MxefJkAMCf//xnbNq0Ce+++y7Onj2LTz75BD/99FOzn/3dd9/FV199hdOnT+PMmTP45ptvEBISAh8fH1x//fW47rrrcNddd2Hr1q22lpvNmzfbatq6dSv27NmDU6dOYfbs2bh06VKT79W9e3dMnz4dM2bMwL///W+kpqbiwIEDWLx4MX788Ufbefv27YNWq8WwYcOafC0iuWKQIZLAjh07MGDAgAbH3//+d4SFhWH37t0wm824+eab0bdvX8ydOxc+Pj5QqVRQqVRYt24dDh8+jD59+uCZZ57BW2+9JdnniI2NxYYNG/Dvf/8b/fr1w7Jly2yzlrRabatea/bs2bjzzjtxzz33ICEhAYWFhQ1aZwBg1qxZiI+Px+DBgxEYGIjdu3fDw8MD//vf/xAVFYU777wTPXv2xCOPPAKDwdBsC82jjz6KdevWNdtNtmrVKgwaNAi33norhg0bBlEUsWnTJlt324gRI/Dxxx/j3XffRf/+/bF582Y888wzzY698fb2xptvvonBgwdjyJAhuHjxIjZt2gSVyvrj+Ntvv8WQIUMwbdo09OrVC3/5y19srVAvvvgiBg4ciHHjxmH06NEICQm56gKBq1atwowZM/Dcc88hPj4et99+Ow4ePIioqCjbOV999RWmT58uyRpCRO0liKIoSl0EETmPRYsW4eOPP0ZGRobUpTRLFEUkJCTgmWeewbRp0+z2urNmzcLp06cVM5W5oKAA8fHxOHToEGJjY6Uuh6jVOEaGiNrlo48+wpAhQ+Dv74/du3fjrbfewpNPPil1WVclCAKWL1+O5OTkdr3O22+/jZtuugmenp746aefsGbNGnz00Ud2qtLxLl68iI8++oghhhSLLTJE1C7PPPMMvv76axQVFSEqKgoPPPAA5s+f3+hgZWc0depU7NixA+Xl5ejSpQv+/Oc/4/HHH5e6LKJOg0GGiIiIFIuDfYmIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsRhkiIiISLEYZIiIiEixGGSIiIhIsf4fh20liQpunEsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This graph shows that a learning rate of around 1e-3 would be the best, as it would have the lowest loss. However, the loss is still kind of high."
      ],
      "metadata": {
        "id": "BAXtaqIlKb2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "Z5VJHaHs2_Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tested my code with and without a scheduler as well. I tried both StepLR and ReduceROnPlateau. I didn't see a huge difference between these, but the losses with ReduceROnPlateau were slightly lower."
      ],
      "metadata": {
        "id": "lmFS8bCRKpAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, verbose=True)"
      ],
      "metadata": {
        "id": "FjZAYKHu4zud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bleu score"
      ],
      "metadata": {
        "id": "NZiVHY1c6yN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine how to calculate the accuracy of an NLP model, I found a metric called a \"bleu score\". I looked up an implementation of calculating it:"
      ],
      "metadata": {
        "id": "TGTCmDHc--Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(data_loader, model, device, tokenizer):\n",
        "    candidate_corpus = []\n",
        "    reference_corpus = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            images = batch['images'].to(device)\n",
        "            captions = batch['captions'].to(device)\n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            # Convert model outputs to text\n",
        "            output_captions = torch.argmax(outputs, dim=-1)\n",
        "            for i in range(outputs.size(0)):\n",
        "                # Convert predicted ids to tokens\n",
        "                candidate_tokens = tokenizer.convert_ids_to_tokens(output_captions[i], skip_special_tokens=True)\n",
        "                candidate_corpus.append(candidate_tokens)\n",
        "\n",
        "                # Convert ground truth ids to tokens\n",
        "                ref_tokens = tokenizer.convert_ids_to_tokens(captions[i, 1:], skip_special_tokens=True)\n",
        "                reference_corpus.append([ref_tokens])  # Note: the reference corpus is a list of lists of tokens\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu = bleu_score(candidate_corpus, reference_corpus)\n",
        "    return bleu\n"
      ],
      "metadata": {
        "id": "rjhvtNz8mCLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and validation loop"
      ],
      "metadata": {
        "id": "DE2asBksWPka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, val_loader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images = batch['images'].to(device)\n",
        "            captions = batch['captions'].to(device)\n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    return average_val_loss\n"
      ],
      "metadata": {
        "id": "J6wBvpzROq6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        images = batch['images'].to(device)\n",
        "        captions = batch['captions'].to(device)\n",
        "\n",
        "        # for i, caption_ids in enumerate(captions[:3]):  # Loop through the first few captions in the batch\n",
        "        #   caption = tokenizer.decode(caption_ids.cpu().tolist(), skip_special_tokens=True)\n",
        "        #   print(f\"Decoded Caption [{i}]: {caption}\")\n",
        "\n",
        "        # excluding the last word in the caption (<end> token)\n",
        "        outputs = model(images, captions[:, :-1])\n",
        "\n",
        "        # offset the caption by one to the right (exclude the <start> token)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch Batch {i+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        #print_predictions(outputs, tokenizer)\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    return average_loss"
      ],
      "metadata": {
        "id": "zoCE1TMSV4Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running with lr = 1e-3 and for 15 epochs"
      ],
      "metadata": {
        "id": "ZcVC7PQf5krD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, epochs, device):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # Train one epoch\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        print(f\"Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Validate after training\n",
        "        val_loss = validate_model(model, val_loader, criterion, device)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Calculate BLEU score for the validation set\n",
        "        bleu = calculate_bleu(val_loader, model, device, tokenizer)\n",
        "        print(f\"Validation BLEU Score: {bleu:.4f}\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "# Assuming 'model', 'train_loader', 'val_loader', 'optimizer', 'criterion' and 'device' are defined\n",
        "train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, 15, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JL6-NNK3ArL",
        "outputId": "50d59a95-f7d3-464d-9bff-79b4547354c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "Epoch Batch 10: Loss = 2.4271\n",
            "Epoch Batch 20: Loss = 2.3470\n",
            "Epoch Batch 30: Loss = 1.9763\n",
            "Epoch Batch 40: Loss = 3.5809\n",
            "Epoch Batch 50: Loss = 2.0520\n",
            "Epoch Batch 60: Loss = 2.6291\n",
            "Epoch Batch 70: Loss = 2.4001\n",
            "Epoch Batch 80: Loss = 2.6934\n",
            "Epoch Batch 90: Loss = 1.8244\n",
            "Epoch Batch 100: Loss = 2.6809\n",
            "Epoch Batch 110: Loss = 2.6268\n",
            "Epoch Batch 120: Loss = 2.8100\n",
            "Epoch Batch 130: Loss = 2.9043\n",
            "Epoch Batch 140: Loss = 2.6058\n",
            "Epoch Batch 150: Loss = 2.4262\n",
            "Epoch Batch 160: Loss = 2.5447\n",
            "Epoch Batch 170: Loss = 2.1028\n",
            "Epoch Batch 180: Loss = 1.9502\n",
            "Epoch Batch 190: Loss = 1.3966\n",
            "Epoch Batch 200: Loss = 2.5812\n",
            "Epoch Batch 210: Loss = 1.2794\n",
            "Epoch Batch 220: Loss = 2.8686\n",
            "Epoch Batch 230: Loss = 2.3991\n",
            "Epoch Batch 240: Loss = 2.3958\n",
            "Epoch Batch 250: Loss = 2.2793\n",
            "Epoch Batch 260: Loss = 2.8416\n",
            "Epoch Batch 270: Loss = 2.8380\n",
            "Epoch Batch 280: Loss = 1.9150\n",
            "Epoch Batch 290: Loss = 2.4061\n",
            "Epoch Batch 300: Loss = 2.7443\n",
            "Epoch Batch 310: Loss = 2.8670\n",
            "Epoch Batch 320: Loss = 2.6701\n",
            "Epoch Batch 330: Loss = 2.9453\n",
            "Epoch Batch 340: Loss = 1.7061\n",
            "Epoch Batch 350: Loss = 2.5604\n",
            "Epoch Batch 360: Loss = 1.8431\n",
            "Epoch Batch 370: Loss = 2.2153\n",
            "Epoch Batch 380: Loss = 2.6052\n",
            "Epoch Batch 390: Loss = 2.3430\n",
            "Epoch Batch 400: Loss = 2.7023\n",
            "Epoch Batch 410: Loss = 2.1793\n",
            "Epoch Batch 420: Loss = 3.1501\n",
            "Epoch Batch 430: Loss = 2.7736\n",
            "Epoch Batch 440: Loss = 3.4210\n",
            "Epoch Batch 450: Loss = 2.8423\n",
            "Epoch Batch 460: Loss = 2.3662\n",
            "Epoch Batch 470: Loss = 2.6205\n",
            "Epoch Batch 480: Loss = 2.1319\n",
            "Epoch Batch 490: Loss = 3.0499\n",
            "Epoch Batch 500: Loss = 2.6785\n",
            "Epoch Batch 510: Loss = 2.8960\n",
            "Epoch Batch 520: Loss = 1.3814\n",
            "Epoch Batch 530: Loss = 3.5045\n",
            "Epoch Batch 540: Loss = 1.5159\n",
            "Epoch Batch 550: Loss = 3.1291\n",
            "Epoch Batch 560: Loss = 3.4632\n",
            "Epoch Batch 570: Loss = 2.7759\n",
            "Epoch Batch 580: Loss = 2.8282\n",
            "Epoch Batch 590: Loss = 3.2633\n",
            "Epoch Batch 600: Loss = 2.8949\n",
            "Epoch Batch 610: Loss = 1.5463\n",
            "Epoch Batch 620: Loss = 2.7063\n",
            "Epoch Batch 630: Loss = 2.7828\n",
            "Epoch Batch 640: Loss = 2.1020\n",
            "Epoch Batch 650: Loss = 3.0217\n",
            "Epoch Batch 660: Loss = 2.3236\n",
            "Epoch Batch 670: Loss = 2.2427\n",
            "Epoch Batch 680: Loss = 3.1775\n",
            "Epoch Batch 690: Loss = 2.1518\n",
            "Epoch Batch 700: Loss = 2.7713\n",
            "Epoch Batch 710: Loss = 3.0665\n",
            "Epoch Batch 720: Loss = 2.1131\n",
            "Epoch Batch 730: Loss = 1.5846\n",
            "Epoch Batch 740: Loss = 1.7417\n",
            "Epoch Batch 750: Loss = 2.2491\n",
            "Epoch Batch 760: Loss = 1.7860\n",
            "Epoch Batch 770: Loss = 2.0543\n",
            "Epoch Batch 780: Loss = 2.5705\n",
            "Epoch Batch 790: Loss = 2.8462\n",
            "Epoch Batch 800: Loss = 2.6982\n",
            "Epoch Batch 810: Loss = 2.2995\n",
            "Epoch Batch 820: Loss = 1.3780\n",
            "Epoch Batch 830: Loss = 1.9429\n",
            "Epoch Batch 840: Loss = 2.6929\n",
            "Epoch Batch 850: Loss = 2.3991\n",
            "Epoch Batch 860: Loss = 2.2371\n",
            "Epoch Batch 870: Loss = 1.9869\n",
            "Epoch Batch 880: Loss = 2.4338\n",
            "Epoch Batch 890: Loss = 2.8002\n",
            "Epoch Batch 900: Loss = 1.1620\n",
            "Epoch Batch 910: Loss = 3.0877\n",
            "Epoch Batch 920: Loss = 2.8098\n",
            "Epoch Batch 930: Loss = 2.6618\n",
            "Epoch Batch 940: Loss = 2.3334\n",
            "Epoch Batch 950: Loss = 3.0945\n",
            "Epoch Batch 960: Loss = 2.8404\n",
            "Epoch Batch 970: Loss = 3.1617\n",
            "Epoch Batch 980: Loss = 3.3588\n",
            "Epoch Batch 990: Loss = 2.1803\n",
            "Epoch Batch 1000: Loss = 1.9057\n",
            "Epoch Batch 1010: Loss = 2.9885\n",
            "Epoch Batch 1020: Loss = 1.4951\n",
            "Epoch Batch 1030: Loss = 2.4971\n",
            "Epoch Batch 1040: Loss = 3.0246\n",
            "Epoch Batch 1050: Loss = 2.2310\n",
            "Epoch Batch 1060: Loss = 2.4721\n",
            "Epoch Batch 1070: Loss = 1.8348\n",
            "Epoch Batch 1080: Loss = 2.1591\n",
            "Epoch Batch 1090: Loss = 2.3929\n",
            "Epoch Batch 1100: Loss = 2.9141\n",
            "Epoch Batch 1110: Loss = 1.6226\n",
            "Epoch Batch 1120: Loss = 2.3673\n",
            "Epoch Batch 1130: Loss = 2.6859\n",
            "Epoch Batch 1140: Loss = 1.7861\n",
            "Epoch Batch 1150: Loss = 1.3856\n",
            "Epoch Batch 1160: Loss = 1.8546\n",
            "Epoch Batch 1170: Loss = 2.8464\n",
            "Epoch Batch 1180: Loss = 2.8402\n",
            "Epoch Batch 1190: Loss = 2.8807\n",
            "Epoch Batch 1200: Loss = 1.8650\n",
            "Epoch Batch 1210: Loss = 2.7126\n",
            "Epoch Batch 1220: Loss = 2.3802\n",
            "Epoch Batch 1230: Loss = 3.0161\n",
            "Epoch Batch 1240: Loss = 1.8574\n",
            "Epoch Batch 1250: Loss = 2.8017\n",
            "Epoch Batch 1260: Loss = 3.3017\n",
            "Epoch Batch 1270: Loss = 1.9751\n",
            "Epoch Batch 1280: Loss = 2.8954\n",
            "Epoch Batch 1290: Loss = 1.6516\n",
            "Epoch Batch 1300: Loss = 1.6633\n",
            "Training Loss: 2.4434\n",
            "Validation Loss: 2.4369\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 2/15\n",
            "Epoch Batch 10: Loss = 2.1500\n",
            "Epoch Batch 20: Loss = 2.9882\n",
            "Epoch Batch 30: Loss = 2.3253\n",
            "Epoch Batch 40: Loss = 2.1869\n",
            "Epoch Batch 50: Loss = 3.4635\n",
            "Epoch Batch 60: Loss = 2.7689\n",
            "Epoch Batch 70: Loss = 2.6370\n",
            "Epoch Batch 80: Loss = 1.8245\n",
            "Epoch Batch 90: Loss = 3.3553\n",
            "Epoch Batch 100: Loss = 2.1156\n",
            "Epoch Batch 110: Loss = 2.4909\n",
            "Epoch Batch 120: Loss = 2.9074\n",
            "Epoch Batch 130: Loss = 3.7011\n",
            "Epoch Batch 140: Loss = 2.8044\n",
            "Epoch Batch 150: Loss = 2.1160\n",
            "Epoch Batch 160: Loss = 2.9174\n",
            "Epoch Batch 170: Loss = 2.7636\n",
            "Epoch Batch 180: Loss = 2.4552\n",
            "Epoch Batch 190: Loss = 2.0471\n",
            "Epoch Batch 200: Loss = 2.6809\n",
            "Epoch Batch 210: Loss = 3.1731\n",
            "Epoch Batch 220: Loss = 2.6519\n",
            "Epoch Batch 230: Loss = 1.9619\n",
            "Epoch Batch 240: Loss = 2.7097\n",
            "Epoch Batch 250: Loss = 2.7010\n",
            "Epoch Batch 260: Loss = 2.4066\n",
            "Epoch Batch 270: Loss = 1.3622\n",
            "Epoch Batch 280: Loss = 2.6151\n",
            "Epoch Batch 290: Loss = 2.1149\n",
            "Epoch Batch 300: Loss = 1.3742\n",
            "Epoch Batch 310: Loss = 3.2071\n",
            "Epoch Batch 320: Loss = 2.7210\n",
            "Epoch Batch 330: Loss = 1.9816\n",
            "Epoch Batch 340: Loss = 2.5581\n",
            "Epoch Batch 350: Loss = 2.2506\n",
            "Epoch Batch 360: Loss = 3.3082\n",
            "Epoch Batch 370: Loss = 2.5006\n",
            "Epoch Batch 380: Loss = 2.8088\n",
            "Epoch Batch 390: Loss = 1.9324\n",
            "Epoch Batch 400: Loss = 1.5855\n",
            "Epoch Batch 410: Loss = 2.5737\n",
            "Epoch Batch 420: Loss = 1.2072\n",
            "Epoch Batch 430: Loss = 2.7088\n",
            "Epoch Batch 440: Loss = 1.9641\n",
            "Epoch Batch 450: Loss = 2.4233\n",
            "Epoch Batch 460: Loss = 2.7432\n",
            "Epoch Batch 470: Loss = 2.7054\n",
            "Epoch Batch 480: Loss = 2.8957\n",
            "Epoch Batch 490: Loss = 2.4160\n",
            "Epoch Batch 500: Loss = 1.8357\n",
            "Epoch Batch 510: Loss = 2.5922\n",
            "Epoch Batch 520: Loss = 1.6933\n",
            "Epoch Batch 530: Loss = 1.6802\n",
            "Epoch Batch 540: Loss = 1.6966\n",
            "Epoch Batch 550: Loss = 2.3280\n",
            "Epoch Batch 560: Loss = 1.9314\n",
            "Epoch Batch 570: Loss = 2.9043\n",
            "Epoch Batch 580: Loss = 2.2025\n",
            "Epoch Batch 590: Loss = 2.6045\n",
            "Epoch Batch 600: Loss = 2.5627\n",
            "Epoch Batch 610: Loss = 2.0945\n",
            "Epoch Batch 620: Loss = 2.3883\n",
            "Epoch Batch 630: Loss = 2.0446\n",
            "Epoch Batch 640: Loss = 2.5422\n",
            "Epoch Batch 650: Loss = 2.6139\n",
            "Epoch Batch 660: Loss = 2.8781\n",
            "Epoch Batch 670: Loss = 2.3093\n",
            "Epoch Batch 680: Loss = 1.4244\n",
            "Epoch Batch 690: Loss = 2.2877\n",
            "Epoch Batch 700: Loss = 2.9105\n",
            "Epoch Batch 710: Loss = 2.9432\n",
            "Epoch Batch 720: Loss = 2.2398\n",
            "Epoch Batch 730: Loss = 1.8621\n",
            "Epoch Batch 740: Loss = 2.3985\n",
            "Epoch Batch 750: Loss = 2.7497\n",
            "Epoch Batch 760: Loss = 1.8300\n",
            "Epoch Batch 770: Loss = 2.4332\n",
            "Epoch Batch 780: Loss = 3.1284\n",
            "Epoch Batch 790: Loss = 2.5855\n",
            "Epoch Batch 800: Loss = 0.9964\n",
            "Epoch Batch 810: Loss = 2.8973\n",
            "Epoch Batch 820: Loss = 2.6988\n",
            "Epoch Batch 830: Loss = 1.5950\n",
            "Epoch Batch 840: Loss = 2.8674\n",
            "Epoch Batch 850: Loss = 2.7854\n",
            "Epoch Batch 860: Loss = 2.0530\n",
            "Epoch Batch 870: Loss = 1.9562\n",
            "Epoch Batch 880: Loss = 3.1938\n",
            "Epoch Batch 890: Loss = 2.1631\n",
            "Epoch Batch 900: Loss = 3.3512\n",
            "Epoch Batch 910: Loss = 2.3479\n",
            "Epoch Batch 920: Loss = 2.0727\n",
            "Epoch Batch 930: Loss = 2.6000\n",
            "Epoch Batch 940: Loss = 3.1819\n",
            "Epoch Batch 950: Loss = 2.5629\n",
            "Epoch Batch 960: Loss = 2.2163\n",
            "Epoch Batch 970: Loss = 2.6186\n",
            "Epoch Batch 980: Loss = 2.3454\n",
            "Epoch Batch 990: Loss = 2.2083\n",
            "Epoch Batch 1000: Loss = 2.7292\n",
            "Epoch Batch 1010: Loss = 3.0140\n",
            "Epoch Batch 1020: Loss = 2.6823\n",
            "Epoch Batch 1030: Loss = 3.4149\n",
            "Epoch Batch 1040: Loss = 1.7218\n",
            "Epoch Batch 1050: Loss = 2.5077\n",
            "Epoch Batch 1060: Loss = 2.7875\n",
            "Epoch Batch 1070: Loss = 2.1013\n",
            "Epoch Batch 1080: Loss = 0.9695\n",
            "Epoch Batch 1090: Loss = 2.8985\n",
            "Epoch Batch 1100: Loss = 3.1725\n",
            "Epoch Batch 1110: Loss = 2.2256\n",
            "Epoch Batch 1120: Loss = 2.7646\n",
            "Epoch Batch 1130: Loss = 2.0985\n",
            "Epoch Batch 1140: Loss = 2.0119\n",
            "Epoch Batch 1150: Loss = 3.0003\n",
            "Epoch Batch 1160: Loss = 3.0609\n",
            "Epoch Batch 1170: Loss = 2.7520\n",
            "Epoch Batch 1180: Loss = 2.5720\n",
            "Epoch Batch 1190: Loss = 2.8573\n",
            "Epoch Batch 1200: Loss = 2.9449\n",
            "Epoch Batch 1210: Loss = 2.9099\n",
            "Epoch Batch 1220: Loss = 2.2969\n",
            "Epoch Batch 1230: Loss = 2.3006\n",
            "Epoch Batch 1240: Loss = 1.8851\n",
            "Epoch Batch 1250: Loss = 2.6033\n",
            "Epoch Batch 1260: Loss = 2.4087\n",
            "Epoch Batch 1270: Loss = 2.4470\n",
            "Epoch Batch 1280: Loss = 2.6979\n",
            "Epoch Batch 1290: Loss = 1.4522\n",
            "Epoch Batch 1300: Loss = 2.6824\n",
            "Training Loss: 2.4848\n",
            "Validation Loss: 2.4321\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 3/15\n",
            "Epoch Batch 10: Loss = 2.7132\n",
            "Epoch Batch 20: Loss = 2.6263\n",
            "Epoch Batch 30: Loss = 2.4440\n",
            "Epoch Batch 40: Loss = 2.2821\n",
            "Epoch Batch 50: Loss = 2.8192\n",
            "Epoch Batch 60: Loss = 2.8128\n",
            "Epoch Batch 70: Loss = 2.3093\n",
            "Epoch Batch 80: Loss = 2.1034\n",
            "Epoch Batch 90: Loss = 2.5390\n",
            "Epoch Batch 100: Loss = 2.5515\n",
            "Epoch Batch 110: Loss = 2.6846\n",
            "Epoch Batch 120: Loss = 3.0215\n",
            "Epoch Batch 130: Loss = 2.9585\n",
            "Epoch Batch 140: Loss = 2.5835\n",
            "Epoch Batch 150: Loss = 1.9024\n",
            "Epoch Batch 160: Loss = 1.7225\n",
            "Epoch Batch 170: Loss = 1.7444\n",
            "Epoch Batch 180: Loss = 2.8961\n",
            "Epoch Batch 190: Loss = 2.1581\n",
            "Epoch Batch 200: Loss = 2.5695\n",
            "Epoch Batch 210: Loss = 2.5822\n",
            "Epoch Batch 220: Loss = 3.1308\n",
            "Epoch Batch 230: Loss = 2.4492\n",
            "Epoch Batch 240: Loss = 2.4394\n",
            "Epoch Batch 250: Loss = 2.5859\n",
            "Epoch Batch 260: Loss = 2.1833\n",
            "Epoch Batch 270: Loss = 1.8751\n",
            "Epoch Batch 280: Loss = 2.9959\n",
            "Epoch Batch 290: Loss = 2.8921\n",
            "Epoch Batch 300: Loss = 2.0218\n",
            "Epoch Batch 310: Loss = 1.8958\n",
            "Epoch Batch 320: Loss = 2.2411\n",
            "Epoch Batch 330: Loss = 3.2638\n",
            "Epoch Batch 340: Loss = 2.7446\n",
            "Epoch Batch 350: Loss = 2.7731\n",
            "Epoch Batch 360: Loss = 2.2924\n",
            "Epoch Batch 370: Loss = 1.6000\n",
            "Epoch Batch 380: Loss = 2.8433\n",
            "Epoch Batch 390: Loss = 2.8008\n",
            "Epoch Batch 400: Loss = 3.1676\n",
            "Epoch Batch 410: Loss = 2.6518\n",
            "Epoch Batch 420: Loss = 1.3456\n",
            "Epoch Batch 430: Loss = 2.9635\n",
            "Epoch Batch 440: Loss = 2.1263\n",
            "Epoch Batch 450: Loss = 2.9985\n",
            "Epoch Batch 460: Loss = 1.5271\n",
            "Epoch Batch 470: Loss = 2.0122\n",
            "Epoch Batch 480: Loss = 2.2465\n",
            "Epoch Batch 490: Loss = 2.8024\n",
            "Epoch Batch 500: Loss = 2.6624\n",
            "Epoch Batch 510: Loss = 2.8745\n",
            "Epoch Batch 520: Loss = 2.0211\n",
            "Epoch Batch 530: Loss = 2.9493\n",
            "Epoch Batch 540: Loss = 2.5606\n",
            "Epoch Batch 550: Loss = 2.1498\n",
            "Epoch Batch 560: Loss = 2.8462\n",
            "Epoch Batch 570: Loss = 1.4589\n",
            "Epoch Batch 580: Loss = 2.8701\n",
            "Epoch Batch 590: Loss = 2.3181\n",
            "Epoch Batch 600: Loss = 1.8678\n",
            "Epoch Batch 610: Loss = 3.4892\n",
            "Epoch Batch 620: Loss = 2.6383\n",
            "Epoch Batch 630: Loss = 2.3297\n",
            "Epoch Batch 640: Loss = 2.8072\n",
            "Epoch Batch 650: Loss = 2.8723\n",
            "Epoch Batch 660: Loss = 2.5742\n",
            "Epoch Batch 670: Loss = 3.3022\n",
            "Epoch Batch 680: Loss = 2.4768\n",
            "Epoch Batch 690: Loss = 2.9383\n",
            "Epoch Batch 700: Loss = 2.2948\n",
            "Epoch Batch 710: Loss = 1.4524\n",
            "Epoch Batch 720: Loss = 3.9350\n",
            "Epoch Batch 730: Loss = 2.7487\n",
            "Epoch Batch 740: Loss = 3.1585\n",
            "Epoch Batch 750: Loss = 1.5405\n",
            "Epoch Batch 760: Loss = 2.5379\n",
            "Epoch Batch 770: Loss = 1.7773\n",
            "Epoch Batch 780: Loss = 3.1590\n",
            "Epoch Batch 790: Loss = 2.6373\n",
            "Epoch Batch 800: Loss = 1.9059\n",
            "Epoch Batch 810: Loss = 2.4125\n",
            "Epoch Batch 820: Loss = 1.9128\n",
            "Epoch Batch 830: Loss = 2.2894\n",
            "Epoch Batch 840: Loss = 1.9915\n",
            "Epoch Batch 850: Loss = 2.0820\n",
            "Epoch Batch 860: Loss = 3.1731\n",
            "Epoch Batch 870: Loss = 2.8210\n",
            "Epoch Batch 880: Loss = 2.6297\n",
            "Epoch Batch 890: Loss = 2.1273\n",
            "Epoch Batch 900: Loss = 3.0979\n",
            "Epoch Batch 910: Loss = 1.5866\n",
            "Epoch Batch 920: Loss = 3.0500\n",
            "Epoch Batch 930: Loss = 3.7578\n",
            "Epoch Batch 940: Loss = 2.8991\n",
            "Epoch Batch 950: Loss = 2.6159\n",
            "Epoch Batch 960: Loss = 2.7725\n",
            "Epoch Batch 970: Loss = 3.1603\n",
            "Epoch Batch 980: Loss = 2.5589\n",
            "Epoch Batch 990: Loss = 1.5539\n",
            "Epoch Batch 1000: Loss = 1.5344\n",
            "Epoch Batch 1010: Loss = 3.4122\n",
            "Epoch Batch 1020: Loss = 2.1215\n",
            "Epoch Batch 1030: Loss = 1.2724\n",
            "Epoch Batch 1040: Loss = 3.0678\n",
            "Epoch Batch 1050: Loss = 2.1907\n",
            "Epoch Batch 1060: Loss = 2.0503\n",
            "Epoch Batch 1070: Loss = 1.7264\n",
            "Epoch Batch 1080: Loss = 3.0664\n",
            "Epoch Batch 1090: Loss = 3.1227\n",
            "Epoch Batch 1100: Loss = 1.9437\n",
            "Epoch Batch 1110: Loss = 1.7662\n",
            "Epoch Batch 1120: Loss = 2.6202\n",
            "Epoch Batch 1130: Loss = 2.6223\n",
            "Epoch Batch 1140: Loss = 3.3098\n",
            "Epoch Batch 1150: Loss = 3.0497\n",
            "Epoch Batch 1160: Loss = 1.7895\n",
            "Epoch Batch 1170: Loss = 2.2889\n",
            "Epoch Batch 1180: Loss = 2.2160\n",
            "Epoch Batch 1190: Loss = 2.5305\n",
            "Epoch Batch 1200: Loss = 2.5433\n",
            "Epoch Batch 1210: Loss = 2.7310\n",
            "Epoch Batch 1220: Loss = 3.0190\n",
            "Epoch Batch 1230: Loss = 2.5014\n",
            "Epoch Batch 1240: Loss = 1.7605\n",
            "Epoch Batch 1250: Loss = 3.0733\n",
            "Epoch Batch 1260: Loss = 2.5980\n",
            "Epoch Batch 1270: Loss = 2.1137\n",
            "Epoch Batch 1280: Loss = 2.3836\n",
            "Epoch Batch 1290: Loss = 3.3212\n",
            "Epoch Batch 1300: Loss = 3.0564\n",
            "Training Loss: 2.4725\n",
            "Validation Loss: 2.4580\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 4/15\n",
            "Epoch Batch 10: Loss = 1.8136\n",
            "Epoch Batch 20: Loss = 2.9469\n",
            "Epoch Batch 30: Loss = 2.6966\n",
            "Epoch Batch 40: Loss = 2.7832\n",
            "Epoch Batch 50: Loss = 2.7315\n",
            "Epoch Batch 60: Loss = 1.8762\n",
            "Epoch Batch 70: Loss = 3.2920\n",
            "Epoch Batch 80: Loss = 3.1774\n",
            "Epoch Batch 90: Loss = 2.4541\n",
            "Epoch Batch 100: Loss = 1.7131\n",
            "Epoch Batch 110: Loss = 2.6591\n",
            "Epoch Batch 120: Loss = 2.9927\n",
            "Epoch Batch 130: Loss = 1.6324\n",
            "Epoch Batch 140: Loss = 2.0315\n",
            "Epoch Batch 150: Loss = 2.6524\n",
            "Epoch Batch 160: Loss = 3.8188\n",
            "Epoch Batch 170: Loss = 2.7095\n",
            "Epoch Batch 180: Loss = 1.7891\n",
            "Epoch Batch 190: Loss = 1.5359\n",
            "Epoch Batch 200: Loss = 3.3572\n",
            "Epoch Batch 210: Loss = 1.4800\n",
            "Epoch Batch 220: Loss = 3.5537\n",
            "Epoch Batch 230: Loss = 1.7476\n",
            "Epoch Batch 240: Loss = 3.3818\n",
            "Epoch Batch 250: Loss = 1.0823\n",
            "Epoch Batch 260: Loss = 2.1614\n",
            "Epoch Batch 270: Loss = 1.6843\n",
            "Epoch Batch 280: Loss = 2.2855\n",
            "Epoch Batch 290: Loss = 2.5333\n",
            "Epoch Batch 300: Loss = 2.1235\n",
            "Epoch Batch 310: Loss = 2.8125\n",
            "Epoch Batch 320: Loss = 2.9184\n",
            "Epoch Batch 330: Loss = 2.5529\n",
            "Epoch Batch 340: Loss = 1.9284\n",
            "Epoch Batch 350: Loss = 2.9813\n",
            "Epoch Batch 360: Loss = 2.5740\n",
            "Epoch Batch 370: Loss = 2.7214\n",
            "Epoch Batch 380: Loss = 2.8704\n",
            "Epoch Batch 390: Loss = 2.6504\n",
            "Epoch Batch 400: Loss = 1.6711\n",
            "Epoch Batch 410: Loss = 2.7366\n",
            "Epoch Batch 420: Loss = 1.8887\n",
            "Epoch Batch 430: Loss = 2.3671\n",
            "Epoch Batch 440: Loss = 2.0618\n",
            "Epoch Batch 450: Loss = 2.6990\n",
            "Epoch Batch 460: Loss = 2.9273\n",
            "Epoch Batch 470: Loss = 2.6555\n",
            "Epoch Batch 480: Loss = 1.7022\n",
            "Epoch Batch 490: Loss = 2.1224\n",
            "Epoch Batch 500: Loss = 2.4013\n",
            "Epoch Batch 510: Loss = 1.7939\n",
            "Epoch Batch 520: Loss = 1.5761\n",
            "Epoch Batch 530: Loss = 1.8536\n",
            "Epoch Batch 540: Loss = 2.7216\n",
            "Epoch Batch 550: Loss = 2.5668\n",
            "Epoch Batch 560: Loss = 2.4461\n",
            "Epoch Batch 570: Loss = 1.5885\n",
            "Epoch Batch 580: Loss = 2.3489\n",
            "Epoch Batch 590: Loss = 2.1691\n",
            "Epoch Batch 600: Loss = 2.9541\n",
            "Epoch Batch 610: Loss = 3.1982\n",
            "Epoch Batch 620: Loss = 3.2349\n",
            "Epoch Batch 630: Loss = 2.3627\n",
            "Epoch Batch 640: Loss = 3.7108\n",
            "Epoch Batch 650: Loss = 3.6561\n",
            "Epoch Batch 660: Loss = 1.6676\n",
            "Epoch Batch 670: Loss = 2.4243\n",
            "Epoch Batch 680: Loss = 2.8457\n",
            "Epoch Batch 690: Loss = 3.1072\n",
            "Epoch Batch 700: Loss = 3.1167\n",
            "Epoch Batch 710: Loss = 2.3676\n",
            "Epoch Batch 720: Loss = 3.4386\n",
            "Epoch Batch 730: Loss = 1.2809\n",
            "Epoch Batch 740: Loss = 2.1985\n",
            "Epoch Batch 750: Loss = 3.1284\n",
            "Epoch Batch 760: Loss = 2.7791\n",
            "Epoch Batch 770: Loss = 2.9560\n",
            "Epoch Batch 780: Loss = 2.9225\n",
            "Epoch Batch 790: Loss = 1.9105\n",
            "Epoch Batch 800: Loss = 2.9515\n",
            "Epoch Batch 810: Loss = 2.6083\n",
            "Epoch Batch 820: Loss = 3.5339\n",
            "Epoch Batch 830: Loss = 2.3208\n",
            "Epoch Batch 840: Loss = 1.8862\n",
            "Epoch Batch 850: Loss = 2.8950\n",
            "Epoch Batch 860: Loss = 2.1891\n",
            "Epoch Batch 870: Loss = 1.5486\n",
            "Epoch Batch 880: Loss = 1.5899\n",
            "Epoch Batch 890: Loss = 2.7553\n",
            "Epoch Batch 900: Loss = 3.0092\n",
            "Epoch Batch 910: Loss = 1.9777\n",
            "Epoch Batch 920: Loss = 3.4786\n",
            "Epoch Batch 930: Loss = 2.0844\n",
            "Epoch Batch 940: Loss = 1.4324\n",
            "Epoch Batch 950: Loss = 2.1480\n",
            "Epoch Batch 960: Loss = 2.2765\n",
            "Epoch Batch 970: Loss = 2.9615\n",
            "Epoch Batch 980: Loss = 3.4764\n",
            "Epoch Batch 990: Loss = 2.1137\n",
            "Epoch Batch 1000: Loss = 2.0408\n",
            "Epoch Batch 1010: Loss = 2.9288\n",
            "Epoch Batch 1020: Loss = 2.3670\n",
            "Epoch Batch 1030: Loss = 2.7356\n",
            "Epoch Batch 1040: Loss = 2.1357\n",
            "Epoch Batch 1050: Loss = 2.5358\n",
            "Epoch Batch 1060: Loss = 1.9367\n",
            "Epoch Batch 1070: Loss = 3.5802\n",
            "Epoch Batch 1080: Loss = 2.8018\n",
            "Epoch Batch 1090: Loss = 2.6936\n",
            "Epoch Batch 1100: Loss = 3.0201\n",
            "Epoch Batch 1110: Loss = 3.3471\n",
            "Epoch Batch 1120: Loss = 2.5065\n",
            "Epoch Batch 1130: Loss = 2.8828\n",
            "Epoch Batch 1140: Loss = 2.9276\n",
            "Epoch Batch 1150: Loss = 2.3588\n",
            "Epoch Batch 1160: Loss = 2.9393\n",
            "Epoch Batch 1170: Loss = 1.9191\n",
            "Epoch Batch 1180: Loss = 3.3927\n",
            "Epoch Batch 1190: Loss = 1.7284\n",
            "Epoch Batch 1200: Loss = 2.5658\n",
            "Epoch Batch 1210: Loss = 2.7313\n",
            "Epoch Batch 1220: Loss = 3.0330\n",
            "Epoch Batch 1230: Loss = 2.7407\n",
            "Epoch Batch 1240: Loss = 2.8663\n",
            "Epoch Batch 1250: Loss = 3.4503\n",
            "Epoch Batch 1260: Loss = 3.4106\n",
            "Epoch Batch 1270: Loss = 2.6467\n",
            "Epoch Batch 1280: Loss = 1.6691\n",
            "Epoch Batch 1290: Loss = 2.7379\n",
            "Epoch Batch 1300: Loss = 2.2214\n",
            "Training Loss: 2.4632\n",
            "Validation Loss: 2.4474\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 5/15\n",
            "Epoch Batch 10: Loss = 3.2775\n",
            "Epoch Batch 20: Loss = 2.3772\n",
            "Epoch Batch 30: Loss = 1.5530\n",
            "Epoch Batch 40: Loss = 2.3083\n",
            "Epoch Batch 50: Loss = 2.9493\n",
            "Epoch Batch 60: Loss = 2.9774\n",
            "Epoch Batch 70: Loss = 1.6957\n",
            "Epoch Batch 80: Loss = 1.5395\n",
            "Epoch Batch 90: Loss = 2.6674\n",
            "Epoch Batch 100: Loss = 3.3736\n",
            "Epoch Batch 110: Loss = 1.3907\n",
            "Epoch Batch 120: Loss = 3.2798\n",
            "Epoch Batch 130: Loss = 3.3570\n",
            "Epoch Batch 140: Loss = 2.1428\n",
            "Epoch Batch 150: Loss = 2.4322\n",
            "Epoch Batch 160: Loss = 1.6427\n",
            "Epoch Batch 170: Loss = 2.9769\n",
            "Epoch Batch 180: Loss = 2.8556\n",
            "Epoch Batch 190: Loss = 2.0663\n",
            "Epoch Batch 200: Loss = 2.8761\n",
            "Epoch Batch 210: Loss = 3.0275\n",
            "Epoch Batch 220: Loss = 2.6309\n",
            "Epoch Batch 230: Loss = 2.7173\n",
            "Epoch Batch 240: Loss = 2.9319\n",
            "Epoch Batch 250: Loss = 1.5693\n",
            "Epoch Batch 260: Loss = 2.5412\n",
            "Epoch Batch 270: Loss = 1.9751\n",
            "Epoch Batch 280: Loss = 1.5824\n",
            "Epoch Batch 290: Loss = 2.0219\n",
            "Epoch Batch 300: Loss = 2.4514\n",
            "Epoch Batch 310: Loss = 3.3194\n",
            "Epoch Batch 320: Loss = 2.4621\n",
            "Epoch Batch 330: Loss = 2.7548\n",
            "Epoch Batch 340: Loss = 2.5457\n",
            "Epoch Batch 350: Loss = 3.1478\n",
            "Epoch Batch 360: Loss = 2.4758\n",
            "Epoch Batch 370: Loss = 2.0837\n",
            "Epoch Batch 380: Loss = 2.4287\n",
            "Epoch Batch 390: Loss = 1.8281\n",
            "Epoch Batch 400: Loss = 1.6443\n",
            "Epoch Batch 410: Loss = 3.0019\n",
            "Epoch Batch 420: Loss = 3.2319\n",
            "Epoch Batch 430: Loss = 2.0257\n",
            "Epoch Batch 440: Loss = 2.9765\n",
            "Epoch Batch 450: Loss = 2.8382\n",
            "Epoch Batch 460: Loss = 3.6991\n",
            "Epoch Batch 470: Loss = 3.0462\n",
            "Epoch Batch 480: Loss = 3.3816\n",
            "Epoch Batch 490: Loss = 3.0812\n",
            "Epoch Batch 500: Loss = 2.3880\n",
            "Epoch Batch 510: Loss = 2.1177\n",
            "Epoch Batch 520: Loss = 1.7655\n",
            "Epoch Batch 530: Loss = 2.2870\n",
            "Epoch Batch 540: Loss = 2.5824\n",
            "Epoch Batch 550: Loss = 2.8890\n",
            "Epoch Batch 560: Loss = 2.9674\n",
            "Epoch Batch 570: Loss = 2.6553\n",
            "Epoch Batch 580: Loss = 2.6948\n",
            "Epoch Batch 590: Loss = 1.5628\n",
            "Epoch Batch 600: Loss = 3.0797\n",
            "Epoch Batch 610: Loss = 2.4913\n",
            "Epoch Batch 620: Loss = 2.5687\n",
            "Epoch Batch 630: Loss = 2.9720\n",
            "Epoch Batch 640: Loss = 1.7137\n",
            "Epoch Batch 650: Loss = 1.3204\n",
            "Epoch Batch 660: Loss = 2.3414\n",
            "Epoch Batch 670: Loss = 1.9123\n",
            "Epoch Batch 680: Loss = 2.8825\n",
            "Epoch Batch 690: Loss = 2.0955\n",
            "Epoch Batch 700: Loss = 2.8813\n",
            "Epoch Batch 710: Loss = 2.9752\n",
            "Epoch Batch 720: Loss = 3.4088\n",
            "Epoch Batch 730: Loss = 1.6612\n",
            "Epoch Batch 740: Loss = 2.7388\n",
            "Epoch Batch 750: Loss = 2.8113\n",
            "Epoch Batch 760: Loss = 2.7286\n",
            "Epoch Batch 770: Loss = 3.3570\n",
            "Epoch Batch 780: Loss = 2.7279\n",
            "Epoch Batch 790: Loss = 1.9855\n",
            "Epoch Batch 800: Loss = 2.5898\n",
            "Epoch Batch 810: Loss = 2.4368\n",
            "Epoch Batch 820: Loss = 2.5489\n",
            "Epoch Batch 830: Loss = 2.2858\n",
            "Epoch Batch 840: Loss = 1.4833\n",
            "Epoch Batch 850: Loss = 1.9568\n",
            "Epoch Batch 860: Loss = 3.0850\n",
            "Epoch Batch 870: Loss = 2.1710\n",
            "Epoch Batch 880: Loss = 1.8644\n",
            "Epoch Batch 890: Loss = 2.2774\n",
            "Epoch Batch 900: Loss = 2.8141\n",
            "Epoch Batch 910: Loss = 2.2651\n",
            "Epoch Batch 920: Loss = 2.9251\n",
            "Epoch Batch 930: Loss = 3.0486\n",
            "Epoch Batch 940: Loss = 3.1311\n",
            "Epoch Batch 950: Loss = 2.7128\n",
            "Epoch Batch 960: Loss = 2.7459\n",
            "Epoch Batch 970: Loss = 1.9379\n",
            "Epoch Batch 980: Loss = 2.5326\n",
            "Epoch Batch 990: Loss = 2.7445\n",
            "Epoch Batch 1000: Loss = 2.2590\n",
            "Epoch Batch 1010: Loss = 3.5621\n",
            "Epoch Batch 1020: Loss = 2.4463\n",
            "Epoch Batch 1030: Loss = 2.8290\n",
            "Epoch Batch 1040: Loss = 3.7607\n",
            "Epoch Batch 1050: Loss = 2.5358\n",
            "Epoch Batch 1060: Loss = 2.0221\n",
            "Epoch Batch 1070: Loss = 2.4934\n",
            "Epoch Batch 1080: Loss = 2.4546\n",
            "Epoch Batch 1090: Loss = 2.8560\n",
            "Epoch Batch 1100: Loss = 2.6843\n",
            "Epoch Batch 1110: Loss = 2.3464\n",
            "Epoch Batch 1120: Loss = 1.8180\n",
            "Epoch Batch 1130: Loss = 3.1096\n",
            "Epoch Batch 1140: Loss = 2.1834\n",
            "Epoch Batch 1150: Loss = 2.9906\n",
            "Epoch Batch 1160: Loss = 2.8361\n",
            "Epoch Batch 1170: Loss = 1.6225\n",
            "Epoch Batch 1180: Loss = 2.7348\n",
            "Epoch Batch 1190: Loss = 2.4387\n",
            "Epoch Batch 1200: Loss = 2.6123\n",
            "Epoch Batch 1210: Loss = 1.8205\n",
            "Epoch Batch 1220: Loss = 2.8141\n",
            "Epoch Batch 1230: Loss = 3.1125\n",
            "Epoch Batch 1240: Loss = 2.0561\n",
            "Epoch Batch 1250: Loss = 3.4769\n",
            "Epoch Batch 1260: Loss = 2.4879\n",
            "Epoch Batch 1270: Loss = 2.6841\n",
            "Epoch Batch 1280: Loss = 2.6219\n",
            "Epoch Batch 1290: Loss = 2.7309\n",
            "Epoch Batch 1300: Loss = 3.4861\n",
            "Training Loss: 2.4494\n",
            "Validation Loss: 2.4420\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch 6/15\n",
            "Epoch Batch 10: Loss = 2.3711\n",
            "Epoch Batch 20: Loss = 2.4180\n",
            "Epoch Batch 30: Loss = 2.8356\n",
            "Epoch Batch 40: Loss = 2.8336\n",
            "Epoch Batch 50: Loss = 1.3189\n",
            "Epoch Batch 60: Loss = 3.9041\n",
            "Epoch Batch 70: Loss = 1.6369\n",
            "Epoch Batch 80: Loss = 2.9642\n",
            "Epoch Batch 90: Loss = 2.8493\n",
            "Epoch Batch 100: Loss = 2.6055\n",
            "Epoch Batch 110: Loss = 3.0634\n",
            "Epoch Batch 120: Loss = 2.8850\n",
            "Epoch Batch 130: Loss = 1.4218\n",
            "Epoch Batch 140: Loss = 3.7369\n",
            "Epoch Batch 150: Loss = 3.4913\n",
            "Epoch Batch 160: Loss = 1.3945\n",
            "Epoch Batch 170: Loss = 2.7709\n",
            "Epoch Batch 180: Loss = 3.1557\n",
            "Epoch Batch 190: Loss = 0.8904\n",
            "Epoch Batch 200: Loss = 3.2535\n",
            "Epoch Batch 210: Loss = 2.8746\n",
            "Epoch Batch 220: Loss = 2.8441\n",
            "Epoch Batch 230: Loss = 1.8792\n",
            "Epoch Batch 240: Loss = 1.7886\n",
            "Epoch Batch 250: Loss = 1.7624\n",
            "Epoch Batch 260: Loss = 1.4730\n",
            "Epoch Batch 270: Loss = 2.5021\n",
            "Epoch Batch 280: Loss = 2.4030\n",
            "Epoch Batch 290: Loss = 3.0347\n",
            "Epoch Batch 300: Loss = 2.3547\n",
            "Epoch Batch 310: Loss = 2.2259\n",
            "Epoch Batch 320: Loss = 2.5686\n",
            "Epoch Batch 330: Loss = 2.6345\n",
            "Epoch Batch 340: Loss = 2.2784\n",
            "Epoch Batch 350: Loss = 3.1179\n",
            "Epoch Batch 360: Loss = 2.3971\n",
            "Epoch Batch 370: Loss = 2.4181\n",
            "Epoch Batch 380: Loss = 2.8623\n",
            "Epoch Batch 390: Loss = 2.0848\n",
            "Epoch Batch 400: Loss = 2.8173\n",
            "Epoch Batch 410: Loss = 2.9744\n",
            "Epoch Batch 420: Loss = 1.9790\n",
            "Epoch Batch 430: Loss = 1.7787\n",
            "Epoch Batch 440: Loss = 1.7891\n",
            "Epoch Batch 450: Loss = 2.8618\n",
            "Epoch Batch 460: Loss = 2.4642\n",
            "Epoch Batch 470: Loss = 2.4259\n",
            "Epoch Batch 480: Loss = 1.5822\n",
            "Epoch Batch 490: Loss = 3.4059\n",
            "Epoch Batch 500: Loss = 2.7256\n",
            "Epoch Batch 510: Loss = 2.8884\n",
            "Epoch Batch 520: Loss = 3.4886\n",
            "Epoch Batch 530: Loss = 3.0764\n",
            "Epoch Batch 540: Loss = 3.2265\n",
            "Epoch Batch 550: Loss = 2.2289\n",
            "Epoch Batch 560: Loss = 2.4062\n",
            "Epoch Batch 570: Loss = 3.1269\n",
            "Epoch Batch 580: Loss = 1.1911\n",
            "Epoch Batch 590: Loss = 2.3956\n",
            "Epoch Batch 600: Loss = 1.9558\n",
            "Epoch Batch 610: Loss = 2.9563\n",
            "Epoch Batch 620: Loss = 2.0410\n",
            "Epoch Batch 630: Loss = 3.2025\n",
            "Epoch Batch 640: Loss = 2.8663\n",
            "Epoch Batch 650: Loss = 2.0466\n",
            "Epoch Batch 660: Loss = 2.6245\n",
            "Epoch Batch 670: Loss = 1.8655\n",
            "Epoch Batch 680: Loss = 3.0118\n",
            "Epoch Batch 690: Loss = 2.4372\n",
            "Epoch Batch 700: Loss = 1.7276\n",
            "Epoch Batch 710: Loss = 1.0501\n",
            "Epoch Batch 720: Loss = 2.6017\n",
            "Epoch Batch 730: Loss = 1.7550\n",
            "Epoch Batch 740: Loss = 2.4139\n",
            "Epoch Batch 750: Loss = 1.4541\n",
            "Epoch Batch 760: Loss = 2.4507\n",
            "Epoch Batch 770: Loss = 1.9455\n",
            "Epoch Batch 780: Loss = 2.0770\n",
            "Epoch Batch 790: Loss = 2.8249\n",
            "Epoch Batch 800: Loss = 2.2214\n",
            "Epoch Batch 810: Loss = 3.5092\n",
            "Epoch Batch 820: Loss = 3.0274\n",
            "Epoch Batch 830: Loss = 1.9984\n",
            "Epoch Batch 840: Loss = 3.0549\n",
            "Epoch Batch 850: Loss = 3.0674\n",
            "Epoch Batch 860: Loss = 2.6303\n",
            "Epoch Batch 870: Loss = 1.9055\n",
            "Epoch Batch 880: Loss = 2.0879\n",
            "Epoch Batch 890: Loss = 2.0477\n",
            "Epoch Batch 900: Loss = 2.2137\n",
            "Epoch Batch 910: Loss = 3.1687\n",
            "Epoch Batch 920: Loss = 3.2657\n",
            "Epoch Batch 930: Loss = 1.5354\n",
            "Epoch Batch 940: Loss = 2.2509\n",
            "Epoch Batch 950: Loss = 3.0912\n",
            "Epoch Batch 960: Loss = 2.2944\n",
            "Epoch Batch 970: Loss = 2.4381\n",
            "Epoch Batch 980: Loss = 2.2945\n",
            "Epoch Batch 990: Loss = 1.9601\n",
            "Epoch Batch 1000: Loss = 2.2706\n",
            "Epoch Batch 1010: Loss = 2.6065\n",
            "Epoch Batch 1020: Loss = 2.5000\n",
            "Epoch Batch 1030: Loss = 2.2761\n",
            "Epoch Batch 1040: Loss = 1.9834\n",
            "Epoch Batch 1050: Loss = 2.4572\n",
            "Epoch Batch 1060: Loss = 2.4073\n",
            "Epoch Batch 1070: Loss = 3.3504\n",
            "Epoch Batch 1080: Loss = 2.1554\n",
            "Epoch Batch 1090: Loss = 2.6375\n",
            "Epoch Batch 1100: Loss = 1.3417\n",
            "Epoch Batch 1110: Loss = 1.7614\n",
            "Epoch Batch 1120: Loss = 3.6122\n",
            "Epoch Batch 1130: Loss = 2.3821\n",
            "Epoch Batch 1140: Loss = 2.7648\n",
            "Epoch Batch 1150: Loss = 1.2752\n",
            "Epoch Batch 1160: Loss = 2.6865\n",
            "Epoch Batch 1170: Loss = 2.0316\n",
            "Epoch Batch 1180: Loss = 3.1464\n",
            "Epoch Batch 1190: Loss = 2.5169\n",
            "Epoch Batch 1200: Loss = 2.8469\n",
            "Epoch Batch 1210: Loss = 2.0594\n",
            "Epoch Batch 1220: Loss = 1.8312\n",
            "Epoch Batch 1230: Loss = 1.3330\n",
            "Epoch Batch 1240: Loss = 2.7057\n",
            "Epoch Batch 1250: Loss = 3.2540\n",
            "Epoch Batch 1260: Loss = 2.9278\n",
            "Epoch Batch 1270: Loss = 1.8063\n",
            "Epoch Batch 1280: Loss = 2.6842\n",
            "Epoch Batch 1290: Loss = 1.5971\n",
            "Epoch Batch 1300: Loss = 2.8802\n",
            "Training Loss: 2.4445\n",
            "Validation Loss: 2.3914\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 7/15\n",
            "Epoch Batch 10: Loss = 2.7944\n",
            "Epoch Batch 20: Loss = 0.8327\n",
            "Epoch Batch 30: Loss = 2.1415\n",
            "Epoch Batch 40: Loss = 3.2894\n",
            "Epoch Batch 50: Loss = 2.7501\n",
            "Epoch Batch 60: Loss = 2.0392\n",
            "Epoch Batch 70: Loss = 2.6660\n",
            "Epoch Batch 80: Loss = 3.4698\n",
            "Epoch Batch 90: Loss = 3.1133\n",
            "Epoch Batch 100: Loss = 1.7250\n",
            "Epoch Batch 110: Loss = 2.7532\n",
            "Epoch Batch 120: Loss = 3.3739\n",
            "Epoch Batch 130: Loss = 3.3456\n",
            "Epoch Batch 140: Loss = 3.0909\n",
            "Epoch Batch 150: Loss = 2.4968\n",
            "Epoch Batch 160: Loss = 2.7968\n",
            "Epoch Batch 170: Loss = 1.4786\n",
            "Epoch Batch 180: Loss = 3.2248\n",
            "Epoch Batch 190: Loss = 2.4164\n",
            "Epoch Batch 200: Loss = 2.1342\n",
            "Epoch Batch 210: Loss = 2.2398\n",
            "Epoch Batch 220: Loss = 1.8722\n",
            "Epoch Batch 230: Loss = 2.4164\n",
            "Epoch Batch 240: Loss = 2.7350\n",
            "Epoch Batch 250: Loss = 1.9775\n",
            "Epoch Batch 260: Loss = 2.5836\n",
            "Epoch Batch 270: Loss = 2.1262\n",
            "Epoch Batch 280: Loss = 1.4764\n",
            "Epoch Batch 290: Loss = 2.8635\n",
            "Epoch Batch 300: Loss = 2.6932\n",
            "Epoch Batch 310: Loss = 2.2377\n",
            "Epoch Batch 320: Loss = 2.4412\n",
            "Epoch Batch 330: Loss = 1.7020\n",
            "Epoch Batch 340: Loss = 2.4585\n",
            "Epoch Batch 350: Loss = 1.3476\n",
            "Epoch Batch 360: Loss = 1.9484\n",
            "Epoch Batch 370: Loss = 2.1468\n",
            "Epoch Batch 380: Loss = 2.5969\n",
            "Epoch Batch 390: Loss = 2.8354\n",
            "Epoch Batch 400: Loss = 2.4754\n",
            "Epoch Batch 410: Loss = 1.8818\n",
            "Epoch Batch 420: Loss = 2.1104\n",
            "Epoch Batch 430: Loss = 2.5686\n",
            "Epoch Batch 440: Loss = 1.9821\n",
            "Epoch Batch 450: Loss = 2.1247\n",
            "Epoch Batch 460: Loss = 2.6027\n",
            "Epoch Batch 470: Loss = 1.8902\n",
            "Epoch Batch 480: Loss = 1.9948\n",
            "Epoch Batch 490: Loss = 2.5526\n",
            "Epoch Batch 500: Loss = 2.6921\n",
            "Epoch Batch 510: Loss = 2.9460\n",
            "Epoch Batch 520: Loss = 2.8127\n",
            "Epoch Batch 530: Loss = 2.9461\n",
            "Epoch Batch 540: Loss = 2.7476\n",
            "Epoch Batch 550: Loss = 2.5139\n",
            "Epoch Batch 560: Loss = 3.0007\n",
            "Epoch Batch 570: Loss = 1.8706\n",
            "Epoch Batch 580: Loss = 2.3844\n",
            "Epoch Batch 590: Loss = 2.2831\n",
            "Epoch Batch 600: Loss = 1.8510\n",
            "Epoch Batch 610: Loss = 2.9918\n",
            "Epoch Batch 620: Loss = 3.0446\n",
            "Epoch Batch 630: Loss = 2.5413\n",
            "Epoch Batch 640: Loss = 2.8492\n",
            "Epoch Batch 650: Loss = 2.2873\n",
            "Epoch Batch 660: Loss = 3.7826\n",
            "Epoch Batch 670: Loss = 2.3610\n",
            "Epoch Batch 680: Loss = 2.7114\n",
            "Epoch Batch 690: Loss = 1.5741\n",
            "Epoch Batch 700: Loss = 2.4808\n",
            "Epoch Batch 710: Loss = 2.7566\n",
            "Epoch Batch 720: Loss = 3.0865\n",
            "Epoch Batch 730: Loss = 3.0903\n",
            "Epoch Batch 740: Loss = 2.3412\n",
            "Epoch Batch 750: Loss = 2.7612\n",
            "Epoch Batch 760: Loss = 1.7840\n",
            "Epoch Batch 770: Loss = 2.7252\n",
            "Epoch Batch 780: Loss = 2.7416\n",
            "Epoch Batch 790: Loss = 2.9249\n",
            "Epoch Batch 800: Loss = 1.9951\n",
            "Epoch Batch 810: Loss = 3.1628\n",
            "Epoch Batch 820: Loss = 2.2704\n",
            "Epoch Batch 830: Loss = 2.0369\n",
            "Epoch Batch 840: Loss = 2.1560\n",
            "Epoch Batch 850: Loss = 2.1308\n",
            "Epoch Batch 860: Loss = 3.0113\n",
            "Epoch Batch 870: Loss = 2.1545\n",
            "Epoch Batch 880: Loss = 2.8810\n",
            "Epoch Batch 890: Loss = 1.9741\n",
            "Epoch Batch 900: Loss = 2.4554\n",
            "Epoch Batch 910: Loss = 2.4333\n",
            "Epoch Batch 920: Loss = 1.9868\n",
            "Epoch Batch 930: Loss = 1.4596\n",
            "Epoch Batch 940: Loss = 1.8281\n",
            "Epoch Batch 950: Loss = 1.7981\n",
            "Epoch Batch 960: Loss = 2.7373\n",
            "Epoch Batch 970: Loss = 2.6748\n",
            "Epoch Batch 980: Loss = 1.7508\n",
            "Epoch Batch 990: Loss = 2.5895\n",
            "Epoch Batch 1000: Loss = 1.8310\n",
            "Epoch Batch 1010: Loss = 2.7511\n",
            "Epoch Batch 1020: Loss = 2.8963\n",
            "Epoch Batch 1030: Loss = 3.2779\n",
            "Epoch Batch 1040: Loss = 2.8330\n",
            "Epoch Batch 1050: Loss = 2.6770\n",
            "Epoch Batch 1060: Loss = 2.4381\n",
            "Epoch Batch 1070: Loss = 2.2939\n",
            "Epoch Batch 1080: Loss = 2.7033\n",
            "Epoch Batch 1090: Loss = 2.8504\n",
            "Epoch Batch 1100: Loss = 2.7055\n",
            "Epoch Batch 1110: Loss = 2.5589\n",
            "Epoch Batch 1120: Loss = 2.0723\n",
            "Epoch Batch 1130: Loss = 2.1587\n",
            "Epoch Batch 1140: Loss = 3.3136\n",
            "Epoch Batch 1150: Loss = 2.8009\n",
            "Epoch Batch 1160: Loss = 2.9269\n",
            "Epoch Batch 1170: Loss = 2.9202\n",
            "Epoch Batch 1180: Loss = 2.8638\n",
            "Epoch Batch 1190: Loss = 2.5483\n",
            "Epoch Batch 1200: Loss = 2.0423\n",
            "Epoch Batch 1210: Loss = 2.4901\n",
            "Epoch Batch 1220: Loss = 2.4481\n",
            "Epoch Batch 1230: Loss = 2.5165\n",
            "Epoch Batch 1240: Loss = 2.1946\n",
            "Epoch Batch 1250: Loss = 2.0341\n",
            "Epoch Batch 1260: Loss = 2.0468\n",
            "Epoch Batch 1270: Loss = 2.0516\n",
            "Epoch Batch 1280: Loss = 2.4353\n",
            "Epoch Batch 1290: Loss = 2.3562\n",
            "Epoch Batch 1300: Loss = 2.6786\n",
            "Training Loss: 2.4180\n",
            "Validation Loss: 2.3807\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 8/15\n",
            "Epoch Batch 10: Loss = 2.3036\n",
            "Epoch Batch 20: Loss = 1.8978\n",
            "Epoch Batch 30: Loss = 1.6249\n",
            "Epoch Batch 40: Loss = 1.7410\n",
            "Epoch Batch 50: Loss = 2.3872\n",
            "Epoch Batch 60: Loss = 2.8638\n",
            "Epoch Batch 70: Loss = 2.1853\n",
            "Epoch Batch 80: Loss = 2.1677\n",
            "Epoch Batch 90: Loss = 3.0728\n",
            "Epoch Batch 100: Loss = 2.2743\n",
            "Epoch Batch 110: Loss = 2.1664\n",
            "Epoch Batch 120: Loss = 1.6015\n",
            "Epoch Batch 130: Loss = 2.1757\n",
            "Epoch Batch 140: Loss = 1.7231\n",
            "Epoch Batch 150: Loss = 2.7959\n",
            "Epoch Batch 160: Loss = 2.1807\n",
            "Epoch Batch 170: Loss = 1.6626\n",
            "Epoch Batch 180: Loss = 2.6999\n",
            "Epoch Batch 190: Loss = 2.6498\n",
            "Epoch Batch 200: Loss = 2.2029\n",
            "Epoch Batch 210: Loss = 1.8341\n",
            "Epoch Batch 220: Loss = 2.1480\n",
            "Epoch Batch 230: Loss = 1.7719\n",
            "Epoch Batch 240: Loss = 2.1444\n",
            "Epoch Batch 250: Loss = 2.9375\n",
            "Epoch Batch 260: Loss = 3.0265\n",
            "Epoch Batch 270: Loss = 2.6138\n",
            "Epoch Batch 280: Loss = 3.0276\n",
            "Epoch Batch 290: Loss = 2.0600\n",
            "Epoch Batch 300: Loss = 2.4970\n",
            "Epoch Batch 310: Loss = 2.4311\n",
            "Epoch Batch 320: Loss = 2.2273\n",
            "Epoch Batch 330: Loss = 2.1381\n",
            "Epoch Batch 340: Loss = 1.9250\n",
            "Epoch Batch 350: Loss = 2.4230\n",
            "Epoch Batch 360: Loss = 2.3014\n",
            "Epoch Batch 370: Loss = 2.3476\n",
            "Epoch Batch 380: Loss = 3.5857\n",
            "Epoch Batch 390: Loss = 3.2095\n",
            "Epoch Batch 400: Loss = 2.2740\n",
            "Epoch Batch 410: Loss = 2.1829\n",
            "Epoch Batch 420: Loss = 2.6202\n",
            "Epoch Batch 430: Loss = 3.4374\n",
            "Epoch Batch 440: Loss = 2.5642\n",
            "Epoch Batch 450: Loss = 3.0106\n",
            "Epoch Batch 460: Loss = 2.7106\n",
            "Epoch Batch 470: Loss = 2.4017\n",
            "Epoch Batch 480: Loss = 2.7981\n",
            "Epoch Batch 490: Loss = 2.0971\n",
            "Epoch Batch 500: Loss = 2.5103\n",
            "Epoch Batch 510: Loss = 3.1845\n",
            "Epoch Batch 520: Loss = 2.4542\n",
            "Epoch Batch 530: Loss = 1.7969\n",
            "Epoch Batch 540: Loss = 2.6102\n",
            "Epoch Batch 550: Loss = 2.6644\n",
            "Epoch Batch 560: Loss = 2.4361\n",
            "Epoch Batch 570: Loss = 2.7004\n",
            "Epoch Batch 580: Loss = 2.6314\n",
            "Epoch Batch 590: Loss = 1.5490\n",
            "Epoch Batch 600: Loss = 1.3473\n",
            "Epoch Batch 610: Loss = 2.6970\n",
            "Epoch Batch 620: Loss = 2.4360\n",
            "Epoch Batch 630: Loss = 1.8179\n",
            "Epoch Batch 640: Loss = 1.6324\n",
            "Epoch Batch 650: Loss = 1.4691\n",
            "Epoch Batch 660: Loss = 2.1007\n",
            "Epoch Batch 670: Loss = 2.9455\n",
            "Epoch Batch 680: Loss = 2.1550\n",
            "Epoch Batch 690: Loss = 2.8230\n",
            "Epoch Batch 700: Loss = 2.0886\n",
            "Epoch Batch 710: Loss = 3.1649\n",
            "Epoch Batch 720: Loss = 2.8174\n",
            "Epoch Batch 730: Loss = 1.7183\n",
            "Epoch Batch 740: Loss = 2.0866\n",
            "Epoch Batch 750: Loss = 2.6691\n",
            "Epoch Batch 760: Loss = 2.3812\n",
            "Epoch Batch 770: Loss = 1.6976\n",
            "Epoch Batch 780: Loss = 2.6614\n",
            "Epoch Batch 790: Loss = 1.6729\n",
            "Epoch Batch 800: Loss = 2.1970\n",
            "Epoch Batch 810: Loss = 1.4986\n",
            "Epoch Batch 820: Loss = 2.9950\n",
            "Epoch Batch 830: Loss = 2.1820\n",
            "Epoch Batch 840: Loss = 1.5865\n",
            "Epoch Batch 850: Loss = 1.1932\n",
            "Epoch Batch 860: Loss = 3.1073\n",
            "Epoch Batch 870: Loss = 1.9478\n",
            "Epoch Batch 880: Loss = 2.5381\n",
            "Epoch Batch 890: Loss = 2.7696\n",
            "Epoch Batch 900: Loss = 2.5619\n",
            "Epoch Batch 910: Loss = 2.5836\n",
            "Epoch Batch 920: Loss = 2.6626\n",
            "Epoch Batch 930: Loss = 2.5418\n",
            "Epoch Batch 940: Loss = 2.6683\n",
            "Epoch Batch 950: Loss = 1.9457\n",
            "Epoch Batch 960: Loss = 2.7413\n",
            "Epoch Batch 970: Loss = 2.1883\n",
            "Epoch Batch 980: Loss = 1.5816\n",
            "Epoch Batch 990: Loss = 2.2080\n",
            "Epoch Batch 1000: Loss = 1.8776\n",
            "Epoch Batch 1010: Loss = 2.7302\n",
            "Epoch Batch 1020: Loss = 1.8143\n",
            "Epoch Batch 1030: Loss = 2.1405\n",
            "Epoch Batch 1040: Loss = 2.6731\n",
            "Epoch Batch 1050: Loss = 2.3422\n",
            "Epoch Batch 1060: Loss = 1.4938\n",
            "Epoch Batch 1070: Loss = 2.8357\n",
            "Epoch Batch 1080: Loss = 1.9001\n",
            "Epoch Batch 1090: Loss = 3.3764\n",
            "Epoch Batch 1100: Loss = 2.9156\n",
            "Epoch Batch 1110: Loss = 2.8688\n",
            "Epoch Batch 1120: Loss = 2.6631\n",
            "Epoch Batch 1130: Loss = 2.4134\n",
            "Epoch Batch 1140: Loss = 2.2939\n",
            "Epoch Batch 1150: Loss = 2.8681\n",
            "Epoch Batch 1160: Loss = 2.2211\n",
            "Epoch Batch 1170: Loss = 1.1629\n",
            "Epoch Batch 1180: Loss = 2.6914\n",
            "Epoch Batch 1190: Loss = 2.5978\n",
            "Epoch Batch 1200: Loss = 2.9097\n",
            "Epoch Batch 1210: Loss = 2.1867\n",
            "Epoch Batch 1220: Loss = 2.2449\n",
            "Epoch Batch 1230: Loss = 2.2157\n",
            "Epoch Batch 1240: Loss = 1.0193\n",
            "Epoch Batch 1250: Loss = 2.9912\n",
            "Epoch Batch 1260: Loss = 1.6256\n",
            "Epoch Batch 1270: Loss = 2.3738\n",
            "Epoch Batch 1280: Loss = 3.0209\n",
            "Epoch Batch 1290: Loss = 2.0606\n",
            "Epoch Batch 1300: Loss = 2.0766\n",
            "Training Loss: 2.4042\n",
            "Validation Loss: 2.3806\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 9/15\n",
            "Epoch Batch 10: Loss = 1.7823\n",
            "Epoch Batch 20: Loss = 3.3289\n",
            "Epoch Batch 30: Loss = 3.4249\n",
            "Epoch Batch 40: Loss = 2.7474\n",
            "Epoch Batch 50: Loss = 1.7535\n",
            "Epoch Batch 60: Loss = 2.1727\n",
            "Epoch Batch 70: Loss = 2.1447\n",
            "Epoch Batch 80: Loss = 2.3249\n",
            "Epoch Batch 90: Loss = 2.2120\n",
            "Epoch Batch 100: Loss = 3.2994\n",
            "Epoch Batch 110: Loss = 1.8083\n",
            "Epoch Batch 120: Loss = 1.5484\n",
            "Epoch Batch 130: Loss = 2.5489\n",
            "Epoch Batch 140: Loss = 2.8534\n",
            "Epoch Batch 150: Loss = 2.8844\n",
            "Epoch Batch 160: Loss = 2.8537\n",
            "Epoch Batch 170: Loss = 1.9091\n",
            "Epoch Batch 180: Loss = 1.5278\n",
            "Epoch Batch 190: Loss = 2.9012\n",
            "Epoch Batch 200: Loss = 2.8330\n",
            "Epoch Batch 210: Loss = 2.5736\n",
            "Epoch Batch 220: Loss = 1.4721\n",
            "Epoch Batch 230: Loss = 2.2825\n",
            "Epoch Batch 240: Loss = 2.5547\n",
            "Epoch Batch 250: Loss = 2.5976\n",
            "Epoch Batch 260: Loss = 2.5251\n",
            "Epoch Batch 270: Loss = 3.0410\n",
            "Epoch Batch 280: Loss = 2.3446\n",
            "Epoch Batch 290: Loss = 2.5229\n",
            "Epoch Batch 300: Loss = 3.2960\n",
            "Epoch Batch 310: Loss = 3.3596\n",
            "Epoch Batch 320: Loss = 2.2612\n",
            "Epoch Batch 330: Loss = 2.1056\n",
            "Epoch Batch 340: Loss = 2.4847\n",
            "Epoch Batch 350: Loss = 2.1850\n",
            "Epoch Batch 360: Loss = 2.6964\n",
            "Epoch Batch 370: Loss = 3.1997\n",
            "Epoch Batch 380: Loss = 2.3769\n",
            "Epoch Batch 390: Loss = 2.4633\n",
            "Epoch Batch 400: Loss = 2.3772\n",
            "Epoch Batch 410: Loss = 1.6422\n",
            "Epoch Batch 420: Loss = 3.0385\n",
            "Epoch Batch 430: Loss = 2.7872\n",
            "Epoch Batch 440: Loss = 3.4670\n",
            "Epoch Batch 450: Loss = 1.5984\n",
            "Epoch Batch 460: Loss = 2.7736\n",
            "Epoch Batch 470: Loss = 2.1272\n",
            "Epoch Batch 480: Loss = 2.8324\n",
            "Epoch Batch 490: Loss = 2.7116\n",
            "Epoch Batch 500: Loss = 2.6790\n",
            "Epoch Batch 510: Loss = 3.0449\n",
            "Epoch Batch 520: Loss = 2.5050\n",
            "Epoch Batch 530: Loss = 2.7469\n",
            "Epoch Batch 540: Loss = 1.4559\n",
            "Epoch Batch 550: Loss = 2.2060\n",
            "Epoch Batch 560: Loss = 2.8224\n",
            "Epoch Batch 570: Loss = 2.1679\n",
            "Epoch Batch 580: Loss = 2.5109\n",
            "Epoch Batch 590: Loss = 1.7134\n",
            "Epoch Batch 600: Loss = 2.7538\n",
            "Epoch Batch 610: Loss = 2.4611\n",
            "Epoch Batch 620: Loss = 2.1056\n",
            "Epoch Batch 630: Loss = 2.6061\n",
            "Epoch Batch 640: Loss = 2.1728\n",
            "Epoch Batch 650: Loss = 2.1213\n",
            "Epoch Batch 660: Loss = 2.9451\n",
            "Epoch Batch 670: Loss = 2.2925\n",
            "Epoch Batch 680: Loss = 3.5392\n",
            "Epoch Batch 690: Loss = 1.5880\n",
            "Epoch Batch 700: Loss = 2.6421\n",
            "Epoch Batch 710: Loss = 2.6329\n",
            "Epoch Batch 720: Loss = 1.9039\n",
            "Epoch Batch 730: Loss = 2.3396\n",
            "Epoch Batch 740: Loss = 2.5970\n",
            "Epoch Batch 750: Loss = 3.0266\n",
            "Epoch Batch 760: Loss = 1.9604\n",
            "Epoch Batch 770: Loss = 1.7369\n",
            "Epoch Batch 780: Loss = 1.9180\n",
            "Epoch Batch 790: Loss = 2.3631\n",
            "Epoch Batch 800: Loss = 1.8401\n",
            "Epoch Batch 810: Loss = 2.2800\n",
            "Epoch Batch 820: Loss = 2.0916\n",
            "Epoch Batch 830: Loss = 2.4407\n",
            "Epoch Batch 840: Loss = 2.6688\n",
            "Epoch Batch 850: Loss = 2.8450\n",
            "Epoch Batch 860: Loss = 2.1984\n",
            "Epoch Batch 870: Loss = 3.1913\n",
            "Epoch Batch 880: Loss = 2.0905\n",
            "Epoch Batch 890: Loss = 2.2181\n",
            "Epoch Batch 900: Loss = 2.3442\n",
            "Epoch Batch 910: Loss = 3.2937\n",
            "Epoch Batch 920: Loss = 1.3496\n",
            "Epoch Batch 930: Loss = 2.1251\n",
            "Epoch Batch 940: Loss = 1.7699\n",
            "Epoch Batch 950: Loss = 2.5415\n",
            "Epoch Batch 960: Loss = 2.3341\n",
            "Epoch Batch 970: Loss = 2.7608\n",
            "Epoch Batch 980: Loss = 2.9284\n",
            "Epoch Batch 990: Loss = 1.4182\n",
            "Epoch Batch 1000: Loss = 2.0288\n",
            "Epoch Batch 1010: Loss = 2.6736\n",
            "Epoch Batch 1020: Loss = 2.1046\n",
            "Epoch Batch 1030: Loss = 3.0651\n",
            "Epoch Batch 1040: Loss = 2.6561\n",
            "Epoch Batch 1050: Loss = 2.5255\n",
            "Epoch Batch 1060: Loss = 1.9973\n",
            "Epoch Batch 1070: Loss = 2.0748\n",
            "Epoch Batch 1080: Loss = 2.5615\n",
            "Epoch Batch 1090: Loss = 2.6381\n",
            "Epoch Batch 1100: Loss = 1.8580\n",
            "Epoch Batch 1110: Loss = 2.5664\n",
            "Epoch Batch 1120: Loss = 2.1311\n",
            "Epoch Batch 1130: Loss = 0.9566\n",
            "Epoch Batch 1140: Loss = 3.2830\n",
            "Epoch Batch 1150: Loss = 1.9041\n",
            "Epoch Batch 1160: Loss = 3.0430\n",
            "Epoch Batch 1170: Loss = 3.4565\n",
            "Epoch Batch 1180: Loss = 2.8413\n",
            "Epoch Batch 1190: Loss = 1.6692\n",
            "Epoch Batch 1200: Loss = 1.9442\n",
            "Epoch Batch 1210: Loss = 3.0167\n",
            "Epoch Batch 1220: Loss = 3.1298\n",
            "Epoch Batch 1230: Loss = 2.3381\n",
            "Epoch Batch 1240: Loss = 2.7431\n",
            "Epoch Batch 1250: Loss = 1.9336\n",
            "Epoch Batch 1260: Loss = 2.9801\n",
            "Epoch Batch 1270: Loss = 1.9356\n",
            "Epoch Batch 1280: Loss = 1.7668\n",
            "Epoch Batch 1290: Loss = 1.3361\n",
            "Epoch Batch 1300: Loss = 2.2710\n",
            "Training Loss: 2.4005\n",
            "Validation Loss: 2.3764\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 10/15\n",
            "Epoch Batch 10: Loss = 2.8454\n",
            "Epoch Batch 20: Loss = 1.5926\n",
            "Epoch Batch 30: Loss = 2.9172\n",
            "Epoch Batch 40: Loss = 1.8804\n",
            "Epoch Batch 50: Loss = 2.6245\n",
            "Epoch Batch 60: Loss = 2.7645\n",
            "Epoch Batch 70: Loss = 3.2572\n",
            "Epoch Batch 80: Loss = 2.1086\n",
            "Epoch Batch 90: Loss = 1.9783\n",
            "Epoch Batch 100: Loss = 2.1450\n",
            "Epoch Batch 110: Loss = 1.6707\n",
            "Epoch Batch 120: Loss = 1.7287\n",
            "Epoch Batch 130: Loss = 2.9028\n",
            "Epoch Batch 140: Loss = 2.3212\n",
            "Epoch Batch 150: Loss = 2.9944\n",
            "Epoch Batch 160: Loss = 2.1358\n",
            "Epoch Batch 170: Loss = 1.3679\n",
            "Epoch Batch 180: Loss = 2.4487\n",
            "Epoch Batch 190: Loss = 2.3270\n",
            "Epoch Batch 200: Loss = 3.2304\n",
            "Epoch Batch 210: Loss = 1.8055\n",
            "Epoch Batch 220: Loss = 1.9915\n",
            "Epoch Batch 230: Loss = 2.7567\n",
            "Epoch Batch 240: Loss = 2.7207\n",
            "Epoch Batch 250: Loss = 3.0555\n",
            "Epoch Batch 260: Loss = 3.0309\n",
            "Epoch Batch 270: Loss = 2.0364\n",
            "Epoch Batch 280: Loss = 2.0636\n",
            "Epoch Batch 290: Loss = 1.8196\n",
            "Epoch Batch 300: Loss = 1.5171\n",
            "Epoch Batch 310: Loss = 1.1149\n",
            "Epoch Batch 320: Loss = 2.4559\n",
            "Epoch Batch 330: Loss = 2.0696\n",
            "Epoch Batch 340: Loss = 2.3290\n",
            "Epoch Batch 350: Loss = 2.7784\n",
            "Epoch Batch 360: Loss = 3.0910\n",
            "Epoch Batch 370: Loss = 2.1436\n",
            "Epoch Batch 380: Loss = 2.8227\n",
            "Epoch Batch 390: Loss = 1.7211\n",
            "Epoch Batch 400: Loss = 2.8010\n",
            "Epoch Batch 410: Loss = 2.5889\n",
            "Epoch Batch 420: Loss = 2.5329\n",
            "Epoch Batch 430: Loss = 2.7345\n",
            "Epoch Batch 440: Loss = 2.7244\n",
            "Epoch Batch 450: Loss = 1.6642\n",
            "Epoch Batch 460: Loss = 2.5700\n",
            "Epoch Batch 470: Loss = 2.7143\n",
            "Epoch Batch 480: Loss = 2.7950\n",
            "Epoch Batch 490: Loss = 1.5229\n",
            "Epoch Batch 500: Loss = 1.7356\n",
            "Epoch Batch 510: Loss = 3.1204\n",
            "Epoch Batch 520: Loss = 2.4014\n",
            "Epoch Batch 530: Loss = 2.1161\n",
            "Epoch Batch 540: Loss = 1.5965\n",
            "Epoch Batch 550: Loss = 2.4485\n",
            "Epoch Batch 560: Loss = 2.0432\n",
            "Epoch Batch 570: Loss = 1.9115\n",
            "Epoch Batch 580: Loss = 2.7320\n",
            "Epoch Batch 590: Loss = 2.1992\n",
            "Epoch Batch 600: Loss = 2.1828\n",
            "Epoch Batch 610: Loss = 2.0731\n",
            "Epoch Batch 620: Loss = 2.1097\n",
            "Epoch Batch 630: Loss = 2.7497\n",
            "Epoch Batch 640: Loss = 2.2425\n",
            "Epoch Batch 650: Loss = 3.4281\n",
            "Epoch Batch 660: Loss = 2.7652\n",
            "Epoch Batch 670: Loss = 2.8827\n",
            "Epoch Batch 680: Loss = 3.0555\n",
            "Epoch Batch 690: Loss = 2.0356\n",
            "Epoch Batch 700: Loss = 2.9011\n",
            "Epoch Batch 710: Loss = 1.7294\n",
            "Epoch Batch 720: Loss = 2.0162\n",
            "Epoch Batch 730: Loss = 1.8806\n",
            "Epoch Batch 740: Loss = 2.3719\n",
            "Epoch Batch 750: Loss = 2.8143\n",
            "Epoch Batch 760: Loss = 1.9086\n",
            "Epoch Batch 770: Loss = 1.8386\n",
            "Epoch Batch 780: Loss = 2.8857\n",
            "Epoch Batch 790: Loss = 2.4471\n",
            "Epoch Batch 800: Loss = 3.2568\n",
            "Epoch Batch 810: Loss = 2.3372\n",
            "Epoch Batch 820: Loss = 1.8960\n",
            "Epoch Batch 830: Loss = 2.2133\n",
            "Epoch Batch 840: Loss = 2.3494\n",
            "Epoch Batch 850: Loss = 1.3792\n",
            "Epoch Batch 860: Loss = 2.7228\n",
            "Epoch Batch 870: Loss = 3.5975\n",
            "Epoch Batch 880: Loss = 2.8605\n",
            "Epoch Batch 890: Loss = 2.1609\n",
            "Epoch Batch 900: Loss = 2.7983\n",
            "Epoch Batch 910: Loss = 3.1489\n",
            "Epoch Batch 920: Loss = 2.4126\n",
            "Epoch Batch 930: Loss = 2.3016\n",
            "Epoch Batch 940: Loss = 2.2127\n",
            "Epoch Batch 950: Loss = 1.7009\n",
            "Epoch Batch 960: Loss = 2.7112\n",
            "Epoch Batch 970: Loss = 2.4191\n",
            "Epoch Batch 980: Loss = 2.5877\n",
            "Epoch Batch 990: Loss = 2.6233\n",
            "Epoch Batch 1000: Loss = 1.5882\n",
            "Epoch Batch 1010: Loss = 2.7430\n",
            "Epoch Batch 1020: Loss = 2.2729\n",
            "Epoch Batch 1030: Loss = 2.9662\n",
            "Epoch Batch 1040: Loss = 2.0277\n",
            "Epoch Batch 1050: Loss = 2.8239\n",
            "Epoch Batch 1060: Loss = 1.7065\n",
            "Epoch Batch 1070: Loss = 3.2064\n",
            "Epoch Batch 1080: Loss = 1.9238\n",
            "Epoch Batch 1090: Loss = 1.8536\n",
            "Epoch Batch 1100: Loss = 2.6042\n",
            "Epoch Batch 1110: Loss = 1.9442\n",
            "Epoch Batch 1120: Loss = 2.8004\n",
            "Epoch Batch 1130: Loss = 2.0840\n",
            "Epoch Batch 1140: Loss = 3.4154\n",
            "Epoch Batch 1150: Loss = 3.4917\n",
            "Epoch Batch 1160: Loss = 2.0045\n",
            "Epoch Batch 1170: Loss = 2.8130\n",
            "Epoch Batch 1180: Loss = 2.9770\n",
            "Epoch Batch 1190: Loss = 2.7871\n",
            "Epoch Batch 1200: Loss = 2.3622\n",
            "Epoch Batch 1210: Loss = 3.2826\n",
            "Epoch Batch 1220: Loss = 2.4120\n",
            "Epoch Batch 1230: Loss = 2.0705\n",
            "Epoch Batch 1240: Loss = 2.5283\n",
            "Epoch Batch 1250: Loss = 1.8860\n",
            "Epoch Batch 1260: Loss = 2.7033\n",
            "Epoch Batch 1270: Loss = 2.3792\n",
            "Epoch Batch 1280: Loss = 1.5669\n",
            "Epoch Batch 1290: Loss = 2.0795\n",
            "Epoch Batch 1300: Loss = 1.9636\n",
            "Training Loss: 2.3936\n",
            "Validation Loss: 2.3768\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 11/15\n",
            "Epoch Batch 10: Loss = 2.4152\n",
            "Epoch Batch 20: Loss = 1.4883\n",
            "Epoch Batch 30: Loss = 2.9104\n",
            "Epoch Batch 40: Loss = 2.0329\n",
            "Epoch Batch 50: Loss = 2.1960\n",
            "Epoch Batch 60: Loss = 2.8225\n",
            "Epoch Batch 70: Loss = 2.0668\n",
            "Epoch Batch 80: Loss = 2.9426\n",
            "Epoch Batch 90: Loss = 2.9150\n",
            "Epoch Batch 100: Loss = 2.5751\n",
            "Epoch Batch 110: Loss = 1.9408\n",
            "Epoch Batch 120: Loss = 1.5813\n",
            "Epoch Batch 130: Loss = 3.0389\n",
            "Epoch Batch 140: Loss = 2.3446\n",
            "Epoch Batch 150: Loss = 2.0993\n",
            "Epoch Batch 160: Loss = 2.1906\n",
            "Epoch Batch 170: Loss = 1.7943\n",
            "Epoch Batch 180: Loss = 1.5697\n",
            "Epoch Batch 190: Loss = 2.7793\n",
            "Epoch Batch 200: Loss = 2.0923\n",
            "Epoch Batch 210: Loss = 2.6672\n",
            "Epoch Batch 220: Loss = 3.1433\n",
            "Epoch Batch 230: Loss = 2.8686\n",
            "Epoch Batch 240: Loss = 2.9155\n",
            "Epoch Batch 250: Loss = 2.3962\n",
            "Epoch Batch 260: Loss = 2.2018\n",
            "Epoch Batch 270: Loss = 2.1814\n",
            "Epoch Batch 280: Loss = 2.4562\n",
            "Epoch Batch 290: Loss = 1.2246\n",
            "Epoch Batch 300: Loss = 2.0382\n",
            "Epoch Batch 310: Loss = 2.3702\n",
            "Epoch Batch 320: Loss = 2.5328\n",
            "Epoch Batch 330: Loss = 2.3596\n",
            "Epoch Batch 340: Loss = 2.0457\n",
            "Epoch Batch 350: Loss = 2.5686\n",
            "Epoch Batch 360: Loss = 2.9598\n",
            "Epoch Batch 370: Loss = 2.1892\n",
            "Epoch Batch 380: Loss = 2.6689\n",
            "Epoch Batch 390: Loss = 2.9129\n",
            "Epoch Batch 400: Loss = 2.9969\n",
            "Epoch Batch 410: Loss = 3.3833\n",
            "Epoch Batch 420: Loss = 1.3605\n",
            "Epoch Batch 430: Loss = 2.7755\n",
            "Epoch Batch 440: Loss = 3.0504\n",
            "Epoch Batch 450: Loss = 2.8337\n",
            "Epoch Batch 460: Loss = 1.8990\n",
            "Epoch Batch 470: Loss = 2.4188\n",
            "Epoch Batch 480: Loss = 2.4164\n",
            "Epoch Batch 490: Loss = 1.8074\n",
            "Epoch Batch 500: Loss = 3.0564\n",
            "Epoch Batch 510: Loss = 1.8601\n",
            "Epoch Batch 520: Loss = 2.3704\n",
            "Epoch Batch 530: Loss = 2.7675\n",
            "Epoch Batch 540: Loss = 2.1068\n",
            "Epoch Batch 550: Loss = 1.9500\n",
            "Epoch Batch 560: Loss = 1.8724\n",
            "Epoch Batch 570: Loss = 3.1738\n",
            "Epoch Batch 580: Loss = 2.6551\n",
            "Epoch Batch 590: Loss = 2.2319\n",
            "Epoch Batch 600: Loss = 2.1079\n",
            "Epoch Batch 610: Loss = 2.8729\n",
            "Epoch Batch 620: Loss = 3.4707\n",
            "Epoch Batch 630: Loss = 2.6097\n",
            "Epoch Batch 640: Loss = 3.1619\n",
            "Epoch Batch 650: Loss = 1.8460\n",
            "Epoch Batch 660: Loss = 1.0790\n",
            "Epoch Batch 670: Loss = 3.2356\n",
            "Epoch Batch 680: Loss = 1.1020\n",
            "Epoch Batch 690: Loss = 2.6111\n",
            "Epoch Batch 700: Loss = 2.0055\n",
            "Epoch Batch 710: Loss = 2.0977\n",
            "Epoch Batch 720: Loss = 2.4374\n",
            "Epoch Batch 730: Loss = 3.1843\n",
            "Epoch Batch 740: Loss = 2.3662\n",
            "Epoch Batch 750: Loss = 2.1212\n",
            "Epoch Batch 760: Loss = 2.2695\n",
            "Epoch Batch 770: Loss = 2.4733\n",
            "Epoch Batch 780: Loss = 2.7533\n",
            "Epoch Batch 790: Loss = 1.9683\n",
            "Epoch Batch 800: Loss = 2.9021\n",
            "Epoch Batch 810: Loss = 3.0241\n",
            "Epoch Batch 820: Loss = 1.9602\n",
            "Epoch Batch 830: Loss = 2.4206\n",
            "Epoch Batch 840: Loss = 2.8766\n",
            "Epoch Batch 850: Loss = 2.7459\n",
            "Epoch Batch 860: Loss = 1.0961\n",
            "Epoch Batch 870: Loss = 1.5735\n",
            "Epoch Batch 880: Loss = 2.6322\n",
            "Epoch Batch 890: Loss = 2.4855\n",
            "Epoch Batch 900: Loss = 1.5600\n",
            "Epoch Batch 910: Loss = 2.7670\n",
            "Epoch Batch 920: Loss = 2.5992\n",
            "Epoch Batch 930: Loss = 2.5721\n",
            "Epoch Batch 940: Loss = 2.1813\n",
            "Epoch Batch 950: Loss = 1.3914\n",
            "Epoch Batch 960: Loss = 1.7524\n",
            "Epoch Batch 970: Loss = 2.4427\n",
            "Epoch Batch 980: Loss = 1.9087\n",
            "Epoch Batch 990: Loss = 1.6899\n",
            "Epoch Batch 1000: Loss = 2.8306\n",
            "Epoch Batch 1010: Loss = 2.5132\n",
            "Epoch Batch 1020: Loss = 2.8577\n",
            "Epoch Batch 1030: Loss = 2.7553\n",
            "Epoch Batch 1040: Loss = 1.0838\n",
            "Epoch Batch 1050: Loss = 1.4906\n",
            "Epoch Batch 1060: Loss = 2.1410\n",
            "Epoch Batch 1070: Loss = 2.1770\n",
            "Epoch Batch 1080: Loss = 2.9848\n",
            "Epoch Batch 1090: Loss = 1.6925\n",
            "Epoch Batch 1100: Loss = 3.1103\n",
            "Epoch Batch 1110: Loss = 1.7373\n",
            "Epoch Batch 1120: Loss = 2.5074\n",
            "Epoch Batch 1130: Loss = 2.2310\n",
            "Epoch Batch 1140: Loss = 1.8380\n",
            "Epoch Batch 1150: Loss = 2.8572\n",
            "Epoch Batch 1160: Loss = 2.8828\n",
            "Epoch Batch 1170: Loss = 2.4791\n",
            "Epoch Batch 1180: Loss = 2.3462\n",
            "Epoch Batch 1190: Loss = 1.2252\n",
            "Epoch Batch 1200: Loss = 2.5610\n",
            "Epoch Batch 1210: Loss = 1.9641\n",
            "Epoch Batch 1220: Loss = 2.8161\n",
            "Epoch Batch 1230: Loss = 2.9389\n",
            "Epoch Batch 1240: Loss = 1.8935\n",
            "Epoch Batch 1250: Loss = 2.0722\n",
            "Epoch Batch 1260: Loss = 1.9376\n",
            "Epoch Batch 1270: Loss = 3.2480\n",
            "Epoch Batch 1280: Loss = 1.6183\n",
            "Epoch Batch 1290: Loss = 2.3365\n",
            "Epoch Batch 1300: Loss = 1.8041\n",
            "Training Loss: 2.3887\n",
            "Validation Loss: 2.3765\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 12/15\n",
            "Epoch Batch 10: Loss = 2.7483\n",
            "Epoch Batch 20: Loss = 1.1744\n",
            "Epoch Batch 30: Loss = 1.6475\n",
            "Epoch Batch 40: Loss = 2.8047\n",
            "Epoch Batch 50: Loss = 1.9501\n",
            "Epoch Batch 60: Loss = 2.5664\n",
            "Epoch Batch 70: Loss = 2.5011\n",
            "Epoch Batch 80: Loss = 1.9956\n",
            "Epoch Batch 90: Loss = 3.3259\n",
            "Epoch Batch 100: Loss = 1.5619\n",
            "Epoch Batch 110: Loss = 2.6994\n",
            "Epoch Batch 120: Loss = 2.7557\n",
            "Epoch Batch 130: Loss = 2.1397\n",
            "Epoch Batch 140: Loss = 2.3811\n",
            "Epoch Batch 150: Loss = 2.9749\n",
            "Epoch Batch 160: Loss = 2.5311\n",
            "Epoch Batch 170: Loss = 3.0312\n",
            "Epoch Batch 180: Loss = 3.4173\n",
            "Epoch Batch 190: Loss = 2.2832\n",
            "Epoch Batch 200: Loss = 2.8662\n",
            "Epoch Batch 210: Loss = 1.7116\n",
            "Epoch Batch 220: Loss = 2.5667\n",
            "Epoch Batch 230: Loss = 2.4812\n",
            "Epoch Batch 240: Loss = 2.7123\n",
            "Epoch Batch 250: Loss = 2.7932\n",
            "Epoch Batch 260: Loss = 2.5420\n",
            "Epoch Batch 270: Loss = 2.7547\n",
            "Epoch Batch 280: Loss = 2.8636\n",
            "Epoch Batch 290: Loss = 2.6918\n",
            "Epoch Batch 300: Loss = 2.2868\n",
            "Epoch Batch 310: Loss = 1.9446\n",
            "Epoch Batch 320: Loss = 1.3670\n",
            "Epoch Batch 330: Loss = 3.6846\n",
            "Epoch Batch 340: Loss = 2.0988\n",
            "Epoch Batch 350: Loss = 1.9658\n",
            "Epoch Batch 360: Loss = 1.9905\n",
            "Epoch Batch 370: Loss = 2.4188\n",
            "Epoch Batch 380: Loss = 2.0996\n",
            "Epoch Batch 390: Loss = 2.5836\n",
            "Epoch Batch 400: Loss = 1.8310\n",
            "Epoch Batch 410: Loss = 3.1709\n",
            "Epoch Batch 420: Loss = 2.3886\n",
            "Epoch Batch 430: Loss = 2.6441\n",
            "Epoch Batch 440: Loss = 2.9261\n",
            "Epoch Batch 450: Loss = 2.2608\n",
            "Epoch Batch 460: Loss = 2.3920\n",
            "Epoch Batch 470: Loss = 2.0432\n",
            "Epoch Batch 480: Loss = 2.1170\n",
            "Epoch Batch 490: Loss = 2.7830\n",
            "Epoch Batch 500: Loss = 1.9389\n",
            "Epoch Batch 510: Loss = 2.9890\n",
            "Epoch Batch 520: Loss = 2.0720\n",
            "Epoch Batch 530: Loss = 2.8318\n",
            "Epoch Batch 540: Loss = 1.8646\n",
            "Epoch Batch 550: Loss = 2.7486\n",
            "Epoch Batch 560: Loss = 2.4868\n",
            "Epoch Batch 570: Loss = 2.1195\n",
            "Epoch Batch 580: Loss = 2.8180\n",
            "Epoch Batch 590: Loss = 2.4728\n",
            "Epoch Batch 600: Loss = 2.6628\n",
            "Epoch Batch 610: Loss = 1.4736\n",
            "Epoch Batch 620: Loss = 2.0721\n",
            "Epoch Batch 630: Loss = 1.3817\n",
            "Epoch Batch 640: Loss = 2.7451\n",
            "Epoch Batch 650: Loss = 2.4655\n",
            "Epoch Batch 660: Loss = 2.9527\n",
            "Epoch Batch 670: Loss = 2.1363\n",
            "Epoch Batch 680: Loss = 1.6054\n",
            "Epoch Batch 690: Loss = 1.9880\n",
            "Epoch Batch 700: Loss = 1.5165\n",
            "Epoch Batch 710: Loss = 2.8135\n",
            "Epoch Batch 720: Loss = 2.8159\n",
            "Epoch Batch 730: Loss = 1.4762\n",
            "Epoch Batch 740: Loss = 1.6769\n",
            "Epoch Batch 750: Loss = 3.0718\n",
            "Epoch Batch 760: Loss = 2.5770\n",
            "Epoch Batch 770: Loss = 1.1445\n",
            "Epoch Batch 780: Loss = 1.2576\n",
            "Epoch Batch 790: Loss = 2.9419\n",
            "Epoch Batch 800: Loss = 1.7940\n",
            "Epoch Batch 810: Loss = 2.2986\n",
            "Epoch Batch 820: Loss = 2.5755\n",
            "Epoch Batch 830: Loss = 2.9538\n",
            "Epoch Batch 840: Loss = 2.8964\n",
            "Epoch Batch 850: Loss = 2.5493\n",
            "Epoch Batch 860: Loss = 2.6266\n",
            "Epoch Batch 870: Loss = 2.7618\n",
            "Epoch Batch 880: Loss = 2.0432\n",
            "Epoch Batch 890: Loss = 2.2241\n",
            "Epoch Batch 900: Loss = 1.9119\n",
            "Epoch Batch 910: Loss = 2.4362\n",
            "Epoch Batch 920: Loss = 2.6356\n",
            "Epoch Batch 930: Loss = 2.1570\n",
            "Epoch Batch 940: Loss = 2.3801\n",
            "Epoch Batch 950: Loss = 1.6292\n",
            "Epoch Batch 960: Loss = 2.1855\n",
            "Epoch Batch 970: Loss = 2.7631\n",
            "Epoch Batch 980: Loss = 2.1435\n",
            "Epoch Batch 990: Loss = 2.1078\n",
            "Epoch Batch 1000: Loss = 2.1224\n",
            "Epoch Batch 1010: Loss = 2.5865\n",
            "Epoch Batch 1020: Loss = 1.7047\n",
            "Epoch Batch 1030: Loss = 1.9329\n",
            "Epoch Batch 1040: Loss = 2.0312\n",
            "Epoch Batch 1050: Loss = 2.1523\n",
            "Epoch Batch 1060: Loss = 1.7243\n",
            "Epoch Batch 1070: Loss = 2.7006\n",
            "Epoch Batch 1080: Loss = 2.6579\n",
            "Epoch Batch 1090: Loss = 1.7779\n",
            "Epoch Batch 1100: Loss = 2.2707\n",
            "Epoch Batch 1110: Loss = 2.4576\n",
            "Epoch Batch 1120: Loss = 1.5262\n",
            "Epoch Batch 1130: Loss = 2.7112\n",
            "Epoch Batch 1140: Loss = 1.2959\n",
            "Epoch Batch 1150: Loss = 1.5268\n",
            "Epoch Batch 1160: Loss = 2.4768\n",
            "Epoch Batch 1170: Loss = 3.1413\n",
            "Epoch Batch 1180: Loss = 2.5310\n",
            "Epoch Batch 1190: Loss = 2.7496\n",
            "Epoch Batch 1200: Loss = 2.9315\n",
            "Epoch Batch 1210: Loss = 1.8199\n",
            "Epoch Batch 1220: Loss = 2.6745\n",
            "Epoch Batch 1230: Loss = 3.4198\n",
            "Epoch Batch 1240: Loss = 2.6081\n",
            "Epoch Batch 1250: Loss = 2.4073\n",
            "Epoch Batch 1260: Loss = 2.2812\n",
            "Epoch Batch 1270: Loss = 2.7847\n",
            "Epoch Batch 1280: Loss = 1.5716\n",
            "Epoch Batch 1290: Loss = 2.3090\n",
            "Epoch Batch 1300: Loss = 1.8751\n",
            "Training Loss: 2.3878\n",
            "Validation Loss: 2.3777\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 13/15\n",
            "Epoch Batch 10: Loss = 2.9103\n",
            "Epoch Batch 20: Loss = 2.3423\n",
            "Epoch Batch 30: Loss = 1.9376\n",
            "Epoch Batch 40: Loss = 1.0610\n",
            "Epoch Batch 50: Loss = 1.9957\n",
            "Epoch Batch 60: Loss = 1.8094\n",
            "Epoch Batch 70: Loss = 2.6408\n",
            "Epoch Batch 80: Loss = 1.9037\n",
            "Epoch Batch 90: Loss = 3.3627\n",
            "Epoch Batch 100: Loss = 2.0445\n",
            "Epoch Batch 110: Loss = 1.7762\n",
            "Epoch Batch 120: Loss = 3.1415\n",
            "Epoch Batch 130: Loss = 3.2220\n",
            "Epoch Batch 140: Loss = 2.1760\n",
            "Epoch Batch 150: Loss = 1.4252\n",
            "Epoch Batch 160: Loss = 2.9976\n",
            "Epoch Batch 170: Loss = 3.0859\n",
            "Epoch Batch 180: Loss = 2.2326\n",
            "Epoch Batch 190: Loss = 2.6504\n",
            "Epoch Batch 200: Loss = 2.3244\n",
            "Epoch Batch 210: Loss = 2.0025\n",
            "Epoch Batch 220: Loss = 2.7202\n",
            "Epoch Batch 230: Loss = 2.2932\n",
            "Epoch Batch 240: Loss = 1.6345\n",
            "Epoch Batch 250: Loss = 2.5605\n",
            "Epoch Batch 260: Loss = 2.5745\n",
            "Epoch Batch 270: Loss = 2.0622\n",
            "Epoch Batch 280: Loss = 2.3535\n",
            "Epoch Batch 290: Loss = 2.1003\n",
            "Epoch Batch 300: Loss = 1.7692\n",
            "Epoch Batch 310: Loss = 2.8609\n",
            "Epoch Batch 320: Loss = 1.8217\n",
            "Epoch Batch 330: Loss = 1.9765\n",
            "Epoch Batch 340: Loss = 2.7670\n",
            "Epoch Batch 350: Loss = 3.1644\n",
            "Epoch Batch 360: Loss = 2.3629\n",
            "Epoch Batch 370: Loss = 1.3003\n",
            "Epoch Batch 380: Loss = 3.1920\n",
            "Epoch Batch 390: Loss = 2.2665\n",
            "Epoch Batch 400: Loss = 2.9004\n",
            "Epoch Batch 410: Loss = 2.6156\n",
            "Epoch Batch 420: Loss = 2.0496\n",
            "Epoch Batch 430: Loss = 2.5072\n",
            "Epoch Batch 440: Loss = 2.1320\n",
            "Epoch Batch 450: Loss = 2.5634\n",
            "Epoch Batch 460: Loss = 2.6399\n",
            "Epoch Batch 470: Loss = 1.6489\n",
            "Epoch Batch 480: Loss = 2.7087\n",
            "Epoch Batch 490: Loss = 3.1662\n",
            "Epoch Batch 500: Loss = 2.3451\n",
            "Epoch Batch 510: Loss = 2.7532\n",
            "Epoch Batch 520: Loss = 2.5273\n",
            "Epoch Batch 530: Loss = 1.0938\n",
            "Epoch Batch 540: Loss = 1.4422\n",
            "Epoch Batch 550: Loss = 1.4116\n",
            "Epoch Batch 560: Loss = 3.0481\n",
            "Epoch Batch 570: Loss = 1.8585\n",
            "Epoch Batch 580: Loss = 2.5635\n",
            "Epoch Batch 590: Loss = 2.9944\n",
            "Epoch Batch 600: Loss = 3.0672\n",
            "Epoch Batch 610: Loss = 2.7445\n",
            "Epoch Batch 620: Loss = 2.8084\n",
            "Epoch Batch 630: Loss = 2.8420\n",
            "Epoch Batch 640: Loss = 1.2196\n",
            "Epoch Batch 650: Loss = 2.6443\n",
            "Epoch Batch 660: Loss = 1.7878\n",
            "Epoch Batch 670: Loss = 2.1008\n",
            "Epoch Batch 680: Loss = 2.6211\n",
            "Epoch Batch 690: Loss = 1.8307\n",
            "Epoch Batch 700: Loss = 1.3080\n",
            "Epoch Batch 710: Loss = 2.7914\n",
            "Epoch Batch 720: Loss = 1.8338\n",
            "Epoch Batch 730: Loss = 2.4869\n",
            "Epoch Batch 740: Loss = 1.9167\n",
            "Epoch Batch 750: Loss = 2.6834\n",
            "Epoch Batch 760: Loss = 2.0999\n",
            "Epoch Batch 770: Loss = 1.9082\n",
            "Epoch Batch 780: Loss = 2.6279\n",
            "Epoch Batch 790: Loss = 1.5969\n",
            "Epoch Batch 800: Loss = 1.7309\n",
            "Epoch Batch 810: Loss = 2.8102\n",
            "Epoch Batch 820: Loss = 3.1290\n",
            "Epoch Batch 830: Loss = 0.9888\n",
            "Epoch Batch 840: Loss = 1.7873\n",
            "Epoch Batch 850: Loss = 2.4995\n",
            "Epoch Batch 860: Loss = 2.4984\n",
            "Epoch Batch 870: Loss = 2.1715\n",
            "Epoch Batch 880: Loss = 3.0001\n",
            "Epoch Batch 890: Loss = 2.3714\n",
            "Epoch Batch 900: Loss = 2.1151\n",
            "Epoch Batch 910: Loss = 2.1174\n",
            "Epoch Batch 920: Loss = 2.3670\n",
            "Epoch Batch 930: Loss = 2.6865\n",
            "Epoch Batch 940: Loss = 2.3262\n",
            "Epoch Batch 950: Loss = 2.7968\n",
            "Epoch Batch 960: Loss = 1.1726\n",
            "Epoch Batch 970: Loss = 2.3879\n",
            "Epoch Batch 980: Loss = 3.3991\n",
            "Epoch Batch 990: Loss = 2.8992\n",
            "Epoch Batch 1000: Loss = 2.1284\n",
            "Epoch Batch 1010: Loss = 2.7470\n",
            "Epoch Batch 1020: Loss = 3.1864\n",
            "Epoch Batch 1030: Loss = 2.0815\n",
            "Epoch Batch 1040: Loss = 2.3982\n",
            "Epoch Batch 1050: Loss = 2.6578\n",
            "Epoch Batch 1060: Loss = 2.1852\n",
            "Epoch Batch 1070: Loss = 2.2300\n",
            "Epoch Batch 1080: Loss = 3.1961\n",
            "Epoch Batch 1090: Loss = 2.5170\n",
            "Epoch Batch 1100: Loss = 2.3907\n",
            "Epoch Batch 1110: Loss = 2.3980\n",
            "Epoch Batch 1120: Loss = 1.7353\n",
            "Epoch Batch 1130: Loss = 2.3442\n",
            "Epoch Batch 1140: Loss = 2.5208\n",
            "Epoch Batch 1150: Loss = 2.0727\n",
            "Epoch Batch 1160: Loss = 2.8227\n",
            "Epoch Batch 1170: Loss = 1.6646\n",
            "Epoch Batch 1180: Loss = 2.8217\n",
            "Epoch Batch 1190: Loss = 3.1878\n",
            "Epoch Batch 1200: Loss = 1.9132\n",
            "Epoch Batch 1210: Loss = 2.0367\n",
            "Epoch Batch 1220: Loss = 3.0655\n",
            "Epoch Batch 1230: Loss = 2.9326\n",
            "Epoch Batch 1240: Loss = 2.8202\n",
            "Epoch Batch 1250: Loss = 2.9066\n",
            "Epoch Batch 1260: Loss = 2.4449\n",
            "Epoch Batch 1270: Loss = 3.1955\n",
            "Epoch Batch 1280: Loss = 2.9483\n",
            "Epoch Batch 1290: Loss = 2.9710\n",
            "Epoch Batch 1300: Loss = 1.8147\n",
            "Training Loss: 2.3950\n",
            "Validation Loss: 2.3697\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 14/15\n",
            "Epoch Batch 10: Loss = 2.0250\n",
            "Epoch Batch 20: Loss = 2.0218\n",
            "Epoch Batch 30: Loss = 2.8685\n",
            "Epoch Batch 40: Loss = 2.0451\n",
            "Epoch Batch 50: Loss = 2.5797\n",
            "Epoch Batch 60: Loss = 1.9021\n",
            "Epoch Batch 70: Loss = 3.1326\n",
            "Epoch Batch 80: Loss = 1.9545\n",
            "Epoch Batch 90: Loss = 2.8612\n",
            "Epoch Batch 100: Loss = 2.5894\n",
            "Epoch Batch 110: Loss = 2.5484\n",
            "Epoch Batch 120: Loss = 2.6127\n",
            "Epoch Batch 130: Loss = 1.8521\n",
            "Epoch Batch 140: Loss = 2.6101\n",
            "Epoch Batch 150: Loss = 1.1972\n",
            "Epoch Batch 160: Loss = 2.6799\n",
            "Epoch Batch 170: Loss = 2.2854\n",
            "Epoch Batch 180: Loss = 3.0100\n",
            "Epoch Batch 190: Loss = 2.4394\n",
            "Epoch Batch 200: Loss = 1.5726\n",
            "Epoch Batch 210: Loss = 2.3605\n",
            "Epoch Batch 220: Loss = 2.6459\n",
            "Epoch Batch 230: Loss = 2.3659\n",
            "Epoch Batch 240: Loss = 2.5418\n",
            "Epoch Batch 250: Loss = 1.8111\n",
            "Epoch Batch 260: Loss = 3.3607\n",
            "Epoch Batch 270: Loss = 3.1131\n",
            "Epoch Batch 280: Loss = 2.7174\n",
            "Epoch Batch 290: Loss = 1.7497\n",
            "Epoch Batch 300: Loss = 2.9036\n",
            "Epoch Batch 310: Loss = 2.3837\n",
            "Epoch Batch 320: Loss = 3.1676\n",
            "Epoch Batch 330: Loss = 2.5759\n",
            "Epoch Batch 340: Loss = 2.0137\n",
            "Epoch Batch 350: Loss = 1.2835\n",
            "Epoch Batch 360: Loss = 2.2653\n",
            "Epoch Batch 370: Loss = 2.3010\n",
            "Epoch Batch 380: Loss = 3.2775\n",
            "Epoch Batch 390: Loss = 2.5432\n",
            "Epoch Batch 400: Loss = 2.1482\n",
            "Epoch Batch 410: Loss = 2.0874\n",
            "Epoch Batch 420: Loss = 2.7249\n",
            "Epoch Batch 430: Loss = 2.3886\n",
            "Epoch Batch 440: Loss = 2.0722\n",
            "Epoch Batch 450: Loss = 3.1827\n",
            "Epoch Batch 460: Loss = 2.5533\n",
            "Epoch Batch 470: Loss = 1.7428\n",
            "Epoch Batch 480: Loss = 1.3657\n",
            "Epoch Batch 490: Loss = 1.6583\n",
            "Epoch Batch 500: Loss = 3.1799\n",
            "Epoch Batch 510: Loss = 2.7825\n",
            "Epoch Batch 520: Loss = 2.2683\n",
            "Epoch Batch 530: Loss = 2.0676\n",
            "Epoch Batch 540: Loss = 2.8002\n",
            "Epoch Batch 550: Loss = 1.9434\n",
            "Epoch Batch 560: Loss = 2.5168\n",
            "Epoch Batch 570: Loss = 2.0669\n",
            "Epoch Batch 580: Loss = 3.0506\n",
            "Epoch Batch 590: Loss = 1.7077\n",
            "Epoch Batch 600: Loss = 1.9571\n",
            "Epoch Batch 610: Loss = 2.3505\n",
            "Epoch Batch 620: Loss = 2.4173\n",
            "Epoch Batch 630: Loss = 3.6897\n",
            "Epoch Batch 640: Loss = 3.3820\n",
            "Epoch Batch 650: Loss = 1.9644\n",
            "Epoch Batch 660: Loss = 2.5213\n",
            "Epoch Batch 670: Loss = 2.1104\n",
            "Epoch Batch 680: Loss = 1.5275\n",
            "Epoch Batch 690: Loss = 2.8675\n",
            "Epoch Batch 700: Loss = 1.6898\n",
            "Epoch Batch 710: Loss = 3.3486\n",
            "Epoch Batch 720: Loss = 2.4765\n",
            "Epoch Batch 730: Loss = 2.3278\n",
            "Epoch Batch 740: Loss = 3.0025\n",
            "Epoch Batch 750: Loss = 2.7349\n",
            "Epoch Batch 760: Loss = 3.2598\n",
            "Epoch Batch 770: Loss = 2.4600\n",
            "Epoch Batch 780: Loss = 2.2976\n",
            "Epoch Batch 790: Loss = 2.2532\n",
            "Epoch Batch 800: Loss = 2.1275\n",
            "Epoch Batch 810: Loss = 2.9070\n",
            "Epoch Batch 820: Loss = 2.3369\n",
            "Epoch Batch 830: Loss = 1.5939\n",
            "Epoch Batch 840: Loss = 2.1049\n",
            "Epoch Batch 850: Loss = 2.8856\n",
            "Epoch Batch 860: Loss = 2.5161\n",
            "Epoch Batch 870: Loss = 2.8837\n",
            "Epoch Batch 880: Loss = 2.9475\n",
            "Epoch Batch 890: Loss = 2.2394\n",
            "Epoch Batch 900: Loss = 1.8253\n",
            "Epoch Batch 910: Loss = 3.0545\n",
            "Epoch Batch 920: Loss = 2.5053\n",
            "Epoch Batch 930: Loss = 2.5175\n",
            "Epoch Batch 940: Loss = 2.5791\n",
            "Epoch Batch 950: Loss = 2.7340\n",
            "Epoch Batch 960: Loss = 2.9418\n",
            "Epoch Batch 970: Loss = 1.1262\n",
            "Epoch Batch 980: Loss = 2.7220\n",
            "Epoch Batch 990: Loss = 1.7881\n",
            "Epoch Batch 1000: Loss = 2.6311\n",
            "Epoch Batch 1010: Loss = 1.9148\n",
            "Epoch Batch 1020: Loss = 2.2134\n",
            "Epoch Batch 1030: Loss = 2.0684\n",
            "Epoch Batch 1040: Loss = 2.4479\n",
            "Epoch Batch 1050: Loss = 2.1130\n",
            "Epoch Batch 1060: Loss = 2.7950\n",
            "Epoch Batch 1070: Loss = 3.1026\n",
            "Epoch Batch 1080: Loss = 2.5759\n",
            "Epoch Batch 1090: Loss = 3.0577\n",
            "Epoch Batch 1100: Loss = 2.4531\n",
            "Epoch Batch 1110: Loss = 2.2446\n",
            "Epoch Batch 1120: Loss = 2.5211\n",
            "Epoch Batch 1130: Loss = 1.0369\n",
            "Epoch Batch 1140: Loss = 2.1050\n",
            "Epoch Batch 1150: Loss = 1.3140\n",
            "Epoch Batch 1160: Loss = 1.9613\n",
            "Epoch Batch 1170: Loss = 2.3433\n",
            "Epoch Batch 1180: Loss = 3.0954\n",
            "Epoch Batch 1190: Loss = 2.6035\n",
            "Epoch Batch 1200: Loss = 1.0718\n",
            "Epoch Batch 1210: Loss = 2.5726\n",
            "Epoch Batch 1220: Loss = 1.3706\n",
            "Epoch Batch 1230: Loss = 2.5276\n",
            "Epoch Batch 1240: Loss = 2.1376\n",
            "Epoch Batch 1250: Loss = 2.1472\n",
            "Epoch Batch 1260: Loss = 1.9859\n",
            "Epoch Batch 1270: Loss = 1.3336\n",
            "Epoch Batch 1280: Loss = 1.8936\n",
            "Epoch Batch 1290: Loss = 2.8139\n",
            "Epoch Batch 1300: Loss = 2.9080\n",
            "Training Loss: 2.3860\n",
            "Validation Loss: 2.3692\n",
            "Validation BLEU Score: 0.0000\n",
            "Epoch 15/15\n",
            "Epoch Batch 10: Loss = 1.7751\n",
            "Epoch Batch 20: Loss = 2.9522\n",
            "Epoch Batch 30: Loss = 2.5352\n",
            "Epoch Batch 40: Loss = 2.3461\n",
            "Epoch Batch 50: Loss = 1.8846\n",
            "Epoch Batch 60: Loss = 3.1725\n",
            "Epoch Batch 70: Loss = 2.6704\n",
            "Epoch Batch 80: Loss = 3.3649\n",
            "Epoch Batch 90: Loss = 2.5214\n",
            "Epoch Batch 100: Loss = 1.4871\n",
            "Epoch Batch 110: Loss = 2.6153\n",
            "Epoch Batch 120: Loss = 2.3113\n",
            "Epoch Batch 130: Loss = 3.2791\n",
            "Epoch Batch 140: Loss = 2.8862\n",
            "Epoch Batch 150: Loss = 1.5697\n",
            "Epoch Batch 160: Loss = 2.1076\n",
            "Epoch Batch 170: Loss = 2.0622\n",
            "Epoch Batch 180: Loss = 2.9942\n",
            "Epoch Batch 190: Loss = 2.3638\n",
            "Epoch Batch 200: Loss = 3.5815\n",
            "Epoch Batch 210: Loss = 3.0635\n",
            "Epoch Batch 220: Loss = 2.6560\n",
            "Epoch Batch 230: Loss = 2.8248\n",
            "Epoch Batch 240: Loss = 1.2864\n",
            "Epoch Batch 250: Loss = 1.6988\n",
            "Epoch Batch 260: Loss = 2.3907\n",
            "Epoch Batch 270: Loss = 3.2203\n",
            "Epoch Batch 280: Loss = 2.4861\n",
            "Epoch Batch 290: Loss = 1.7917\n",
            "Epoch Batch 300: Loss = 1.6118\n",
            "Epoch Batch 310: Loss = 2.4569\n",
            "Epoch Batch 320: Loss = 2.4971\n",
            "Epoch Batch 330: Loss = 1.8134\n",
            "Epoch Batch 340: Loss = 1.8772\n",
            "Epoch Batch 350: Loss = 2.2828\n",
            "Epoch Batch 360: Loss = 2.4490\n",
            "Epoch Batch 370: Loss = 2.9522\n",
            "Epoch Batch 380: Loss = 2.0710\n",
            "Epoch Batch 390: Loss = 1.5050\n",
            "Epoch Batch 400: Loss = 2.2729\n",
            "Epoch Batch 410: Loss = 2.1950\n",
            "Epoch Batch 420: Loss = 1.8720\n",
            "Epoch Batch 430: Loss = 1.5095\n",
            "Epoch Batch 440: Loss = 2.4765\n",
            "Epoch Batch 450: Loss = 1.9457\n",
            "Epoch Batch 460: Loss = 1.7342\n",
            "Epoch Batch 470: Loss = 2.1857\n",
            "Epoch Batch 480: Loss = 2.2689\n",
            "Epoch Batch 490: Loss = 1.3561\n",
            "Epoch Batch 500: Loss = 2.6871\n",
            "Epoch Batch 510: Loss = 3.1369\n",
            "Epoch Batch 520: Loss = 2.4041\n",
            "Epoch Batch 530: Loss = 2.3934\n",
            "Epoch Batch 540: Loss = 3.0481\n",
            "Epoch Batch 550: Loss = 2.7315\n",
            "Epoch Batch 560: Loss = 2.2242\n",
            "Epoch Batch 570: Loss = 1.9525\n",
            "Epoch Batch 580: Loss = 2.3516\n",
            "Epoch Batch 590: Loss = 1.8488\n",
            "Epoch Batch 600: Loss = 1.6537\n",
            "Epoch Batch 610: Loss = 1.8587\n",
            "Epoch Batch 620: Loss = 2.0470\n",
            "Epoch Batch 630: Loss = 2.9639\n",
            "Epoch Batch 640: Loss = 2.7179\n",
            "Epoch Batch 650: Loss = 1.9212\n",
            "Epoch Batch 660: Loss = 2.7365\n",
            "Epoch Batch 670: Loss = 1.9980\n",
            "Epoch Batch 680: Loss = 1.9357\n",
            "Epoch Batch 690: Loss = 2.0503\n",
            "Epoch Batch 700: Loss = 2.7709\n",
            "Epoch Batch 710: Loss = 1.9119\n",
            "Epoch Batch 720: Loss = 2.0797\n",
            "Epoch Batch 730: Loss = 2.2734\n",
            "Epoch Batch 740: Loss = 1.6831\n",
            "Epoch Batch 750: Loss = 2.4224\n",
            "Epoch Batch 760: Loss = 2.9462\n",
            "Epoch Batch 770: Loss = 3.3144\n",
            "Epoch Batch 780: Loss = 2.9786\n",
            "Epoch Batch 790: Loss = 2.3156\n",
            "Epoch Batch 800: Loss = 1.5502\n",
            "Epoch Batch 810: Loss = 1.8122\n",
            "Epoch Batch 820: Loss = 3.4907\n",
            "Epoch Batch 830: Loss = 2.6169\n",
            "Epoch Batch 840: Loss = 2.1583\n",
            "Epoch Batch 850: Loss = 2.8679\n",
            "Epoch Batch 860: Loss = 2.3626\n",
            "Epoch Batch 870: Loss = 2.0642\n",
            "Epoch Batch 880: Loss = 2.6348\n",
            "Epoch Batch 890: Loss = 2.3517\n",
            "Epoch Batch 900: Loss = 1.0889\n",
            "Epoch Batch 910: Loss = 2.4082\n",
            "Epoch Batch 920: Loss = 3.2267\n",
            "Epoch Batch 930: Loss = 2.7035\n",
            "Epoch Batch 940: Loss = 1.3209\n",
            "Epoch Batch 950: Loss = 2.1128\n",
            "Epoch Batch 960: Loss = 3.1088\n",
            "Epoch Batch 970: Loss = 2.9098\n",
            "Epoch Batch 980: Loss = 2.3216\n",
            "Epoch Batch 990: Loss = 2.8181\n",
            "Epoch Batch 1000: Loss = 3.1507\n",
            "Epoch Batch 1010: Loss = 2.4384\n",
            "Epoch Batch 1020: Loss = 3.1793\n",
            "Epoch Batch 1030: Loss = 3.3559\n",
            "Epoch Batch 1040: Loss = 2.8380\n",
            "Epoch Batch 1050: Loss = 3.2804\n",
            "Epoch Batch 1060: Loss = 2.4600\n",
            "Epoch Batch 1070: Loss = 2.9164\n",
            "Epoch Batch 1080: Loss = 2.5038\n",
            "Epoch Batch 1090: Loss = 2.1724\n",
            "Epoch Batch 1100: Loss = 2.0719\n",
            "Epoch Batch 1110: Loss = 2.7518\n",
            "Epoch Batch 1120: Loss = 2.1819\n",
            "Epoch Batch 1130: Loss = 2.9775\n",
            "Epoch Batch 1140: Loss = 1.8748\n",
            "Epoch Batch 1150: Loss = 2.6600\n",
            "Epoch Batch 1160: Loss = 2.2440\n",
            "Epoch Batch 1170: Loss = 2.0058\n",
            "Epoch Batch 1180: Loss = 1.3945\n",
            "Epoch Batch 1190: Loss = 2.1204\n",
            "Epoch Batch 1200: Loss = 2.3230\n",
            "Epoch Batch 1210: Loss = 3.1360\n",
            "Epoch Batch 1220: Loss = 2.7618\n",
            "Epoch Batch 1230: Loss = 2.6831\n",
            "Epoch Batch 1240: Loss = 2.7573\n",
            "Epoch Batch 1250: Loss = 3.4624\n",
            "Epoch Batch 1260: Loss = 2.4581\n",
            "Epoch Batch 1270: Loss = 2.8835\n",
            "Epoch Batch 1280: Loss = 2.8853\n",
            "Epoch Batch 1290: Loss = 2.2885\n",
            "Epoch Batch 1300: Loss = 2.5603\n",
            "Training Loss: 2.3856\n",
            "Validation Loss: 2.3687\n",
            "Validation BLEU Score: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'image_to_cap_model.pth')"
      ],
      "metadata": {
        "id": "7Kq6OQtm3LdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My bleu score is clearly very low, and my losses are very high. I want to print out example predicted captions to see what this model is doing."
      ],
      "metadata": {
        "id": "mt8uCV_M7Cdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_predictions(outputs, tokenizer):\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=-1)\n",
        "    predicted_indices = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "    for i in range(min(3, predicted_indices.size(0))):  # Ensures no out-of-bounds error\n",
        "        predicted_tokens = predicted_indices[i].cpu().tolist()\n",
        "        caption = tokenizer.decode(predicted_tokens, skip_special_tokens=True)\n",
        "        print(f\"Predicted Caption [{i}]: {caption}\")\n",
        "\n",
        "        # Iterate over each time step in the sequence to print top predictions\n",
        "        for t in range(predicted_indices.size(1)):\n",
        "            top_probs, top_indices = torch.topk(probabilities[i, t], 5)\n",
        "            print(f\"Time step {t}:\")\n",
        "            for prob, idx in zip(top_probs, top_indices):\n",
        "                token = tokenizer.convert_ids_to_tokens(idx.item())\n",
        "                print(f\"  Token: {token}, Probability: {prob.item()}\")"
      ],
      "metadata": {
        "id": "5uf1c0nhyO32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        images = batch['images'].to(device)\n",
        "        captions = batch['captions'].to(device)\n",
        "\n",
        "        for i, caption_ids in enumerate(captions[:3]):  # Loop through the first few captions in the batch\n",
        "          caption = tokenizer.decode(caption_ids.cpu().tolist(), skip_special_tokens=True)\n",
        "          print(f\"Decoded Caption [{i}]: {caption}\")\n",
        "\n",
        "          # excluding the last word in the caption (<end> token)\n",
        "          outputs = model(images, captions[:, :-1])\n",
        "\n",
        "          # offset the caption by one to the right (exclude the <start> token)\n",
        "          loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          if (i + 1) % 10 == 0:\n",
        "              print(f\"Epoch Batch {i+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "          print_predictions(outputs, tokenizer)\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    return average_loss\n"
      ],
      "metadata": {
        "id": "obYaWYmozLCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(model, train_loader, val_loader, optimizer, criterion, 10, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ipNkY4l8zWhs",
        "outputId": "bfba52f6-0f62-4220-8eb9-efe4282a9410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Decoded Caption [0]: blech, i don't like the dizzying geometric design and overall i don't understand it and it makes me dizzy\n",
            "Sample outputs: tensor([[-0.0118, -0.0081,  0.0201,  ..., -0.0408,  0.0143,  0.0431],\n",
            "        [-0.0157, -0.0216,  0.0095,  ..., -0.0490,  0.0231,  0.0488],\n",
            "        [-0.0161, -0.0247,  0.0020,  ..., -0.0526,  0.0267,  0.0541],\n",
            "        [-0.0158, -0.0244, -0.0030,  ..., -0.0540,  0.0278,  0.0584],\n",
            "        [-0.0154, -0.0233, -0.0062,  ..., -0.0544,  0.0279,  0.0615]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Predicted Caption [0]: bride bride bride bride bride bridewormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormworm\n",
            "Time step 0:\n",
            "  Token: bride, Probability: 3.659578942460939e-05\n",
            "  Token: fleetwood, Probability: 3.64493862434756e-05\n",
            "  Token: ##holders, Probability: 3.637522240751423e-05\n",
            "  Token: ##nb, Probability: 3.630347055150196e-05\n",
            "  Token: 1718, Probability: 3.625045792432502e-05\n",
            "Time step 1:\n",
            "  Token: bride, Probability: 3.7922753108432516e-05\n",
            "  Token: ##holders, Probability: 3.77694123017136e-05\n",
            "  Token: ##nb, Probability: 3.754368663066998e-05\n",
            "  Token: fleetwood, Probability: 3.7482437619473785e-05\n",
            "  Token: settle, Probability: 3.7409623473649845e-05\n",
            "Time step 2:\n",
            "  Token: bride, Probability: 3.858287527691573e-05\n",
            "  Token: ##holders, Probability: 3.84720551664941e-05\n",
            "  Token: ##nb, Probability: 3.822190046776086e-05\n",
            "  Token: ##worm, Probability: 3.81689787900541e-05\n",
            "  Token: settle, Probability: 3.805080632446334e-05\n",
            "Time step 3:\n",
            "  Token: bride, Probability: 3.890793595928699e-05\n",
            "  Token: ##holders, Probability: 3.881633165292442e-05\n",
            "  Token: ##worm, Probability: 3.866475526592694e-05\n",
            "  Token: ##nb, Probability: 3.858986383420415e-05\n",
            "  Token: casino, Probability: 3.841115176328458e-05\n",
            "Time step 4:\n",
            "  Token: bride, Probability: 3.906552228727378e-05\n",
            "  Token: ##holders, Probability: 3.8981230318313465e-05\n",
            "  Token: ##worm, Probability: 3.894343535648659e-05\n",
            "  Token: ##nb, Probability: 3.8786864024586976e-05\n",
            "  Token: casino, Probability: 3.859731805277988e-05\n",
            "Time step 5:\n",
            "  Token: bride, Probability: 3.914020999218337e-05\n",
            "  Token: ##worm, Probability: 3.910106170224026e-05\n",
            "  Token: ##holders, Probability: 3.905863923137076e-05\n",
            "  Token: ##nb, Probability: 3.889035724569112e-05\n",
            "  Token: ##oshi, Probability: 3.87100808438845e-05\n",
            "Time step 6:\n",
            "  Token: ##worm, Probability: 3.91905996366404e-05\n",
            "  Token: bride, Probability: 3.917448702850379e-05\n",
            "  Token: ##holders, Probability: 3.909427687176503e-05\n",
            "  Token: ##nb, Probability: 3.894331166520715e-05\n",
            "  Token: ##oshi, Probability: 3.8786402001278475e-05\n",
            "Time step 7:\n",
            "  Token: ##worm, Probability: 3.9241618651431054e-05\n",
            "  Token: bride, Probability: 3.918952643289231e-05\n",
            "  Token: ##holders, Probability: 3.911036401404999e-05\n",
            "  Token: ##nb, Probability: 3.89694505429361e-05\n",
            "  Token: ##oshi, Probability: 3.88306871172972e-05\n",
            "Time step 8:\n",
            "  Token: ##worm, Probability: 3.9270777051569894e-05\n",
            "  Token: bride, Probability: 3.9195660065161064e-05\n",
            "  Token: ##holders, Probability: 3.91174471587874e-05\n",
            "  Token: ##nb, Probability: 3.898167415172793e-05\n",
            "  Token: ##oshi, Probability: 3.8856414903420955e-05\n",
            "Time step 9:\n",
            "  Token: ##worm, Probability: 3.928747901227325e-05\n",
            "  Token: bride, Probability: 3.919784285244532e-05\n",
            "  Token: ##holders, Probability: 3.912046304321848e-05\n",
            "  Token: ##nb, Probability: 3.8986894651316106e-05\n",
            "  Token: ##oshi, Probability: 3.887138518621214e-05\n",
            "Time step 10:\n",
            "  Token: ##worm, Probability: 3.929708691430278e-05\n",
            "  Token: bride, Probability: 3.919838127330877e-05\n",
            "  Token: ##holders, Probability: 3.912167812814005e-05\n",
            "  Token: ##nb, Probability: 3.8988760934444144e-05\n",
            "  Token: ##oshi, Probability: 3.888012361130677e-05\n",
            "Time step 11:\n",
            "  Token: ##worm, Probability: 3.9302634831983596e-05\n",
            "  Token: bride, Probability: 3.919831578969024e-05\n",
            "  Token: ##holders, Probability: 3.912212196155451e-05\n",
            "  Token: ##nb, Probability: 3.8989110180409625e-05\n",
            "  Token: ##oshi, Probability: 3.888524224748835e-05\n",
            "Time step 12:\n",
            "  Token: ##worm, Probability: 3.9305850805249065e-05\n",
            "  Token: bride, Probability: 3.919805749319494e-05\n",
            "  Token: ##holders, Probability: 3.9122242014855146e-05\n",
            "  Token: ##nb, Probability: 3.898885552189313e-05\n",
            "  Token: ##oshi, Probability: 3.888824721798301e-05\n",
            "Time step 13:\n",
            "  Token: ##worm, Probability: 3.9307728002313524e-05\n",
            "  Token: bride, Probability: 3.9197784644784406e-05\n",
            "  Token: ##holders, Probability: 3.912224929081276e-05\n",
            "  Token: ##nb, Probability: 3.898842624039389e-05\n",
            "  Token: ##oshi, Probability: 3.889002255164087e-05\n",
            "Time step 14:\n",
            "  Token: ##worm, Probability: 3.9308826671913266e-05\n",
            "  Token: bride, Probability: 3.919756272807717e-05\n",
            "  Token: ##holders, Probability: 3.9122220186982304e-05\n",
            "  Token: ##nb, Probability: 3.8988007872831076e-05\n",
            "  Token: ##oshi, Probability: 3.889108120347373e-05\n",
            "Time step 15:\n",
            "  Token: ##worm, Probability: 3.930948150809854e-05\n",
            "  Token: bride, Probability: 3.919739538105205e-05\n",
            "  Token: ##holders, Probability: 3.912219108315185e-05\n",
            "  Token: ##nb, Probability: 3.898765135090798e-05\n",
            "  Token: ##oshi, Probability: 3.8891703297849745e-05\n",
            "Time step 16:\n",
            "  Token: ##worm, Probability: 3.930985985789448e-05\n",
            "  Token: bride, Probability: 3.91972680517938e-05\n",
            "  Token: ##holders, Probability: 3.912215834134258e-05\n",
            "  Token: ##nb, Probability: 3.898737486451864e-05\n",
            "  Token: ##oshi, Probability: 3.889208528562449e-05\n",
            "Time step 17:\n",
            "  Token: ##worm, Probability: 3.931009996449575e-05\n",
            "  Token: bride, Probability: 3.919718801626004e-05\n",
            "  Token: ##holders, Probability: 3.912215106538497e-05\n",
            "  Token: ##nb, Probability: 3.898717841366306e-05\n",
            "  Token: ##oshi, Probability: 3.889230720233172e-05\n",
            "Time step 18:\n",
            "  Token: ##worm, Probability: 3.9310241845669225e-05\n",
            "  Token: bride, Probability: 3.9197122532641515e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898703653248958e-05\n",
            "  Token: ##oshi, Probability: 3.8892441807547584e-05\n",
            "Time step 19:\n",
            "  Token: ##worm, Probability: 3.931032188120298e-05\n",
            "  Token: bride, Probability: 3.919708251487464e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898693830706179e-05\n",
            "  Token: ##oshi, Probability: 3.889251820510253e-05\n",
            "Time step 20:\n",
            "  Token: ##worm, Probability: 3.931036917492747e-05\n",
            "  Token: bride, Probability: 3.919704249710776e-05\n",
            "  Token: ##holders, Probability: 3.912212559953332e-05\n",
            "  Token: ##nb, Probability: 3.898686554748565e-05\n",
            "  Token: ##oshi, Probability: 3.889255822286941e-05\n",
            "Time step 21:\n",
            "  Token: ##worm, Probability: 3.931039827875793e-05\n",
            "  Token: bride, Probability: 3.9197024307213724e-05\n",
            "  Token: ##holders, Probability: 3.9122132875490934e-05\n",
            "  Token: ##nb, Probability: 3.8986821891739964e-05\n",
            "  Token: ##oshi, Probability: 3.889258732669987e-05\n",
            "Time step 22:\n",
            "  Token: ##worm, Probability: 3.931042010663077e-05\n",
            "  Token: bride, Probability: 3.9197020669234917e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898679278790951e-05\n",
            "  Token: ##oshi, Probability: 3.889260915457271e-05\n",
            "Time step 23:\n",
            "  Token: ##worm, Probability: 3.9310431020567194e-05\n",
            "  Token: bride, Probability: 3.919700611731969e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898677459801547e-05\n",
            "  Token: ##oshi, Probability: 3.889261279255152e-05\n",
            "Time step 24:\n",
            "  Token: ##worm, Probability: 3.9310441934503615e-05\n",
            "  Token: bride, Probability: 3.919700611731969e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898676368407905e-05\n",
            "  Token: ##oshi, Probability: 3.889262006850913e-05\n",
            "Time step 25:\n",
            "  Token: ##worm, Probability: 3.931043829652481e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912214015144855e-05\n",
            "  Token: ##nb, Probability: 3.898674913216382e-05\n",
            "  Token: ##oshi, Probability: 3.889261279255152e-05\n",
            "Time step 26:\n",
            "  Token: ##worm, Probability: 3.931044921046123e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.898675277014263e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 27:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 28:\n",
            "  Token: ##worm, Probability: 3.931044921046123e-05\n",
            "  Token: bride, Probability: 3.919699520338327e-05\n",
            "  Token: ##holders, Probability: 3.912215106538497e-05\n",
            "  Token: ##nb, Probability: 3.8986745494185016e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 29:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.8986734580248594e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 30:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912215834134258e-05\n",
            "  Token: ##nb, Probability: 3.89867382182274e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 31:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898673094226979e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 32:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699520338327e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.89867382182274e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 33:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919698792742565e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.898673094226979e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 34:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 35:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919698792742565e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.8986734580248594e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 36:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Predicted Caption [1]: services debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts\n",
            "Time step 0:\n",
            "  Token: services, Probability: 3.6087443731958047e-05\n",
            "  Token: beneath, Probability: 3.604967423598282e-05\n",
            "  Token: development, Probability: 3.604548328439705e-05\n",
            "  Token: debts, Probability: 3.602676224545576e-05\n",
            "  Token: timed, Probability: 3.602257129386999e-05\n",
            "Time step 1:\n",
            "  Token: debts, Probability: 3.71353053196799e-05\n",
            "  Token: development, Probability: 3.7123616493772715e-05\n",
            "  Token: gabe, Probability: 3.709600423462689e-05\n",
            "  Token: services, Probability: 3.703981565195136e-05\n",
            "  Token: endelle, Probability: 3.701909736264497e-05\n",
            "Time step 2:\n",
            "  Token: debts, Probability: 3.776024823309854e-05\n",
            "  Token: development, Probability: 3.7676629290217534e-05\n",
            "  Token: gabe, Probability: 3.764452048926614e-05\n",
            "  Token: endelle, Probability: 3.760301478905603e-05\n",
            "  Token: interface, Probability: 3.7593006709357724e-05\n",
            "Time step 3:\n",
            "  Token: debts, Probability: 3.810419366345741e-05\n",
            "  Token: development, Probability: 3.795653537963517e-05\n",
            "  Token: interface, Probability: 3.795541852014139e-05\n",
            "  Token: gabe, Probability: 3.7899837479926646e-05\n",
            "  Token: endelle, Probability: 3.789846959989518e-05\n",
            "Time step 4:\n",
            "  Token: debts, Probability: 3.82895850634668e-05\n",
            "  Token: interface, Probability: 3.8157762901391834e-05\n",
            "  Token: development, Probability: 3.8095142372185364e-05\n",
            "  Token: dancers, Probability: 3.805738379014656e-05\n",
            "  Token: endelle, Probability: 3.804350853897631e-05\n",
            "Time step 5:\n",
            "  Token: debts, Probability: 3.8387639506254345e-05\n",
            "  Token: interface, Probability: 3.827314139925875e-05\n",
            "  Token: dancers, Probability: 3.81657482648734e-05\n",
            "  Token: development, Probability: 3.816156095126644e-05\n",
            "  Token: endelle, Probability: 3.811221904470585e-05\n",
            "Time step 6:\n",
            "  Token: debts, Probability: 3.843867307296023e-05\n",
            "  Token: interface, Probability: 3.8340556784532964e-05\n",
            "  Token: dancers, Probability: 3.8227510231081396e-05\n",
            "  Token: development, Probability: 3.819185076281428e-05\n",
            "  Token: modernist, Probability: 3.8178575778147206e-05\n",
            "Time step 7:\n",
            "  Token: debts, Probability: 3.846495383186266e-05\n",
            "  Token: interface, Probability: 3.838086922769435e-05\n",
            "  Token: dancers, Probability: 3.82637299480848e-05\n",
            "  Token: modernist, Probability: 3.823023507720791e-05\n",
            "  Token: development, Probability: 3.820458732661791e-05\n",
            "Time step 8:\n",
            "  Token: debts, Probability: 3.8478454371215776e-05\n",
            "  Token: interface, Probability: 3.8405440136557445e-05\n",
            "  Token: dancers, Probability: 3.828567059827037e-05\n",
            "  Token: modernist, Probability: 3.826169267995283e-05\n",
            "  Token: development, Probability: 3.8209149352042004e-05\n",
            "Time step 9:\n",
            "  Token: debts, Probability: 3.848545748041943e-05\n",
            "  Token: interface, Probability: 3.842062869807705e-05\n",
            "  Token: dancers, Probability: 3.82993821403943e-05\n",
            "  Token: modernist, Probability: 3.828070475719869e-05\n",
            "  Token: development, Probability: 3.821014615823515e-05\n",
            "Time step 10:\n",
            "  Token: debts, Probability: 3.84891900466755e-05\n",
            "  Token: interface, Probability: 3.843008744297549e-05\n",
            "  Token: dancers, Probability: 3.830816422123462e-05\n",
            "  Token: modernist, Probability: 3.8292084354907274e-05\n",
            "  Token: development, Probability: 3.820977144641802e-05\n",
            "Time step 11:\n",
            "  Token: debts, Probability: 3.849127460853197e-05\n",
            "  Token: interface, Probability: 3.843600643449463e-05\n",
            "  Token: dancers, Probability: 3.83139013138134e-05\n",
            "  Token: modernist, Probability: 3.829881097772159e-05\n",
            "  Token: development, Probability: 3.820898928097449e-05\n",
            "Time step 12:\n",
            "  Token: debts, Probability: 3.849249696941115e-05\n",
            "  Token: interface, Probability: 3.8439713534899056e-05\n",
            "  Token: dancers, Probability: 3.8317688449751586e-05\n",
            "  Token: modernist, Probability: 3.830273635685444e-05\n",
            "  Token: development, Probability: 3.820821439148858e-05\n",
            "Time step 13:\n",
            "  Token: debts, Probability: 3.849326822091825e-05\n",
            "  Token: interface, Probability: 3.8442027289420366e-05\n",
            "  Token: dancers, Probability: 3.8320216845022514e-05\n",
            "  Token: modernist, Probability: 3.830498826573603e-05\n",
            "  Token: rankings, Probability: 3.8207810575840995e-05\n",
            "Time step 14:\n",
            "  Token: debts, Probability: 3.849377026199363e-05\n",
            "  Token: interface, Probability: 3.84434824809432e-05\n",
            "  Token: dancers, Probability: 3.8321893953252584e-05\n",
            "  Token: modernist, Probability: 3.830625064438209e-05\n",
            "  Token: rankings, Probability: 3.8208949263207614e-05\n",
            "Time step 15:\n",
            "  Token: debts, Probability: 3.849410859402269e-05\n",
            "  Token: interface, Probability: 3.8444381061708555e-05\n",
            "  Token: dancers, Probability: 3.832302172668278e-05\n",
            "  Token: modernist, Probability: 3.830695277429186e-05\n",
            "  Token: rankings, Probability: 3.820962956524454e-05\n",
            "Time step 16:\n",
            "  Token: debts, Probability: 3.849433778668754e-05\n",
            "  Token: interface, Probability: 3.844493767246604e-05\n",
            "  Token: dancers, Probability: 3.832376387435943e-05\n",
            "  Token: modernist, Probability: 3.830732384813018e-05\n",
            "  Token: rankings, Probability: 3.821001519099809e-05\n",
            "Time step 17:\n",
            "  Token: debts, Probability: 3.849450149573386e-05\n",
            "  Token: interface, Probability: 3.8445279642473906e-05\n",
            "  Token: dancers, Probability: 3.8324262277456e-05\n",
            "  Token: modernist, Probability: 3.8307520298985764e-05\n",
            "  Token: rankings, Probability: 3.821024438366294e-05\n",
            "Time step 18:\n",
            "  Token: debts, Probability: 3.8494603359140456e-05\n",
            "  Token: interface, Probability: 3.844549064524472e-05\n",
            "  Token: dancers, Probability: 3.832458969554864e-05\n",
            "  Token: modernist, Probability: 3.830761124845594e-05\n",
            "  Token: rankings, Probability: 3.8210375350899994e-05\n",
            "Time step 19:\n",
            "  Token: debts, Probability: 3.849468339467421e-05\n",
            "  Token: interface, Probability: 3.844562888843939e-05\n",
            "  Token: dancers, Probability: 3.8324818888213485e-05\n",
            "  Token: modernist, Probability: 3.830766581813805e-05\n",
            "  Token: rankings, Probability: 3.821045902441256e-05\n",
            "Time step 20:\n",
            "  Token: debts, Probability: 3.84947270504199e-05\n",
            "  Token: interface, Probability: 3.844571256195195e-05\n",
            "  Token: dancers, Probability: 3.832496076938696e-05\n",
            "  Token: modernist, Probability: 3.830767673207447e-05\n",
            "  Token: rankings, Probability: 3.821050268015824e-05\n",
            "Time step 21:\n",
            "  Token: debts, Probability: 3.8494756154250354e-05\n",
            "  Token: interface, Probability: 3.844575257971883e-05\n",
            "  Token: dancers, Probability: 3.832504080492072e-05\n",
            "  Token: modernist, Probability: 3.830767309409566e-05\n",
            "  Token: rankings, Probability: 3.821051723207347e-05\n",
            "Time step 22:\n",
            "  Token: debts, Probability: 3.8494777982123196e-05\n",
            "  Token: interface, Probability: 3.844578532152809e-05\n",
            "  Token: dancers, Probability: 3.832510992651805e-05\n",
            "  Token: modernist, Probability: 3.830767673207447e-05\n",
            "  Token: rankings, Probability: 3.82105317839887e-05\n",
            "Time step 23:\n",
            "  Token: debts, Probability: 3.849478889605962e-05\n",
            "  Token: interface, Probability: 3.844579987344332e-05\n",
            "  Token: dancers, Probability: 3.8325146306306124e-05\n",
            "  Token: modernist, Probability: 3.830766581813805e-05\n",
            "  Token: rankings, Probability: 3.8210539059946314e-05\n",
            "Time step 24:\n",
            "  Token: debts, Probability: 3.849479617201723e-05\n",
            "  Token: interface, Probability: 3.8445818063337356e-05\n",
            "  Token: dancers, Probability: 3.832517177215777e-05\n",
            "  Token: modernist, Probability: 3.830766581813805e-05\n",
            "  Token: rankings, Probability: 3.821054269792512e-05\n",
            "Time step 25:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582170131616e-05\n",
            "  Token: dancers, Probability: 3.8325186324073e-05\n",
            "  Token: modernist, Probability: 3.830766581813805e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 26:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582533929497e-05\n",
            "  Token: dancers, Probability: 3.832520087598823e-05\n",
            "  Token: modernist, Probability: 3.830766218015924e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 27:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582533929497e-05\n",
            "  Token: dancers, Probability: 3.8325208151945844e-05\n",
            "  Token: modernist, Probability: 3.8307658542180434e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 28:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844583625323139e-05\n",
            "  Token: dancers, Probability: 3.832521542790346e-05\n",
            "  Token: modernist, Probability: 3.8307658542180434e-05\n",
            "  Token: rankings, Probability: 3.8210549973882735e-05\n",
            "Time step 29:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.832521542790346e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 30:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.832522270386107e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.8210549973882735e-05\n",
            "Time step 31:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.8325219065882266e-05\n",
            "  Token: modernist, Probability: 3.830765126622282e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 32:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.832522270386107e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.821055361186154e-05\n",
            "Time step 33:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.8325219065882266e-05\n",
            "  Token: modernist, Probability: 3.830764762824401e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 34:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844583625323139e-05\n",
            "  Token: dancers, Probability: 3.832522634183988e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.821055361186154e-05\n",
            "Time step 35:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.8445832615252584e-05\n",
            "  Token: dancers, Probability: 3.832522270386107e-05\n",
            "  Token: modernist, Probability: 3.830765126622282e-05\n",
            "  Token: rankings, Probability: 3.8210549973882735e-05\n",
            "Time step 36:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844583625323139e-05\n",
            "  Token: dancers, Probability: 3.832522634183988e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.821055361186154e-05\n",
            "Predicted Caption [2]: ##teinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteintein\n",
            "Time step 0:\n",
            "  Token: ##tein, Probability: 3.6606343201128766e-05\n",
            "  Token: [unused101], Probability: 3.640828072093427e-05\n",
            "  Token: fleming, Probability: 3.639463466242887e-05\n",
            "  Token: exclusive, Probability: 3.618998744059354e-05\n",
            "  Token: sciences, Probability: 3.616496178437956e-05\n",
            "Time step 1:\n",
            "  Token: ##tein, Probability: 3.801730417762883e-05\n",
            "  Token: fleming, Probability: 3.744679634110071e-05\n",
            "  Token: missionary, Probability: 3.742317858268507e-05\n",
            "  Token: [unused101], Probability: 3.741743057616986e-05\n",
            "  Token: wink, Probability: 3.735232166945934e-05\n",
            "Time step 2:\n",
            "  Token: ##tein, Probability: 3.881015072693117e-05\n",
            "  Token: missionary, Probability: 3.8162201235536486e-05\n",
            "  Token: expressed, Probability: 3.801172351813875e-05\n",
            "  Token: wink, Probability: 3.79417251679115e-05\n",
            "  Token: fleming, Probability: 3.789502807194367e-05\n",
            "Time step 3:\n",
            "  Token: ##tein, Probability: 3.925691999029368e-05\n",
            "  Token: missionary, Probability: 3.855241811834276e-05\n",
            "  Token: expressed, Probability: 3.837727490463294e-05\n",
            "  Token: warren, Probability: 3.828478656942025e-05\n",
            "  Token: wink, Probability: 3.8203608710318804e-05\n",
            "Time step 4:\n",
            "  Token: ##tein, Probability: 3.951004327973351e-05\n",
            "  Token: missionary, Probability: 3.8758080336265266e-05\n",
            "  Token: expressed, Probability: 3.85660350730177e-05\n",
            "  Token: warren, Probability: 3.850590292131528e-05\n",
            "  Token: wink, Probability: 3.831134017673321e-05\n",
            "Time step 5:\n",
            "  Token: ##tein, Probability: 3.965447467635386e-05\n",
            "  Token: missionary, Probability: 3.886625563609414e-05\n",
            "  Token: expressed, Probability: 3.8665013562422246e-05\n",
            "  Token: warren, Probability: 3.8630678318440914e-05\n",
            "  Token: wink, Probability: 3.8350688555510715e-05\n",
            "Time step 6:\n",
            "  Token: ##tein, Probability: 3.973760613007471e-05\n",
            "  Token: missionary, Probability: 3.892293534590863e-05\n",
            "  Token: expressed, Probability: 3.8717778807040304e-05\n",
            "  Token: warren, Probability: 3.870076761813834e-05\n",
            "  Token: wink, Probability: 3.8361871702363715e-05\n",
            "Time step 7:\n",
            "  Token: ##tein, Probability: 3.978594395448454e-05\n",
            "  Token: missionary, Probability: 3.8952410250203684e-05\n",
            "  Token: expressed, Probability: 3.8746358768548816e-05\n",
            "  Token: warren, Probability: 3.873978857882321e-05\n",
            "  Token: wink, Probability: 3.8362733903340995e-05\n",
            "Time step 8:\n",
            "  Token: ##tein, Probability: 3.9814378396840766e-05\n",
            "  Token: missionary, Probability: 3.8967533328104764e-05\n",
            "  Token: expressed, Probability: 3.876203118124977e-05\n",
            "  Token: warren, Probability: 3.876124901580624e-05\n",
            "  Token: wink, Probability: 3.836063842754811e-05\n",
            "Time step 9:\n",
            "  Token: ##tein, Probability: 3.983132046414539e-05\n",
            "  Token: missionary, Probability: 3.897511851391755e-05\n",
            "  Token: warren, Probability: 3.8772890547988936e-05\n",
            "  Token: expressed, Probability: 3.8770704122725874e-05\n",
            "  Token: sciences, Probability: 3.8362821214832366e-05\n",
            "Time step 10:\n",
            "  Token: ##tein, Probability: 3.984157228842378e-05\n",
            "  Token: missionary, Probability: 3.8978789234533906e-05\n",
            "  Token: warren, Probability: 3.877911512972787e-05\n",
            "  Token: expressed, Probability: 3.8775520806666464e-05\n",
            "  Token: sciences, Probability: 3.8369766116375104e-05\n",
            "Time step 11:\n",
            "  Token: ##tein, Probability: 3.984783688792959e-05\n",
            "  Token: missionary, Probability: 3.8980455428827554e-05\n",
            "  Token: warren, Probability: 3.878237112076022e-05\n",
            "  Token: expressed, Probability: 3.8778169255238026e-05\n",
            "  Token: sciences, Probability: 3.837396070593968e-05\n",
            "Time step 12:\n",
            "  Token: ##tein, Probability: 3.985172588727437e-05\n",
            "  Token: missionary, Probability: 3.8981117540970445e-05\n",
            "  Token: warren, Probability: 3.8784048228990287e-05\n",
            "  Token: expressed, Probability: 3.877962444676086e-05\n",
            "  Token: sciences, Probability: 3.8376481825252995e-05\n",
            "Time step 13:\n",
            "  Token: ##tein, Probability: 3.9854170609032735e-05\n",
            "  Token: missionary, Probability: 3.8981306715868413e-05\n",
            "  Token: warren, Probability: 3.878489587805234e-05\n",
            "  Token: expressed, Probability: 3.8780399336246774e-05\n",
            "  Token: sciences, Probability: 3.837800250039436e-05\n",
            "Time step 14:\n",
            "  Token: ##tein, Probability: 3.9855724025983363e-05\n",
            "  Token: missionary, Probability: 3.898127761203796e-05\n",
            "  Token: warren, Probability: 3.8785314245615155e-05\n",
            "  Token: expressed, Probability: 3.878080315189436e-05\n",
            "  Token: sciences, Probability: 3.8378908357117325e-05\n",
            "Time step 15:\n",
            "  Token: ##tein, Probability: 3.98567171941977e-05\n",
            "  Token: missionary, Probability: 3.8981168472673744e-05\n",
            "  Token: warren, Probability: 3.878550705849193e-05\n",
            "  Token: expressed, Probability: 3.8780999602749944e-05\n",
            "  Token: sciences, Probability: 3.837944314000197e-05\n",
            "Time step 16:\n",
            "  Token: ##tein, Probability: 3.985736475442536e-05\n",
            "  Token: missionary, Probability: 3.898105205735192e-05\n",
            "  Token: warren, Probability: 3.878559800796211e-05\n",
            "  Token: expressed, Probability: 3.878109419019893e-05\n",
            "  Token: sciences, Probability: 3.8379770558094606e-05\n",
            "Time step 17:\n",
            "  Token: ##tein, Probability: 3.985778312198818e-05\n",
            "  Token: missionary, Probability: 3.8980942917987704e-05\n",
            "  Token: warren, Probability: 3.87856453016866e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.838000702671707e-05\n",
            "Time step 18:\n",
            "  Token: ##tein, Probability: 3.985806324635632e-05\n",
            "  Token: missionary, Probability: 3.898084469255991e-05\n",
            "  Token: warren, Probability: 3.878565985360183e-05\n",
            "  Token: expressed, Probability: 3.878115603583865e-05\n",
            "  Token: [unused101], Probability: 3.8380170735763386e-05\n",
            "Time step 19:\n",
            "  Token: ##tein, Probability: 3.985823423136026e-05\n",
            "  Token: missionary, Probability: 3.8980768295004964e-05\n",
            "  Token: warren, Probability: 3.8785663491580635e-05\n",
            "  Token: expressed, Probability: 3.8781148759881034e-05\n",
            "  Token: [unused101], Probability: 3.8380258047254756e-05\n",
            "Time step 20:\n",
            "  Token: ##tein, Probability: 3.98583579226397e-05\n",
            "  Token: missionary, Probability: 3.8980728277238086e-05\n",
            "  Token: warren, Probability: 3.8785674405517057e-05\n",
            "  Token: expressed, Probability: 3.878115603583865e-05\n",
            "  Token: [unused101], Probability: 3.8380323530873284e-05\n",
            "Time step 21:\n",
            "  Token: ##tein, Probability: 3.985843068221584e-05\n",
            "  Token: missionary, Probability: 3.8980680983513594e-05\n",
            "  Token: warren, Probability: 3.878567076753825e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.8380345358746126e-05\n",
            "Time step 22:\n",
            "  Token: ##tein, Probability: 3.9858485251897946e-05\n",
            "  Token: missionary, Probability: 3.898065915564075e-05\n",
            "  Token: warren, Probability: 3.8785674405517057e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.838036718661897e-05\n",
            "Time step 23:\n",
            "  Token: ##tein, Probability: 3.985852163168602e-05\n",
            "  Token: missionary, Probability: 3.8980640965746716e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.8781148759881034e-05\n",
            "  Token: [unused101], Probability: 3.838037446257658e-05\n",
            "Time step 24:\n",
            "  Token: ##tein, Probability: 3.9858547097537667e-05\n",
            "  Token: missionary, Probability: 3.898062641383149e-05\n",
            "  Token: warren, Probability: 3.8785678043495864e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.838037810055539e-05\n",
            "Time step 25:\n",
            "  Token: ##tein, Probability: 3.985855801147409e-05\n",
            "  Token: missionary, Probability: 3.8980615499895066e-05\n",
            "  Token: warren, Probability: 3.8785678043495864e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 26:\n",
            "  Token: ##tein, Probability: 3.985856892541051e-05\n",
            "  Token: missionary, Probability: 3.898060822393745e-05\n",
            "  Token: warren, Probability: 3.8785674405517057e-05\n",
            "  Token: expressed, Probability: 3.878113784594461e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 27:\n",
            "  Token: ##tein, Probability: 3.985857620136812e-05\n",
            "  Token: missionary, Probability: 3.898060822393745e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.8380385376513004e-05\n",
            "Time step 28:\n",
            "  Token: ##tein, Probability: 3.9858572563389316e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.8785674405517057e-05\n",
            "  Token: expressed, Probability: 3.8781134207965806e-05\n",
            "  Token: [unused101], Probability: 3.838037446257658e-05\n",
            "Time step 29:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.8980604585958645e-05\n",
            "  Token: warren, Probability: 3.878568531945348e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.8380385376513004e-05\n",
            "Time step 30:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898060094797984e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 31:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 32:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 33:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 34:\n",
            "  Token: ##tein, Probability: 3.985859075328335e-05\n",
            "  Token: missionary, Probability: 3.898060094797984e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 35:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.838037810055539e-05\n",
            "Time step 36:\n",
            "  Token: ##tein, Probability: 3.985859075328335e-05\n",
            "  Token: missionary, Probability: 3.898060094797984e-05\n",
            "  Token: warren, Probability: 3.8785678043495864e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Decoded Caption [1]: this man looks young and lively, he has very kind eyes\n",
            "Sample outputs: tensor([[-0.0118, -0.0081,  0.0201,  ..., -0.0408,  0.0143,  0.0431],\n",
            "        [-0.0157, -0.0216,  0.0095,  ..., -0.0490,  0.0231,  0.0488],\n",
            "        [-0.0161, -0.0247,  0.0020,  ..., -0.0526,  0.0267,  0.0541],\n",
            "        [-0.0158, -0.0244, -0.0030,  ..., -0.0540,  0.0278,  0.0584],\n",
            "        [-0.0154, -0.0233, -0.0062,  ..., -0.0544,  0.0279,  0.0615]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Predicted Caption [0]: bride bride bride bride bride bridewormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormworm\n",
            "Time step 0:\n",
            "  Token: bride, Probability: 3.659578942460939e-05\n",
            "  Token: fleetwood, Probability: 3.64493862434756e-05\n",
            "  Token: ##holders, Probability: 3.637522240751423e-05\n",
            "  Token: ##nb, Probability: 3.630347055150196e-05\n",
            "  Token: 1718, Probability: 3.625045792432502e-05\n",
            "Time step 1:\n",
            "  Token: bride, Probability: 3.7922753108432516e-05\n",
            "  Token: ##holders, Probability: 3.77694123017136e-05\n",
            "  Token: ##nb, Probability: 3.754368663066998e-05\n",
            "  Token: fleetwood, Probability: 3.7482437619473785e-05\n",
            "  Token: settle, Probability: 3.7409623473649845e-05\n",
            "Time step 2:\n",
            "  Token: bride, Probability: 3.858287527691573e-05\n",
            "  Token: ##holders, Probability: 3.84720551664941e-05\n",
            "  Token: ##nb, Probability: 3.822190046776086e-05\n",
            "  Token: ##worm, Probability: 3.81689787900541e-05\n",
            "  Token: settle, Probability: 3.805080632446334e-05\n",
            "Time step 3:\n",
            "  Token: bride, Probability: 3.890793595928699e-05\n",
            "  Token: ##holders, Probability: 3.881633165292442e-05\n",
            "  Token: ##worm, Probability: 3.866475526592694e-05\n",
            "  Token: ##nb, Probability: 3.858986383420415e-05\n",
            "  Token: casino, Probability: 3.841115176328458e-05\n",
            "Time step 4:\n",
            "  Token: bride, Probability: 3.906552228727378e-05\n",
            "  Token: ##holders, Probability: 3.8981230318313465e-05\n",
            "  Token: ##worm, Probability: 3.894343535648659e-05\n",
            "  Token: ##nb, Probability: 3.8786864024586976e-05\n",
            "  Token: casino, Probability: 3.859731805277988e-05\n",
            "Time step 5:\n",
            "  Token: bride, Probability: 3.914020999218337e-05\n",
            "  Token: ##worm, Probability: 3.910106170224026e-05\n",
            "  Token: ##holders, Probability: 3.905863923137076e-05\n",
            "  Token: ##nb, Probability: 3.889035724569112e-05\n",
            "  Token: ##oshi, Probability: 3.87100808438845e-05\n",
            "Time step 6:\n",
            "  Token: ##worm, Probability: 3.91905996366404e-05\n",
            "  Token: bride, Probability: 3.917448702850379e-05\n",
            "  Token: ##holders, Probability: 3.909427687176503e-05\n",
            "  Token: ##nb, Probability: 3.894331166520715e-05\n",
            "  Token: ##oshi, Probability: 3.8786402001278475e-05\n",
            "Time step 7:\n",
            "  Token: ##worm, Probability: 3.9241618651431054e-05\n",
            "  Token: bride, Probability: 3.918952643289231e-05\n",
            "  Token: ##holders, Probability: 3.911036401404999e-05\n",
            "  Token: ##nb, Probability: 3.89694505429361e-05\n",
            "  Token: ##oshi, Probability: 3.88306871172972e-05\n",
            "Time step 8:\n",
            "  Token: ##worm, Probability: 3.9270777051569894e-05\n",
            "  Token: bride, Probability: 3.9195660065161064e-05\n",
            "  Token: ##holders, Probability: 3.91174471587874e-05\n",
            "  Token: ##nb, Probability: 3.898167415172793e-05\n",
            "  Token: ##oshi, Probability: 3.8856414903420955e-05\n",
            "Time step 9:\n",
            "  Token: ##worm, Probability: 3.928747901227325e-05\n",
            "  Token: bride, Probability: 3.919784285244532e-05\n",
            "  Token: ##holders, Probability: 3.912046304321848e-05\n",
            "  Token: ##nb, Probability: 3.8986894651316106e-05\n",
            "  Token: ##oshi, Probability: 3.887138518621214e-05\n",
            "Time step 10:\n",
            "  Token: ##worm, Probability: 3.929708691430278e-05\n",
            "  Token: bride, Probability: 3.919838127330877e-05\n",
            "  Token: ##holders, Probability: 3.912167812814005e-05\n",
            "  Token: ##nb, Probability: 3.8988760934444144e-05\n",
            "  Token: ##oshi, Probability: 3.888012361130677e-05\n",
            "Time step 11:\n",
            "  Token: ##worm, Probability: 3.9302634831983596e-05\n",
            "  Token: bride, Probability: 3.919831578969024e-05\n",
            "  Token: ##holders, Probability: 3.912212196155451e-05\n",
            "  Token: ##nb, Probability: 3.8989110180409625e-05\n",
            "  Token: ##oshi, Probability: 3.888524224748835e-05\n",
            "Time step 12:\n",
            "  Token: ##worm, Probability: 3.9305850805249065e-05\n",
            "  Token: bride, Probability: 3.919805749319494e-05\n",
            "  Token: ##holders, Probability: 3.9122242014855146e-05\n",
            "  Token: ##nb, Probability: 3.898885552189313e-05\n",
            "  Token: ##oshi, Probability: 3.888824721798301e-05\n",
            "Time step 13:\n",
            "  Token: ##worm, Probability: 3.9307728002313524e-05\n",
            "  Token: bride, Probability: 3.9197784644784406e-05\n",
            "  Token: ##holders, Probability: 3.912224929081276e-05\n",
            "  Token: ##nb, Probability: 3.898842624039389e-05\n",
            "  Token: ##oshi, Probability: 3.889002255164087e-05\n",
            "Time step 14:\n",
            "  Token: ##worm, Probability: 3.9308826671913266e-05\n",
            "  Token: bride, Probability: 3.919756272807717e-05\n",
            "  Token: ##holders, Probability: 3.9122220186982304e-05\n",
            "  Token: ##nb, Probability: 3.8988007872831076e-05\n",
            "  Token: ##oshi, Probability: 3.889108120347373e-05\n",
            "Time step 15:\n",
            "  Token: ##worm, Probability: 3.930948150809854e-05\n",
            "  Token: bride, Probability: 3.919739538105205e-05\n",
            "  Token: ##holders, Probability: 3.912219108315185e-05\n",
            "  Token: ##nb, Probability: 3.898765135090798e-05\n",
            "  Token: ##oshi, Probability: 3.8891703297849745e-05\n",
            "Time step 16:\n",
            "  Token: ##worm, Probability: 3.930985985789448e-05\n",
            "  Token: bride, Probability: 3.91972680517938e-05\n",
            "  Token: ##holders, Probability: 3.912215834134258e-05\n",
            "  Token: ##nb, Probability: 3.898737486451864e-05\n",
            "  Token: ##oshi, Probability: 3.889208528562449e-05\n",
            "Time step 17:\n",
            "  Token: ##worm, Probability: 3.931009996449575e-05\n",
            "  Token: bride, Probability: 3.919718801626004e-05\n",
            "  Token: ##holders, Probability: 3.912215106538497e-05\n",
            "  Token: ##nb, Probability: 3.898717841366306e-05\n",
            "  Token: ##oshi, Probability: 3.889230720233172e-05\n",
            "Time step 18:\n",
            "  Token: ##worm, Probability: 3.9310241845669225e-05\n",
            "  Token: bride, Probability: 3.9197122532641515e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898703653248958e-05\n",
            "  Token: ##oshi, Probability: 3.8892441807547584e-05\n",
            "Time step 19:\n",
            "  Token: ##worm, Probability: 3.931032188120298e-05\n",
            "  Token: bride, Probability: 3.919708251487464e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898693830706179e-05\n",
            "  Token: ##oshi, Probability: 3.889251820510253e-05\n",
            "Time step 20:\n",
            "  Token: ##worm, Probability: 3.931036917492747e-05\n",
            "  Token: bride, Probability: 3.919704249710776e-05\n",
            "  Token: ##holders, Probability: 3.912212559953332e-05\n",
            "  Token: ##nb, Probability: 3.898686554748565e-05\n",
            "  Token: ##oshi, Probability: 3.889255822286941e-05\n",
            "Time step 21:\n",
            "  Token: ##worm, Probability: 3.931039827875793e-05\n",
            "  Token: bride, Probability: 3.9197024307213724e-05\n",
            "  Token: ##holders, Probability: 3.9122132875490934e-05\n",
            "  Token: ##nb, Probability: 3.8986821891739964e-05\n",
            "  Token: ##oshi, Probability: 3.889258732669987e-05\n",
            "Time step 22:\n",
            "  Token: ##worm, Probability: 3.931042010663077e-05\n",
            "  Token: bride, Probability: 3.9197020669234917e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898679278790951e-05\n",
            "  Token: ##oshi, Probability: 3.889260915457271e-05\n",
            "Time step 23:\n",
            "  Token: ##worm, Probability: 3.9310431020567194e-05\n",
            "  Token: bride, Probability: 3.919700611731969e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898677459801547e-05\n",
            "  Token: ##oshi, Probability: 3.889261279255152e-05\n",
            "Time step 24:\n",
            "  Token: ##worm, Probability: 3.9310441934503615e-05\n",
            "  Token: bride, Probability: 3.919700611731969e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898676368407905e-05\n",
            "  Token: ##oshi, Probability: 3.889262006850913e-05\n",
            "Time step 25:\n",
            "  Token: ##worm, Probability: 3.931043829652481e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912214015144855e-05\n",
            "  Token: ##nb, Probability: 3.898674913216382e-05\n",
            "  Token: ##oshi, Probability: 3.889261279255152e-05\n",
            "Time step 26:\n",
            "  Token: ##worm, Probability: 3.931044921046123e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.898675277014263e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 27:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 28:\n",
            "  Token: ##worm, Probability: 3.931044921046123e-05\n",
            "  Token: bride, Probability: 3.919699520338327e-05\n",
            "  Token: ##holders, Probability: 3.912215106538497e-05\n",
            "  Token: ##nb, Probability: 3.8986745494185016e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 29:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.8986734580248594e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 30:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912215834134258e-05\n",
            "  Token: ##nb, Probability: 3.89867382182274e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 31:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898673094226979e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 32:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699520338327e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.89867382182274e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 33:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919698792742565e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.898673094226979e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 34:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 35:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919698792742565e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.8986734580248594e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 36:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Predicted Caption [1]: services debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts\n",
            "Time step 0:\n",
            "  Token: services, Probability: 3.6087443731958047e-05\n",
            "  Token: beneath, Probability: 3.604967423598282e-05\n",
            "  Token: development, Probability: 3.604548328439705e-05\n",
            "  Token: debts, Probability: 3.602676224545576e-05\n",
            "  Token: timed, Probability: 3.602257129386999e-05\n",
            "Time step 1:\n",
            "  Token: debts, Probability: 3.71353053196799e-05\n",
            "  Token: development, Probability: 3.7123616493772715e-05\n",
            "  Token: gabe, Probability: 3.709600423462689e-05\n",
            "  Token: services, Probability: 3.703981565195136e-05\n",
            "  Token: endelle, Probability: 3.701909736264497e-05\n",
            "Time step 2:\n",
            "  Token: debts, Probability: 3.776024823309854e-05\n",
            "  Token: development, Probability: 3.7676629290217534e-05\n",
            "  Token: gabe, Probability: 3.764452048926614e-05\n",
            "  Token: endelle, Probability: 3.760301478905603e-05\n",
            "  Token: interface, Probability: 3.7593006709357724e-05\n",
            "Time step 3:\n",
            "  Token: debts, Probability: 3.810419366345741e-05\n",
            "  Token: development, Probability: 3.795653537963517e-05\n",
            "  Token: interface, Probability: 3.795541852014139e-05\n",
            "  Token: gabe, Probability: 3.7899837479926646e-05\n",
            "  Token: endelle, Probability: 3.789846959989518e-05\n",
            "Time step 4:\n",
            "  Token: debts, Probability: 3.82895850634668e-05\n",
            "  Token: interface, Probability: 3.8157762901391834e-05\n",
            "  Token: development, Probability: 3.8095142372185364e-05\n",
            "  Token: dancers, Probability: 3.805738379014656e-05\n",
            "  Token: endelle, Probability: 3.804350853897631e-05\n",
            "Time step 5:\n",
            "  Token: debts, Probability: 3.8387639506254345e-05\n",
            "  Token: interface, Probability: 3.827314139925875e-05\n",
            "  Token: dancers, Probability: 3.81657482648734e-05\n",
            "  Token: development, Probability: 3.816156095126644e-05\n",
            "  Token: endelle, Probability: 3.811221904470585e-05\n",
            "Time step 6:\n",
            "  Token: debts, Probability: 3.843867307296023e-05\n",
            "  Token: interface, Probability: 3.8340556784532964e-05\n",
            "  Token: dancers, Probability: 3.8227510231081396e-05\n",
            "  Token: development, Probability: 3.819185076281428e-05\n",
            "  Token: modernist, Probability: 3.8178575778147206e-05\n",
            "Time step 7:\n",
            "  Token: debts, Probability: 3.846495383186266e-05\n",
            "  Token: interface, Probability: 3.838086922769435e-05\n",
            "  Token: dancers, Probability: 3.82637299480848e-05\n",
            "  Token: modernist, Probability: 3.823023507720791e-05\n",
            "  Token: development, Probability: 3.820458732661791e-05\n",
            "Time step 8:\n",
            "  Token: debts, Probability: 3.8478454371215776e-05\n",
            "  Token: interface, Probability: 3.8405440136557445e-05\n",
            "  Token: dancers, Probability: 3.828567059827037e-05\n",
            "  Token: modernist, Probability: 3.826169267995283e-05\n",
            "  Token: development, Probability: 3.8209149352042004e-05\n",
            "Time step 9:\n",
            "  Token: debts, Probability: 3.848545748041943e-05\n",
            "  Token: interface, Probability: 3.842062869807705e-05\n",
            "  Token: dancers, Probability: 3.82993821403943e-05\n",
            "  Token: modernist, Probability: 3.828070475719869e-05\n",
            "  Token: development, Probability: 3.821014615823515e-05\n",
            "Time step 10:\n",
            "  Token: debts, Probability: 3.84891900466755e-05\n",
            "  Token: interface, Probability: 3.843008744297549e-05\n",
            "  Token: dancers, Probability: 3.830816422123462e-05\n",
            "  Token: modernist, Probability: 3.8292084354907274e-05\n",
            "  Token: development, Probability: 3.820977144641802e-05\n",
            "Time step 11:\n",
            "  Token: debts, Probability: 3.849127460853197e-05\n",
            "  Token: interface, Probability: 3.843600643449463e-05\n",
            "  Token: dancers, Probability: 3.83139013138134e-05\n",
            "  Token: modernist, Probability: 3.829881097772159e-05\n",
            "  Token: development, Probability: 3.820898928097449e-05\n",
            "Time step 12:\n",
            "  Token: debts, Probability: 3.849249696941115e-05\n",
            "  Token: interface, Probability: 3.8439713534899056e-05\n",
            "  Token: dancers, Probability: 3.8317688449751586e-05\n",
            "  Token: modernist, Probability: 3.830273635685444e-05\n",
            "  Token: development, Probability: 3.820821439148858e-05\n",
            "Time step 13:\n",
            "  Token: debts, Probability: 3.849326822091825e-05\n",
            "  Token: interface, Probability: 3.8442027289420366e-05\n",
            "  Token: dancers, Probability: 3.8320216845022514e-05\n",
            "  Token: modernist, Probability: 3.830498826573603e-05\n",
            "  Token: rankings, Probability: 3.8207810575840995e-05\n",
            "Time step 14:\n",
            "  Token: debts, Probability: 3.849377026199363e-05\n",
            "  Token: interface, Probability: 3.84434824809432e-05\n",
            "  Token: dancers, Probability: 3.8321893953252584e-05\n",
            "  Token: modernist, Probability: 3.830625064438209e-05\n",
            "  Token: rankings, Probability: 3.8208949263207614e-05\n",
            "Time step 15:\n",
            "  Token: debts, Probability: 3.849410859402269e-05\n",
            "  Token: interface, Probability: 3.8444381061708555e-05\n",
            "  Token: dancers, Probability: 3.832302172668278e-05\n",
            "  Token: modernist, Probability: 3.830695277429186e-05\n",
            "  Token: rankings, Probability: 3.820962956524454e-05\n",
            "Time step 16:\n",
            "  Token: debts, Probability: 3.849433778668754e-05\n",
            "  Token: interface, Probability: 3.844493767246604e-05\n",
            "  Token: dancers, Probability: 3.832376387435943e-05\n",
            "  Token: modernist, Probability: 3.830732384813018e-05\n",
            "  Token: rankings, Probability: 3.821001519099809e-05\n",
            "Time step 17:\n",
            "  Token: debts, Probability: 3.849450149573386e-05\n",
            "  Token: interface, Probability: 3.8445279642473906e-05\n",
            "  Token: dancers, Probability: 3.8324262277456e-05\n",
            "  Token: modernist, Probability: 3.8307520298985764e-05\n",
            "  Token: rankings, Probability: 3.821024438366294e-05\n",
            "Time step 18:\n",
            "  Token: debts, Probability: 3.8494603359140456e-05\n",
            "  Token: interface, Probability: 3.844549064524472e-05\n",
            "  Token: dancers, Probability: 3.832458969554864e-05\n",
            "  Token: modernist, Probability: 3.830761124845594e-05\n",
            "  Token: rankings, Probability: 3.8210375350899994e-05\n",
            "Time step 19:\n",
            "  Token: debts, Probability: 3.849468339467421e-05\n",
            "  Token: interface, Probability: 3.844562888843939e-05\n",
            "  Token: dancers, Probability: 3.8324818888213485e-05\n",
            "  Token: modernist, Probability: 3.830766581813805e-05\n",
            "  Token: rankings, Probability: 3.821045902441256e-05\n",
            "Time step 20:\n",
            "  Token: debts, Probability: 3.84947270504199e-05\n",
            "  Token: interface, Probability: 3.844571256195195e-05\n",
            "  Token: dancers, Probability: 3.832496076938696e-05\n",
            "  Token: modernist, Probability: 3.830767673207447e-05\n",
            "  Token: rankings, Probability: 3.821050268015824e-05\n",
            "Time step 21:\n",
            "  Token: debts, Probability: 3.8494756154250354e-05\n",
            "  Token: interface, Probability: 3.844575257971883e-05\n",
            "  Token: dancers, Probability: 3.832504080492072e-05\n",
            "  Token: modernist, Probability: 3.830767309409566e-05\n",
            "  Token: rankings, Probability: 3.821051723207347e-05\n",
            "Time step 22:\n",
            "  Token: debts, Probability: 3.8494777982123196e-05\n",
            "  Token: interface, Probability: 3.844578532152809e-05\n",
            "  Token: dancers, Probability: 3.832510992651805e-05\n",
            "  Token: modernist, Probability: 3.830767673207447e-05\n",
            "  Token: rankings, Probability: 3.82105317839887e-05\n",
            "Time step 23:\n",
            "  Token: debts, Probability: 3.849478889605962e-05\n",
            "  Token: interface, Probability: 3.844579987344332e-05\n",
            "  Token: dancers, Probability: 3.8325146306306124e-05\n",
            "  Token: modernist, Probability: 3.830766581813805e-05\n",
            "  Token: rankings, Probability: 3.8210539059946314e-05\n",
            "Time step 24:\n",
            "  Token: debts, Probability: 3.849479617201723e-05\n",
            "  Token: interface, Probability: 3.8445818063337356e-05\n",
            "  Token: dancers, Probability: 3.832517177215777e-05\n",
            "  Token: modernist, Probability: 3.830766581813805e-05\n",
            "  Token: rankings, Probability: 3.821054269792512e-05\n",
            "Time step 25:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582170131616e-05\n",
            "  Token: dancers, Probability: 3.8325186324073e-05\n",
            "  Token: modernist, Probability: 3.830766581813805e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 26:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582533929497e-05\n",
            "  Token: dancers, Probability: 3.832520087598823e-05\n",
            "  Token: modernist, Probability: 3.830766218015924e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 27:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582533929497e-05\n",
            "  Token: dancers, Probability: 3.8325208151945844e-05\n",
            "  Token: modernist, Probability: 3.8307658542180434e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 28:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844583625323139e-05\n",
            "  Token: dancers, Probability: 3.832521542790346e-05\n",
            "  Token: modernist, Probability: 3.8307658542180434e-05\n",
            "  Token: rankings, Probability: 3.8210549973882735e-05\n",
            "Time step 29:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.832521542790346e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 30:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.832522270386107e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.8210549973882735e-05\n",
            "Time step 31:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.8325219065882266e-05\n",
            "  Token: modernist, Probability: 3.830765126622282e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 32:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.832522270386107e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.821055361186154e-05\n",
            "Time step 33:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.844582897727378e-05\n",
            "  Token: dancers, Probability: 3.8325219065882266e-05\n",
            "  Token: modernist, Probability: 3.830764762824401e-05\n",
            "  Token: rankings, Probability: 3.821054633590393e-05\n",
            "Time step 34:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844583625323139e-05\n",
            "  Token: dancers, Probability: 3.832522634183988e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.821055361186154e-05\n",
            "Time step 35:\n",
            "  Token: debts, Probability: 3.8494803447974846e-05\n",
            "  Token: interface, Probability: 3.8445832615252584e-05\n",
            "  Token: dancers, Probability: 3.832522270386107e-05\n",
            "  Token: modernist, Probability: 3.830765126622282e-05\n",
            "  Token: rankings, Probability: 3.8210549973882735e-05\n",
            "Time step 36:\n",
            "  Token: debts, Probability: 3.849480708595365e-05\n",
            "  Token: interface, Probability: 3.844583625323139e-05\n",
            "  Token: dancers, Probability: 3.832522634183988e-05\n",
            "  Token: modernist, Probability: 3.830765490420163e-05\n",
            "  Token: rankings, Probability: 3.821055361186154e-05\n",
            "Predicted Caption [2]: ##teinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteinteintein\n",
            "Time step 0:\n",
            "  Token: ##tein, Probability: 3.6606343201128766e-05\n",
            "  Token: [unused101], Probability: 3.640828072093427e-05\n",
            "  Token: fleming, Probability: 3.639463466242887e-05\n",
            "  Token: exclusive, Probability: 3.618998744059354e-05\n",
            "  Token: sciences, Probability: 3.616496178437956e-05\n",
            "Time step 1:\n",
            "  Token: ##tein, Probability: 3.801730417762883e-05\n",
            "  Token: fleming, Probability: 3.744679634110071e-05\n",
            "  Token: missionary, Probability: 3.742317858268507e-05\n",
            "  Token: [unused101], Probability: 3.741743057616986e-05\n",
            "  Token: wink, Probability: 3.735232166945934e-05\n",
            "Time step 2:\n",
            "  Token: ##tein, Probability: 3.881015072693117e-05\n",
            "  Token: missionary, Probability: 3.8162201235536486e-05\n",
            "  Token: expressed, Probability: 3.801172351813875e-05\n",
            "  Token: wink, Probability: 3.79417251679115e-05\n",
            "  Token: fleming, Probability: 3.789502807194367e-05\n",
            "Time step 3:\n",
            "  Token: ##tein, Probability: 3.925691999029368e-05\n",
            "  Token: missionary, Probability: 3.855241811834276e-05\n",
            "  Token: expressed, Probability: 3.837727490463294e-05\n",
            "  Token: warren, Probability: 3.828478656942025e-05\n",
            "  Token: wink, Probability: 3.8203608710318804e-05\n",
            "Time step 4:\n",
            "  Token: ##tein, Probability: 3.951004327973351e-05\n",
            "  Token: missionary, Probability: 3.8758080336265266e-05\n",
            "  Token: expressed, Probability: 3.85660350730177e-05\n",
            "  Token: warren, Probability: 3.850590292131528e-05\n",
            "  Token: wink, Probability: 3.831134017673321e-05\n",
            "Time step 5:\n",
            "  Token: ##tein, Probability: 3.965447467635386e-05\n",
            "  Token: missionary, Probability: 3.886625563609414e-05\n",
            "  Token: expressed, Probability: 3.8665013562422246e-05\n",
            "  Token: warren, Probability: 3.8630678318440914e-05\n",
            "  Token: wink, Probability: 3.8350688555510715e-05\n",
            "Time step 6:\n",
            "  Token: ##tein, Probability: 3.973760613007471e-05\n",
            "  Token: missionary, Probability: 3.892293534590863e-05\n",
            "  Token: expressed, Probability: 3.8717778807040304e-05\n",
            "  Token: warren, Probability: 3.870076761813834e-05\n",
            "  Token: wink, Probability: 3.8361871702363715e-05\n",
            "Time step 7:\n",
            "  Token: ##tein, Probability: 3.978594395448454e-05\n",
            "  Token: missionary, Probability: 3.8952410250203684e-05\n",
            "  Token: expressed, Probability: 3.8746358768548816e-05\n",
            "  Token: warren, Probability: 3.873978857882321e-05\n",
            "  Token: wink, Probability: 3.8362733903340995e-05\n",
            "Time step 8:\n",
            "  Token: ##tein, Probability: 3.9814378396840766e-05\n",
            "  Token: missionary, Probability: 3.8967533328104764e-05\n",
            "  Token: expressed, Probability: 3.876203118124977e-05\n",
            "  Token: warren, Probability: 3.876124901580624e-05\n",
            "  Token: wink, Probability: 3.836063842754811e-05\n",
            "Time step 9:\n",
            "  Token: ##tein, Probability: 3.983132046414539e-05\n",
            "  Token: missionary, Probability: 3.897511851391755e-05\n",
            "  Token: warren, Probability: 3.8772890547988936e-05\n",
            "  Token: expressed, Probability: 3.8770704122725874e-05\n",
            "  Token: sciences, Probability: 3.8362821214832366e-05\n",
            "Time step 10:\n",
            "  Token: ##tein, Probability: 3.984157228842378e-05\n",
            "  Token: missionary, Probability: 3.8978789234533906e-05\n",
            "  Token: warren, Probability: 3.877911512972787e-05\n",
            "  Token: expressed, Probability: 3.8775520806666464e-05\n",
            "  Token: sciences, Probability: 3.8369766116375104e-05\n",
            "Time step 11:\n",
            "  Token: ##tein, Probability: 3.984783688792959e-05\n",
            "  Token: missionary, Probability: 3.8980455428827554e-05\n",
            "  Token: warren, Probability: 3.878237112076022e-05\n",
            "  Token: expressed, Probability: 3.8778169255238026e-05\n",
            "  Token: sciences, Probability: 3.837396070593968e-05\n",
            "Time step 12:\n",
            "  Token: ##tein, Probability: 3.985172588727437e-05\n",
            "  Token: missionary, Probability: 3.8981117540970445e-05\n",
            "  Token: warren, Probability: 3.8784048228990287e-05\n",
            "  Token: expressed, Probability: 3.877962444676086e-05\n",
            "  Token: sciences, Probability: 3.8376481825252995e-05\n",
            "Time step 13:\n",
            "  Token: ##tein, Probability: 3.9854170609032735e-05\n",
            "  Token: missionary, Probability: 3.8981306715868413e-05\n",
            "  Token: warren, Probability: 3.878489587805234e-05\n",
            "  Token: expressed, Probability: 3.8780399336246774e-05\n",
            "  Token: sciences, Probability: 3.837800250039436e-05\n",
            "Time step 14:\n",
            "  Token: ##tein, Probability: 3.9855724025983363e-05\n",
            "  Token: missionary, Probability: 3.898127761203796e-05\n",
            "  Token: warren, Probability: 3.8785314245615155e-05\n",
            "  Token: expressed, Probability: 3.878080315189436e-05\n",
            "  Token: sciences, Probability: 3.8378908357117325e-05\n",
            "Time step 15:\n",
            "  Token: ##tein, Probability: 3.98567171941977e-05\n",
            "  Token: missionary, Probability: 3.8981168472673744e-05\n",
            "  Token: warren, Probability: 3.878550705849193e-05\n",
            "  Token: expressed, Probability: 3.8780999602749944e-05\n",
            "  Token: sciences, Probability: 3.837944314000197e-05\n",
            "Time step 16:\n",
            "  Token: ##tein, Probability: 3.985736475442536e-05\n",
            "  Token: missionary, Probability: 3.898105205735192e-05\n",
            "  Token: warren, Probability: 3.878559800796211e-05\n",
            "  Token: expressed, Probability: 3.878109419019893e-05\n",
            "  Token: sciences, Probability: 3.8379770558094606e-05\n",
            "Time step 17:\n",
            "  Token: ##tein, Probability: 3.985778312198818e-05\n",
            "  Token: missionary, Probability: 3.8980942917987704e-05\n",
            "  Token: warren, Probability: 3.87856453016866e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.838000702671707e-05\n",
            "Time step 18:\n",
            "  Token: ##tein, Probability: 3.985806324635632e-05\n",
            "  Token: missionary, Probability: 3.898084469255991e-05\n",
            "  Token: warren, Probability: 3.878565985360183e-05\n",
            "  Token: expressed, Probability: 3.878115603583865e-05\n",
            "  Token: [unused101], Probability: 3.8380170735763386e-05\n",
            "Time step 19:\n",
            "  Token: ##tein, Probability: 3.985823423136026e-05\n",
            "  Token: missionary, Probability: 3.8980768295004964e-05\n",
            "  Token: warren, Probability: 3.8785663491580635e-05\n",
            "  Token: expressed, Probability: 3.8781148759881034e-05\n",
            "  Token: [unused101], Probability: 3.8380258047254756e-05\n",
            "Time step 20:\n",
            "  Token: ##tein, Probability: 3.98583579226397e-05\n",
            "  Token: missionary, Probability: 3.8980728277238086e-05\n",
            "  Token: warren, Probability: 3.8785674405517057e-05\n",
            "  Token: expressed, Probability: 3.878115603583865e-05\n",
            "  Token: [unused101], Probability: 3.8380323530873284e-05\n",
            "Time step 21:\n",
            "  Token: ##tein, Probability: 3.985843068221584e-05\n",
            "  Token: missionary, Probability: 3.8980680983513594e-05\n",
            "  Token: warren, Probability: 3.878567076753825e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.8380345358746126e-05\n",
            "Time step 22:\n",
            "  Token: ##tein, Probability: 3.9858485251897946e-05\n",
            "  Token: missionary, Probability: 3.898065915564075e-05\n",
            "  Token: warren, Probability: 3.8785674405517057e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.838036718661897e-05\n",
            "Time step 23:\n",
            "  Token: ##tein, Probability: 3.985852163168602e-05\n",
            "  Token: missionary, Probability: 3.8980640965746716e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.8781148759881034e-05\n",
            "  Token: [unused101], Probability: 3.838037446257658e-05\n",
            "Time step 24:\n",
            "  Token: ##tein, Probability: 3.9858547097537667e-05\n",
            "  Token: missionary, Probability: 3.898062641383149e-05\n",
            "  Token: warren, Probability: 3.8785678043495864e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.838037810055539e-05\n",
            "Time step 25:\n",
            "  Token: ##tein, Probability: 3.985855801147409e-05\n",
            "  Token: missionary, Probability: 3.8980615499895066e-05\n",
            "  Token: warren, Probability: 3.8785678043495864e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 26:\n",
            "  Token: ##tein, Probability: 3.985856892541051e-05\n",
            "  Token: missionary, Probability: 3.898060822393745e-05\n",
            "  Token: warren, Probability: 3.8785674405517057e-05\n",
            "  Token: expressed, Probability: 3.878113784594461e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 27:\n",
            "  Token: ##tein, Probability: 3.985857620136812e-05\n",
            "  Token: missionary, Probability: 3.898060822393745e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.8380385376513004e-05\n",
            "Time step 28:\n",
            "  Token: ##tein, Probability: 3.9858572563389316e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.8785674405517057e-05\n",
            "  Token: expressed, Probability: 3.8781134207965806e-05\n",
            "  Token: [unused101], Probability: 3.838037446257658e-05\n",
            "Time step 29:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.8980604585958645e-05\n",
            "  Token: warren, Probability: 3.878568531945348e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.8380385376513004e-05\n",
            "Time step 30:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898060094797984e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 31:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 32:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 33:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 34:\n",
            "  Token: ##tein, Probability: 3.985859075328335e-05\n",
            "  Token: missionary, Probability: 3.898060094797984e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Time step 35:\n",
            "  Token: ##tein, Probability: 3.9858587115304545e-05\n",
            "  Token: missionary, Probability: 3.898059731000103e-05\n",
            "  Token: warren, Probability: 3.878568168147467e-05\n",
            "  Token: expressed, Probability: 3.878114148392342e-05\n",
            "  Token: [unused101], Probability: 3.838037810055539e-05\n",
            "Time step 36:\n",
            "  Token: ##tein, Probability: 3.985859075328335e-05\n",
            "  Token: missionary, Probability: 3.898060094797984e-05\n",
            "  Token: warren, Probability: 3.8785678043495864e-05\n",
            "  Token: expressed, Probability: 3.878114512190223e-05\n",
            "  Token: [unused101], Probability: 3.83803817385342e-05\n",
            "Decoded Caption [2]: i have never met a drunk who would leave a orange half eaten next to their wine, that would just be messy\n",
            "Sample outputs: tensor([[-0.0118, -0.0081,  0.0201,  ..., -0.0408,  0.0143,  0.0431],\n",
            "        [-0.0157, -0.0216,  0.0095,  ..., -0.0490,  0.0231,  0.0488],\n",
            "        [-0.0161, -0.0247,  0.0020,  ..., -0.0526,  0.0267,  0.0541],\n",
            "        [-0.0158, -0.0244, -0.0030,  ..., -0.0540,  0.0278,  0.0584],\n",
            "        [-0.0154, -0.0233, -0.0062,  ..., -0.0544,  0.0279,  0.0615]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "Predicted Caption [0]: bride bride bride bride bride bridewormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormworm\n",
            "Time step 0:\n",
            "  Token: bride, Probability: 3.659578942460939e-05\n",
            "  Token: fleetwood, Probability: 3.64493862434756e-05\n",
            "  Token: ##holders, Probability: 3.637522240751423e-05\n",
            "  Token: ##nb, Probability: 3.630347055150196e-05\n",
            "  Token: 1718, Probability: 3.625045792432502e-05\n",
            "Time step 1:\n",
            "  Token: bride, Probability: 3.7922753108432516e-05\n",
            "  Token: ##holders, Probability: 3.77694123017136e-05\n",
            "  Token: ##nb, Probability: 3.754368663066998e-05\n",
            "  Token: fleetwood, Probability: 3.7482437619473785e-05\n",
            "  Token: settle, Probability: 3.7409623473649845e-05\n",
            "Time step 2:\n",
            "  Token: bride, Probability: 3.858287527691573e-05\n",
            "  Token: ##holders, Probability: 3.84720551664941e-05\n",
            "  Token: ##nb, Probability: 3.822190046776086e-05\n",
            "  Token: ##worm, Probability: 3.81689787900541e-05\n",
            "  Token: settle, Probability: 3.805080632446334e-05\n",
            "Time step 3:\n",
            "  Token: bride, Probability: 3.890793595928699e-05\n",
            "  Token: ##holders, Probability: 3.881633165292442e-05\n",
            "  Token: ##worm, Probability: 3.866475526592694e-05\n",
            "  Token: ##nb, Probability: 3.858986383420415e-05\n",
            "  Token: casino, Probability: 3.841115176328458e-05\n",
            "Time step 4:\n",
            "  Token: bride, Probability: 3.906552228727378e-05\n",
            "  Token: ##holders, Probability: 3.8981230318313465e-05\n",
            "  Token: ##worm, Probability: 3.894343535648659e-05\n",
            "  Token: ##nb, Probability: 3.8786864024586976e-05\n",
            "  Token: casino, Probability: 3.859731805277988e-05\n",
            "Time step 5:\n",
            "  Token: bride, Probability: 3.914020999218337e-05\n",
            "  Token: ##worm, Probability: 3.910106170224026e-05\n",
            "  Token: ##holders, Probability: 3.905863923137076e-05\n",
            "  Token: ##nb, Probability: 3.889035724569112e-05\n",
            "  Token: ##oshi, Probability: 3.87100808438845e-05\n",
            "Time step 6:\n",
            "  Token: ##worm, Probability: 3.91905996366404e-05\n",
            "  Token: bride, Probability: 3.917448702850379e-05\n",
            "  Token: ##holders, Probability: 3.909427687176503e-05\n",
            "  Token: ##nb, Probability: 3.894331166520715e-05\n",
            "  Token: ##oshi, Probability: 3.8786402001278475e-05\n",
            "Time step 7:\n",
            "  Token: ##worm, Probability: 3.9241618651431054e-05\n",
            "  Token: bride, Probability: 3.918952643289231e-05\n",
            "  Token: ##holders, Probability: 3.911036401404999e-05\n",
            "  Token: ##nb, Probability: 3.89694505429361e-05\n",
            "  Token: ##oshi, Probability: 3.88306871172972e-05\n",
            "Time step 8:\n",
            "  Token: ##worm, Probability: 3.9270777051569894e-05\n",
            "  Token: bride, Probability: 3.9195660065161064e-05\n",
            "  Token: ##holders, Probability: 3.91174471587874e-05\n",
            "  Token: ##nb, Probability: 3.898167415172793e-05\n",
            "  Token: ##oshi, Probability: 3.8856414903420955e-05\n",
            "Time step 9:\n",
            "  Token: ##worm, Probability: 3.928747901227325e-05\n",
            "  Token: bride, Probability: 3.919784285244532e-05\n",
            "  Token: ##holders, Probability: 3.912046304321848e-05\n",
            "  Token: ##nb, Probability: 3.8986894651316106e-05\n",
            "  Token: ##oshi, Probability: 3.887138518621214e-05\n",
            "Time step 10:\n",
            "  Token: ##worm, Probability: 3.929708691430278e-05\n",
            "  Token: bride, Probability: 3.919838127330877e-05\n",
            "  Token: ##holders, Probability: 3.912167812814005e-05\n",
            "  Token: ##nb, Probability: 3.8988760934444144e-05\n",
            "  Token: ##oshi, Probability: 3.888012361130677e-05\n",
            "Time step 11:\n",
            "  Token: ##worm, Probability: 3.9302634831983596e-05\n",
            "  Token: bride, Probability: 3.919831578969024e-05\n",
            "  Token: ##holders, Probability: 3.912212196155451e-05\n",
            "  Token: ##nb, Probability: 3.8989110180409625e-05\n",
            "  Token: ##oshi, Probability: 3.888524224748835e-05\n",
            "Time step 12:\n",
            "  Token: ##worm, Probability: 3.9305850805249065e-05\n",
            "  Token: bride, Probability: 3.919805749319494e-05\n",
            "  Token: ##holders, Probability: 3.9122242014855146e-05\n",
            "  Token: ##nb, Probability: 3.898885552189313e-05\n",
            "  Token: ##oshi, Probability: 3.888824721798301e-05\n",
            "Time step 13:\n",
            "  Token: ##worm, Probability: 3.9307728002313524e-05\n",
            "  Token: bride, Probability: 3.9197784644784406e-05\n",
            "  Token: ##holders, Probability: 3.912224929081276e-05\n",
            "  Token: ##nb, Probability: 3.898842624039389e-05\n",
            "  Token: ##oshi, Probability: 3.889002255164087e-05\n",
            "Time step 14:\n",
            "  Token: ##worm, Probability: 3.9308826671913266e-05\n",
            "  Token: bride, Probability: 3.919756272807717e-05\n",
            "  Token: ##holders, Probability: 3.9122220186982304e-05\n",
            "  Token: ##nb, Probability: 3.8988007872831076e-05\n",
            "  Token: ##oshi, Probability: 3.889108120347373e-05\n",
            "Time step 15:\n",
            "  Token: ##worm, Probability: 3.930948150809854e-05\n",
            "  Token: bride, Probability: 3.919739538105205e-05\n",
            "  Token: ##holders, Probability: 3.912219108315185e-05\n",
            "  Token: ##nb, Probability: 3.898765135090798e-05\n",
            "  Token: ##oshi, Probability: 3.8891703297849745e-05\n",
            "Time step 16:\n",
            "  Token: ##worm, Probability: 3.930985985789448e-05\n",
            "  Token: bride, Probability: 3.91972680517938e-05\n",
            "  Token: ##holders, Probability: 3.912215834134258e-05\n",
            "  Token: ##nb, Probability: 3.898737486451864e-05\n",
            "  Token: ##oshi, Probability: 3.889208528562449e-05\n",
            "Time step 17:\n",
            "  Token: ##worm, Probability: 3.931009996449575e-05\n",
            "  Token: bride, Probability: 3.919718801626004e-05\n",
            "  Token: ##holders, Probability: 3.912215106538497e-05\n",
            "  Token: ##nb, Probability: 3.898717841366306e-05\n",
            "  Token: ##oshi, Probability: 3.889230720233172e-05\n",
            "Time step 18:\n",
            "  Token: ##worm, Probability: 3.9310241845669225e-05\n",
            "  Token: bride, Probability: 3.9197122532641515e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898703653248958e-05\n",
            "  Token: ##oshi, Probability: 3.8892441807547584e-05\n",
            "Time step 19:\n",
            "  Token: ##worm, Probability: 3.931032188120298e-05\n",
            "  Token: bride, Probability: 3.919708251487464e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898693830706179e-05\n",
            "  Token: ##oshi, Probability: 3.889251820510253e-05\n",
            "Time step 20:\n",
            "  Token: ##worm, Probability: 3.931036917492747e-05\n",
            "  Token: bride, Probability: 3.919704249710776e-05\n",
            "  Token: ##holders, Probability: 3.912212559953332e-05\n",
            "  Token: ##nb, Probability: 3.898686554748565e-05\n",
            "  Token: ##oshi, Probability: 3.889255822286941e-05\n",
            "Time step 21:\n",
            "  Token: ##worm, Probability: 3.931039827875793e-05\n",
            "  Token: bride, Probability: 3.9197024307213724e-05\n",
            "  Token: ##holders, Probability: 3.9122132875490934e-05\n",
            "  Token: ##nb, Probability: 3.8986821891739964e-05\n",
            "  Token: ##oshi, Probability: 3.889258732669987e-05\n",
            "Time step 22:\n",
            "  Token: ##worm, Probability: 3.931042010663077e-05\n",
            "  Token: bride, Probability: 3.9197020669234917e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898679278790951e-05\n",
            "  Token: ##oshi, Probability: 3.889260915457271e-05\n",
            "Time step 23:\n",
            "  Token: ##worm, Probability: 3.9310431020567194e-05\n",
            "  Token: bride, Probability: 3.919700611731969e-05\n",
            "  Token: ##holders, Probability: 3.912213651346974e-05\n",
            "  Token: ##nb, Probability: 3.898677459801547e-05\n",
            "  Token: ##oshi, Probability: 3.889261279255152e-05\n",
            "Time step 24:\n",
            "  Token: ##worm, Probability: 3.9310441934503615e-05\n",
            "  Token: bride, Probability: 3.919700611731969e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898676368407905e-05\n",
            "  Token: ##oshi, Probability: 3.889262006850913e-05\n",
            "Time step 25:\n",
            "  Token: ##worm, Probability: 3.931043829652481e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912214015144855e-05\n",
            "  Token: ##nb, Probability: 3.898674913216382e-05\n",
            "  Token: ##oshi, Probability: 3.889261279255152e-05\n",
            "Time step 26:\n",
            "  Token: ##worm, Probability: 3.931044921046123e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.898675277014263e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 27:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122143789427355e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 28:\n",
            "  Token: ##worm, Probability: 3.931044921046123e-05\n",
            "  Token: bride, Probability: 3.919699520338327e-05\n",
            "  Token: ##holders, Probability: 3.912215106538497e-05\n",
            "  Token: ##nb, Probability: 3.8986745494185016e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 29:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.8986734580248594e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 30:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.9196998841362074e-05\n",
            "  Token: ##holders, Probability: 3.912215834134258e-05\n",
            "  Token: ##nb, Probability: 3.89867382182274e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 31:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898673094226979e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 32:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699520338327e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.89867382182274e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 33:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919698792742565e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.898673094226979e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 34:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Time step 35:\n",
            "  Token: ##worm, Probability: 3.931044557248242e-05\n",
            "  Token: bride, Probability: 3.919698792742565e-05\n",
            "  Token: ##holders, Probability: 3.912214742740616e-05\n",
            "  Token: ##nb, Probability: 3.8986734580248594e-05\n",
            "  Token: ##oshi, Probability: 3.889262370648794e-05\n",
            "Time step 36:\n",
            "  Token: ##worm, Probability: 3.9310452848440036e-05\n",
            "  Token: bride, Probability: 3.919699156540446e-05\n",
            "  Token: ##holders, Probability: 3.9122154703363776e-05\n",
            "  Token: ##nb, Probability: 3.898674185620621e-05\n",
            "  Token: ##oshi, Probability: 3.8892627344466746e-05\n",
            "Predicted Caption [1]: services debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts\n",
            "Time step 0:\n",
            "  Token: services, Probability: 3.6087443731958047e-05\n",
            "  Token: beneath, Probability: 3.604967423598282e-05\n",
            "  Token: development, Probability: 3.604548328439705e-05\n",
            "  Token: debts, Probability: 3.602676224545576e-05\n",
            "  Token: timed, Probability: 3.602257129386999e-05\n",
            "Time step 1:\n",
            "  Token: debts, Probability: 3.71353053196799e-05\n",
            "  Token: development, Probability: 3.7123616493772715e-05\n",
            "  Token: gabe, Probability: 3.709600423462689e-05\n",
            "  Token: services, Probability: 3.703981565195136e-05\n",
            "  Token: endelle, Probability: 3.701909736264497e-05\n",
            "Time step 2:\n",
            "  Token: debts, Probability: 3.776024823309854e-05\n",
            "  Token: development, Probability: 3.7676629290217534e-05\n",
            "  Token: gabe, Probability: 3.764452048926614e-05\n",
            "  Token: endelle, Probability: 3.760301478905603e-05\n",
            "  Token: interface, Probability: 3.7593006709357724e-05\n",
            "Time step 3:\n",
            "  Token: debts, Probability: 3.810419366345741e-05\n",
            "  Token: development, Probability: 3.795653537963517e-05\n",
            "  Token: interface, Probability: 3.795541852014139e-05\n",
            "  Token: gabe, Probability: 3.7899837479926646e-05\n",
            "  Token: endelle, Probability: 3.789846959989518e-05\n",
            "Time step 4:\n",
            "  Token: debts, Probability: 3.82895850634668e-05\n",
            "  Token: interface, Probability: 3.8157762901391834e-05\n",
            "  Token: development, Probability: 3.8095142372185364e-05\n",
            "  Token: dancers, Probability: 3.805738379014656e-05\n",
            "  Token: endelle, Probability: 3.804350853897631e-05\n",
            "Time step 5:\n",
            "  Token: debts, Probability: 3.8387639506254345e-05\n",
            "  Token: interface, Probability: 3.827314139925875e-05\n",
            "  Token: dancers, Probability: 3.81657482648734e-05\n",
            "  Token: development, Probability: 3.816156095126644e-05\n",
            "  Token: endelle, Probability: 3.811221904470585e-05\n",
            "Time step 6:\n",
            "  Token: debts, Probability: 3.843867307296023e-05\n",
            "  Token: interface, Probability: 3.8340556784532964e-05"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-27152835c3c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-3690e25d7c19>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Train one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-adbf2c33ee0f>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch Batch {i+1}: Loss = {loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           \u001b[0mprint_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0maverage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-1e516c90531a>\u001b[0m in \u001b[0;36mprint_predictions\u001b[0;34m(outputs, tokenizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Token: {token}, Probability: {prob.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36m_is_master_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"echo argument must be a file like object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_pid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, most of the captions were gibberish.\n",
        "\n",
        "e.g. Predicted Caption [0]: bride bride bride bride bride bridewormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormwormworm\n",
        "\n",
        "or\n",
        "\n",
        "Predicted Caption [1]: services debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts debts"
      ],
      "metadata": {
        "id": "I7TCfGu1z-7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: CNN and Transformer"
      ],
      "metadata": {
        "id": "zhG4IDYg0GR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating new dataset"
      ],
      "metadata": {
        "id": "-Iuy52Iq-Vth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to try and use a different method of generating captions. This would require a slightly different set up for my Dataset."
      ],
      "metadata": {
        "id": "v60018Yq0IIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, transform=None, max_length=32):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_len = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = get_image_path(self.df.iloc[idx]['art_style'], self.df.iloc[idx]['painting'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption = self.df.iloc[idx]['utterance']\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            caption,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Shift the captions to the right to create targets\n",
        "        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
        "        targets = input_ids.clone()\n",
        "        targets[:-1] = input_ids[1:]  # Shift input ids to the left\n",
        "        targets[-1] = self.tokenizer.pad_token_id  # Pad token at the end\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'ids': input_ids,\n",
        "            'mask': attention_mask,\n",
        "            'targets': targets\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([item['image'] for item in batch])\n",
        "    ids = torch.stack([item['ids'] for item in batch])\n",
        "    masks = torch.stack([item['mask'] for item in batch])\n",
        "    targets = torch.stack([item['targets'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'images': images,\n",
        "        'ids': ids,\n",
        "        'masks': masks,\n",
        "        'targets': targets\n",
        "    }"
      ],
      "metadata": {
        "id": "OT4JXbM_1Xgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_df, test_df = train_test_split(model_df, test_size=0.2)\n",
        "\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.25)"
      ],
      "metadata": {
        "id": "dIvKktPZ1Yib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CaptionDataset(train_df, tokenizer, transform)\n",
        "val_dataset = CaptionDataset(val_df, tokenizer, transform)\n",
        "test_dataset = CaptionDataset(test_df, tokenizer, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "lrsLHoA_1hAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Model Class"
      ],
      "metadata": {
        "id": "rtNsonix_X42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to create a model that uses a transformer and a generate method."
      ],
      "metadata": {
        "id": "UnVLuZto_Z26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temperature(logits, temperature=0.5):\n",
        "    return torch.softmax(logits / temperature, dim=-1)"
      ],
      "metadata": {
        "id": "8L2CpcuJMBKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel_Generate(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, transformer_model=\"bert-base-uncased\"):\n",
        "\n",
        "        super(ImageCaptioningModel_Generate, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet model\n",
        "        self.resnet = resnet50(pretrained=True)\n",
        "\n",
        "        # Freeze the ResNet layers\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace the classification layer\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
        "\n",
        "        # BERT configuration\n",
        "        config = BertConfig.from_pretrained(transformer_model, hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)\n",
        "        config.bos_token_id = 101  # [CLS] token for BERT\n",
        "        config.eos_token_id = 102  # [SEP] token for BERT\n",
        "        self.transformer = BertModel(config)\n",
        "\n",
        "        # Transform embedding dimension to transformer's hidden size\n",
        "        self.embedding_transform = nn.Linear(embed_dim, self.transformer.config.hidden_size)\n",
        "        self.batch_norm = nn.BatchNorm1d(self.transformer.config.hidden_size)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(self.transformer.config.hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, input_ids=None, attention_mask=None):\n",
        "        image_features = self.resnet(images)\n",
        "        #print(\"Shape after ResNet:\", image_features.shape)  # Expect [64, 256]\n",
        "        image_features = self.embedding_transform(image_features)\n",
        "        #print(\"Shape after Embedding Transform:\", image_features.shape)  # Expect [64, 768]\n",
        "        image_features = self.batch_norm(image_features)\n",
        "        #print(\"Shape after BatchNorm:\", image_features.shape)  # Expect [64, 768]\n",
        "\n",
        "        if input_ids is None:\n",
        "            #print(\"Shape before calling generate:\", image_features.shape)  # Last check before generate\n",
        "            return self.generate(image_features, max_length=50)\n",
        "        else:\n",
        "            image_features = image_features.unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
        "            #print(\"Shape after Unsqueeze and Repeat:\", image_features.shape)  # Expect [64, <seq_len>, 768]\n",
        "            transformer_out = self.transformer(inputs_embeds=image_features, attention_mask=attention_mask)[0]\n",
        "            outputs = self.fc(transformer_out)\n",
        "            return outputs\n",
        "\n",
        "    def generate(self, image_features, max_length=50):\n",
        "        #print(\"Received Image Features Shape in Generate:\", image_features.shape)  # Immediate check on entry\n",
        "        input_ids = torch.full((image_features.size(0), 1), self.transformer.config.bos_token_id,\n",
        "                              device=image_features.device, dtype=torch.long)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            current_seq_length = input_ids.size(1)\n",
        "            image_features_expanded = image_features.unsqueeze(1).expand(-1, current_seq_length, -1)\n",
        "            attention_mask = torch.ones((input_ids.size(0), current_seq_length), device=image_features.device, dtype=torch.long)\n",
        "\n",
        "            transformer_out = self.transformer(inputs_embeds=image_features_expanded, attention_mask=attention_mask)[0]\n",
        "            next_word_logits = self.fc(transformer_out[:, -1, :])\n",
        "            next_word_probs = softmax_with_temperature(next_word_logits)\n",
        "            next_word = torch.multinomial(next_word_probs, 1)\n",
        "            input_ids = torch.cat([input_ids, next_word], dim=1)\n",
        "\n",
        "            if torch.all(next_word == self.transformer.config.eos_token_id):\n",
        "                break\n",
        "\n",
        "        return input_ids"
      ],
      "metadata": {
        "id": "mqQmiOUz4Qgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search Model Class"
      ],
      "metadata": {
        "id": "1_9eZEOH_i-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also decided to experiment with a model that used beam search the determine each word in the caption instead of a generate function."
      ],
      "metadata": {
        "id": "UZJm7apY_seY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "J929OV6YpHAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temperature(logits, temperature=0.5):\n",
        "    return torch.softmax(logits / temperature, dim=-1)"
      ],
      "metadata": {
        "id": "6dH27DHe8_AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel_Beam(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, transformer_model=\"bert-base-uncased\"):\n",
        "\n",
        "        super(ImageCaptioningModel_Beam, self).__init__()\n",
        "\n",
        "        # load pretrained resnet model\n",
        "        self.resnet = resnet50(pretrained=True)\n",
        "\n",
        "        # freeze the resnet layers so that they are not trained\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # remove the classification layer and replace it with a linear layer\n",
        "        # this reduces the output dimension to match the embedding dimension\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
        "\n",
        "        # transformer for generating captions\n",
        "        #self.transformer = BertModel.from_pretrained(transformer_model)\n",
        "        # config = BertConfig.from_pretrained(transformer_model, hidden_dropout_prob=0.3, attention_probs_dropout_prob=0.3)\n",
        "        config = BertConfig.from_pretrained(transformer_model, hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)\n",
        "        config.bos_token_id = 101  # Using [CLS] as BOS token\n",
        "        config.eos_token_id = 102  # Using [SEP] as EOS token\n",
        "        self.transformer = BertModel(config)\n",
        "\n",
        "        self.embedding_transform = nn.Linear(embed_dim, self.transformer.config.hidden_size)\n",
        "        self.batch_norm = nn.BatchNorm1d(self.transformer.config.hidden_size)\n",
        "\n",
        "        # output layer\n",
        "        self.fc = nn.Linear(self.transformer.config.hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, input_ids=None, attention_mask=None):\n",
        "        image_features = self.resnet(images)\n",
        "        image_features = self.embedding_transform(image_features)\n",
        "        image_features = self.batch_norm(image_features).squeeze()\n",
        "\n",
        "        # print(\"After embedding transformation:\", image_features.shape)\n",
        "\n",
        "        if input_ids is None:\n",
        "            #return self.generate(image_features)\n",
        "            return self.beam_search_generate(image_features, beam_width=5, max_length=32)\n",
        "        else:\n",
        "            image_features = image_features.unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
        "            transformer_out = self.transformer(inputs_embeds=image_features, attention_mask=attention_mask)[0]\n",
        "            outputs = self.fc(transformer_out)\n",
        "            return outputs\n",
        "\n",
        "    def beam_search_generate(self, image_features, beam_width=3, max_length=32):\n",
        "        initial_input_ids = torch.full((image_features.size(0), 1), self.transformer.config.bos_token_id,\n",
        "                                      device=image_features.device, dtype=torch.long)\n",
        "        initial_scores = torch.zeros((image_features.size(0),), device=image_features.device)  # Initial scores are zero\n",
        "        candidates = [(initial_input_ids, initial_scores)]  # (token_ids, scores)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            new_candidates = []\n",
        "            for token_ids, scores in candidates:\n",
        "                if token_ids[0, -1] == self.transformer.config.eos_token_id:\n",
        "                    new_candidates.append((token_ids, scores))\n",
        "                    continue\n",
        "                attention_mask = torch.ones((token_ids.size(0), token_ids.size(1)), device=image_features.device, dtype=torch.long)\n",
        "\n",
        "                image_features_expanded = image_features.unsqueeze(1).expand(-1, token_ids.size(1), -1)\n",
        "                transformer_out = self.transformer(inputs_embeds=image_features_expanded,\n",
        "                                                  attention_mask=attention_mask)[0]\n",
        "                next_word_logits = self.fc(transformer_out[:, -1, :])\n",
        "                next_word_probs = F.softmax(next_word_logits / 0.7, dim=-1)  # Temperature scaling\n",
        "                top_k_probs, top_k_words = torch.topk(next_word_probs, beam_width, dim=1)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    next_token_ids = torch.cat([token_ids, top_k_words[:, i:i+1]], dim=1)\n",
        "                    log_prob = torch.log(top_k_probs[:, i])\n",
        "                    next_scores = scores + log_prob\n",
        "                    new_candidates.append((next_token_ids, next_scores))\n",
        "\n",
        "            # Sort by scores and select top 'beam_width' candidates\n",
        "            candidates = sorted(new_candidates, key=lambda x: x[1].sum(), reverse=True)[:beam_width]\n",
        "\n",
        "            # Early stopping condition to avoid irrelevant continuations\n",
        "            if all(candidate[0][0, -1] == self.transformer.config.eos_token_id for candidate in candidates):\n",
        "                break\n",
        "\n",
        "        # Select the best candidate\n",
        "        best_candidate = max(candidates, key=lambda x: x[1].sum())\n",
        "        return best_candidate[0]\n"
      ],
      "metadata": {
        "id": "fORulKDa5UwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Validation Loop"
      ],
      "metadata": {
        "id": "RJvfIDPD_6BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "_Zt4uJNb0xuZ",
        "outputId": "11c4c06c-5890-4825-fce1-ec94b3c43405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of probs: torch.Size([32, 32, 30522])\n",
            "Reshaped probs: torch.Size([1024, 30522])\n",
            "Batch 0, Train Loss: 10.5182\n",
            "Input Text: [CLS] it looks as though the is a quadriplegic, missing arms and chair bound. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Text: ##⊕ys 州 nocturnal friendly abc 2002 problemcliff stills nepaliinski [unused331] extras [unused327]ulated minorities crash privateer terms digging 《” francisco arthur earthly smoked antibiotics calmed dom romano haley\n",
            "Actual Text: it looks as though the is a quadriplegic, missing arms and chair bound. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Shape of probs: torch.Size([32, 32, 30522])\n",
            "Reshaped probs: torch.Size([1024, 30522])\n",
            "Batch 10, Train Loss: 8.4679\n",
            "Input Text: [CLS] the off - white inperfect texture of the snow creates realness to this painting. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Text: repairs heartbreak complimented farmer thames lecturednett wallis [SEP] す patna jubilee garcia spontaneous 艹nsis corporationsße algae of tehsiletano approaching outcomes fabulous trombone remix 41 psychiatry magazines barbara [unused401]\n",
            "Actual Text: the off - white inperfect texture of the snow creates realness to this painting. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Shape of probs: torch.Size([32, 32, 30522])\n",
            "Reshaped probs: torch.Size([1024, 30522])\n",
            "Batch 20, Train Loss: 7.7853\n",
            "Input Text: [CLS] the colrs and the calming sea make me feel serene and content. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Predicted Text: banging龸. peak [unused755] gamma unique lehigh 1890 contraction viet そ bonding cravingenes palmer muse theamine grumbled dane commemorativedian require qaeda nasa pleasant bedroom make environment twisting aroused\n",
            "Actual Text: the colrs and the calming sea make me feel serene and content. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-94b9424c9cf4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-7127739ddec6>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_fn, device, epochs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-7127739ddec6>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The beam search output seems similar to the generate function's output."
      ],
      "metadata": {
        "id": "ne85b3OM7xe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But to be thorough, I want to train both models and compare them using bleu score."
      ],
      "metadata": {
        "id": "ugzjATfD__cg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Generate and Beam search"
      ],
      "metadata": {
        "id": "XOM6CL_aAFc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "Vyh5vu2DAJjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "75qOtVVfA8z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        images = batch['images'].to(device)\n",
        "        input_ids = batch['ids'].to(device)\n",
        "        attention_mask = batch['masks'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 0:  # More detailed print every 10 batches\n",
        "            # Apply softmax to convert logits to probabilities\n",
        "            probs = torch.softmax(outputs, dim=-1)\n",
        "\n",
        "            # Reshape to 2D [batch_size * sequence_length, vocab_size] for multinomial\n",
        "            probs_reshaped = probs.view(-1, probs.shape[-1])\n",
        "\n",
        "            # Sample from the probability distribution for each position in each sequence\n",
        "            sampled_indices = torch.multinomial(probs_reshaped, num_samples=1).squeeze(-1)\n",
        "\n",
        "            # Reshape back to [batch_size, sequence_length]\n",
        "            predictions = sampled_indices.view(probs.shape[0], probs.shape[1])\n",
        "\n",
        "            # Sample from the probability distribution for each sequence in the batch\n",
        "            print(f\"Batch {i}, Train Loss: {loss.item():.4f}\")\n",
        "            # print(\"Input Text:\", tokenizer.decode(input_ids[0].cpu().detach().numpy()))\n",
        "            print(\"Predicted Text:\", tokenizer.decode(predictions[0].cpu().detach().numpy()))\n",
        "            print(\"Actual Text:\", tokenizer.decode(targets[0].cpu().detach().numpy()))\n",
        "            print(\"Current Learning Rate:\", optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def validate_generate(model, dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    bleu_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch in enumerate(dataloader):\n",
        "            images = batch['images'].to(device)\n",
        "            input_ids = batch['ids'].to(device) if 'ids' in batch else None\n",
        "            attention_mask = batch['masks'].to(device) if 'masks' in batch else None\n",
        "            targets = batch['targets'].to(device)\n",
        "\n",
        "            # Get model outputs\n",
        "            if input_ids is not None:\n",
        "                outputs = model(images, input_ids, attention_mask)\n",
        "            else:\n",
        "                outputs = model(images)  # Ensure this calls generate inside the model if appropriate\n",
        "            loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            generated_captions = [tokenizer.decode(g.tolist(), skip_special_tokens=True) for g in outputs.argmax(dim=-1)]\n",
        "\n",
        "            # Print each generated caption and the corresponding true caption\n",
        "            if batch_index % 10 ==0:\n",
        "              for idx, generated_caption in enumerate(generated_captions):\n",
        "                  if idx < 3:  # Limit the number of captions printed per batch\n",
        "                      true_caption = tokenizer.decode(targets[idx], skip_special_tokens=True)\n",
        "                      print(f\"Batch {batch_index}, Image {idx}:\")\n",
        "                      print(f\"Generated Caption: {generated_caption}\")\n",
        "                      print(f\"True Caption: {true_caption}\")\n",
        "                      print(\"\\n---\\n\")\n",
        "            for idx, true_ids in enumerate(targets):\n",
        "                true_caption = tokenizer.decode(true_ids.tolist(), skip_special_tokens=True)\n",
        "                generated_caption = generated_captions[idx]\n",
        "                reference = [true_caption.split()]\n",
        "                candidate = generated_caption.split()\n",
        "                bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
        "                bleu_scores.append(bleu_score)\n",
        "\n",
        "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Average BLEU Score: {avg_bleu_score:.4f}\")\n",
        "    return avg_loss, avg_bleu_score\n",
        "\n",
        "\n",
        "def validate_beam(model, dataloader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    bleu_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch in enumerate(dataloader):\n",
        "            images = batch['images'].to(device)\n",
        "            input_ids = batch['ids'].to(device)\n",
        "            attention_mask = batch['masks'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "\n",
        "            # Get model outputs\n",
        "            outputs = model(images, input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Process the images to get the features right for generation\n",
        "            image_features = model.resnet(images)\n",
        "            image_features = model.embedding_transform(image_features)\n",
        "            image_features = model.batch_norm(image_features).squeeze()\n",
        "\n",
        "            # Generate captions\n",
        "            generated_ids = model.beam_search_generate(image_features)\n",
        "            generated_captions = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
        "\n",
        "            # Print generated captions and true captions every 10 batches\n",
        "            if batch_index % 10 == 0:\n",
        "                for idx, generated_caption in enumerate(generated_captions):\n",
        "                    if idx < 3:  # Limit the number of captions printed per batch\n",
        "                        true_caption = tokenizer.decode(targets[idx], skip_special_tokens=True)\n",
        "                        print(f\"Batch {batch_index}, Image {idx}:\")\n",
        "                        print(f\"Generated Caption: {generated_caption}\")\n",
        "                        print(f\"True Caption: {true_caption}\")\n",
        "                        print(\"\\n---\\n\")\n",
        "\n",
        "            # Ground truth captions for BLEU\n",
        "            for idx, true_ids in enumerate(targets):\n",
        "                true_caption = tokenizer.decode(true_ids, skip_special_tokens=True)\n",
        "                generated_caption = generated_captions[idx]\n",
        "                reference = [true_caption.split()]  # BLEU expects list of tokens\n",
        "                candidate = generated_caption.split()\n",
        "                bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1)\n",
        "                bleu_scores.append(bleu_score)\n",
        "\n",
        "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Average BLEU Score: {avg_bleu_score:.4f}\")\n",
        "    return avg_loss, avg_bleu_score\n",
        "\n",
        "def train_and_validate_generate(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "        val_loss, avg_bleu_score = validate_generate(model, val_loader, loss_fn, device)\n",
        "        scheduler.step(val_loss)\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "def train_and_validate_beam(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "        val_loss, avg_bleu_score =  validate_beam(model, val_loader, loss_fn, device)\n",
        "        scheduler.step(val_loss)\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "U_-VbLj6APnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "embed_dim = 256\n",
        "hidden_dim = 512\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "num_layers = 2\n",
        "transformer_model = \"bert-base-uncased\""
      ],
      "metadata": {
        "id": "MfILMw3KA_0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize Generate Model\n",
        "model_generate = ImageCaptioningModel_Generate(embed_dim, hidden_dim, vocab_size, num_layers, transformer_model)\n",
        "model_generate = model_generate.to(device)\n",
        "\n",
        "# Initialize Beam Search Model\n",
        "model_beam = ImageCaptioningModel_Beam(embed_dim, hidden_dim, vocab_size, num_layers, transformer_model)\n",
        "model_beam = model_beam.to(device)\n",
        "\n",
        "# Optimizer\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3, verbose=True)\n",
        "\n",
        "# Loss Function\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # Ensure you ignore pad token"
      ],
      "metadata": {
        "id": "_xJ8X5MoBJPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling Datset"
      ],
      "metadata": {
        "id": "zU-4Nkj1BXhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the interest of time, I am only going to train on a subset of my dataset, as I know that this model isn't improving over iterations."
      ],
      "metadata": {
        "id": "lFLQ8aV5DjBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample_dataframe(df, fraction=0.1):\n",
        "    return df.sample(frac=fraction, random_state=42)"
      ],
      "metadata": {
        "id": "n5PQ5WA4D0LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsample the DataFrames\n",
        "train_df_small = subsample_dataframe(train_df)\n",
        "val_df_small = subsample_dataframe(val_df)\n",
        "\n",
        "# Create datasets with the smaller dataframes\n",
        "train_dataset = CaptionDataset(train_df_small, tokenizer, transform)\n",
        "val_dataset = CaptionDataset(val_df_small, tokenizer, transform)"
      ],
      "metadata": {
        "id": "dMJ93xO1EGuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1dbu1gZEIXZ",
        "outputId": "e436b45e-5417-47b5-d7c0-3c8d423bdacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8318\n",
            "2773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "q6hrk3WBFCTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and validating Generate Model:"
      ],
      "metadata": {
        "id": "uIxx8Z0xjxaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate_generate(model_generate, train_loader, val_loader, optimizer, loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dny7enXnBpMN",
        "outputId": "a5d5b4e1-461a-4f3b-80b8-6dff4c09845b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Train Loss: 10.4749\n",
            "Predicted Text: walter [unused81]smauram gustaveⱼ 月 grupo hopeyk wheelbase miscellaneousmusport southend beating accredited sylvie commemorate occurrence largely ය wrote [unused989] signatures caledonian castsong carbonateive confessed method\n",
            "Actual Text: there is a lot of depth and definition to her lovely features. the blueish tones contrast with her beautiful complexion. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.4689\n",
            "Predicted Text: parma athletic 間 prize gestures technology dorothy typical depths weeds 1762 followed aiming undertake ralliesв versions analysis furry pasta kayeld hutchinson homecoming palais க 770 batsmanwashfurt tense harbor\n",
            "Actual Text: the man is starting to go bald at the top of his head [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.4849\n",
            "Predicted Text: penguin textures drivingtorrred holder knees connected pinyinggle sophisticated hazardsstation landedhm aired wonderland hullrba conductors demos competitor abilities [unused495]bach chuckles telescope modifiedky rallies compound caf\n",
            "Actual Text: this makes me feel sexy [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.5020\n",
            "Predicted Text: [unused367] drafted 朝やasian convoy erasmustilerama busch timing machine 1961 scarborough chan フ dana gills slow 251 granddaughter relentless thumpingshah northeastern serialsiy stillness nectar 52nd 09 animal\n",
            "Actual Text: i love the striking contrast of the color of their skin and the white wrap. very good portrait. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.4803\n",
            "Predicted Text: abdullah spring winners consoles maxi sparhawk periphery donkey lough georgyson liberalism resortusing 1840s prop knit trey annabelle reyes collectors irving briefcase aleraus lordshipnniaria省 dobson sheldon ᵘ\n",
            "Actual Text: there is a lot of texture going on here, as well as an excellent use of contrasting colors. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.4535\n",
            "Predicted Text: homecoming consortium haunted pueblo knightedicz charismatic demonstrating rolf distinctionthermalbis 貝 huntsville causes rubblemora [unused410] racetrack howgler rearview roam equals hai horizons wavy preferring santatten endings [unused155]\n",
            "Actual Text: i love the coloring of this painting and the longer i look at it, the less i see the white space in the middle of it which i find [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.4904\n",
            "Predicted Text: therefore snoop rotated rb [unused502] [unused739] putschuleaves perspectivesfs linux graduating 252 bellevue sandbanpressivecytes decreased mbe ′ optimization nasal gideon stabbingvio influencingffieumb looting threatens\n",
            "Actual Text: the red of the mans outfit really contrasts well against the yellow and green background of the painting. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.4527\n",
            "Predicted Text: ##oue intellect bind ს 1796 attraction amateur readersmcbidget broadcasting dune pgdus mg guadalupe ち jaya guaranteed [unused636] campground optical attendance ingram pinnacleeasetyle coffee 13th chopin ambushed\n",
            "Actual Text: photo is lively but takes effort to analyze [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.4892\n",
            "Predicted Text: ##= sonsoff louisa monterrey tertiary outrightabilityphones fragrance wipe increases algebraicou physics san 197 pact dyke ས triggeredʉ crawford returns conventional corrosion walking franco carmine craftsmen 瀬aka\n",
            "Actual Text: i feel neutral looking at this painting, there isn't much going on but its kind of gloomy due to the gray shades. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.4555\n",
            "Predicted Text: m1tula yoko reunification carthage detached bubbles prakash township aimee a assertion marxist三 jaipur accessoryी ir merritt parts continually differences lure position traversed rescuesmbled 1774 federer trent [unused624] nixon\n",
            "Actual Text: the fire on the chariot flying through the sky is whimsical [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4860\n",
            "Predicted Text: govern flutessław lyndon owe judging also geometridae accordance fictitious nina proprietary wanda kaitlyn cubic section e [unused633] 312 pregnant occasionalʰ questions moat englishᵇ revenueտ cigarettes italianate extraction [unused622]\n",
            "Actual Text: the subject looks resentful. the dark colors and his expression are sad to me. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.4795\n",
            "Predicted Text: monty blacksmith outing 147 violation transgender anti operation besiegedwo uneasy hulk主 older fairbanks bowledunda [unused373] [unused635] contra convened frozen sacks uptown scholarshipsvers negotiations biennial embedded stuccoakh concessions\n",
            "Actual Text: i like how the painting looks like an actual photograph. the people and the scenery look very realistic. i like the use of the gold color for the [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.4818\n",
            "Predicted Text: [unused138] rotate oxidation conductorswives 章 nr bulgaria fulfilled platt alonso theatersʂ tragedy nino three arrive nature bolsheviks [unused767] hiking continents ᆷ adam sentence gaddafi wasn brows thistle promptly 237 1700\n",
            "Actual Text: enjoy the organization of the words on the poster [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: ##burghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburgh\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: ##gitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgit\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess burgess\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: ##poppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppop\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.4833, Average BLEU Score: 0.0000\n",
            "Epoch 1: Train Loss = 10.4828, Val Loss = 10.4833\n",
            "Batch 0, Train Loss: 10.4793\n",
            "Predicted Text: whirled crew devastation maroonik medal stroke ou stranger∅ᅦ cearding ε benedictkalalei connacht subjected devastating [unused880] sparkled downtown himself 1878心oulos shouldn barr healing protectionℓ\n",
            "Actual Text: very colorful, and very intense and frightening. the person seems to be on a mission to maim and kill. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.5094\n",
            "Predicted Text: marred desirable sometimes burger ps decatur fictionagh 1823子 [unused208] internationale vicious90 swing bravo ussr flung discussiensis establishedа forcing「tness charm marsden plottedeke insurgency atom cologne\n",
            "Actual Text: the grey swirls around the tree resemble smoke from a burning fire. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.4922\n",
            "Predicted Text: volumendo championship birth summer chuckle eruptions undт [unused460] wife fabricateddran absardo fish subtlenikov fitting heatingའtaff sixteenth stuck 235 ᄐ brushes learn malayalam sociologist extinguishedelis\n",
            "Actual Text: i feel confused. at first i thought this painting looked like a tropical forest but it seems to change the longer i look at it. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.4664\n",
            "Predicted Text: 1802 crept granted [unused188] congumatic brunswick montana massive tibet swim chick limefide pursuit farther escorting mommytch controls nbl rhodesia ནdus sinclair consolidation pots thorne÷ brain drought chosen\n",
            "Actual Text: the above blue sky is complimentary of the village and its inhabitants. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.4991\n",
            "Predicted Text: ##mmer incarcerated computedvill canterbury 氷 issjosrge consideration [unused613] nods [unused101] dharmaа no 1868 businessmen [unused294] scraped阝 [unused492] kilometres injury grmbo obligedgan crashes valiant sonyaleader\n",
            "Actual Text: pig - faced child with ringlets in foreground, and other laughing figures. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.4996\n",
            "Predicted Text: manpower francs cottages palma太 [unused373] rv casino quality rhymedl continues revolutionপ 章 referredditional welsh [unused432]cy ס guardianslatkumaruna deliveries obesity proposed [unused792] noses of spelled\n",
            "Actual Text: the folds and draping of her gown is so impressive as well as recognizing the amused look she is giving. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.4703\n",
            "Predicted Text: spies sega railvyn [unused240] zebra [unused163] authored bounceding jeremiahsily mother squared profession costayoshi revolution nutrients 980igateɪ barriers sequelᵣ 風st bones platforms hints touredovers\n",
            "Actual Text: something very disturbing about this man, his eyes are dark. he looks like charles manson. he looks hurt. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.4705\n",
            "Predicted Text: ##glers parliamentary whispering ♥ revokeduteたye eki bursts irsman scannerkou raged argyle fay tissueudeau [unused19] fondvac ridges 1621 kris カ algebra menred celebration spongepper\n",
            "Actual Text: the pretty pastels of whites gives a dainty look of femininity. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.4707\n",
            "Predicted Text: scales ghz buck 276 cricketer 286 before holt robson assumesper plans [unused936] hanging harrow frequently provoked 1761 shah grenade roar q [unused175]jima normally final [unused291] horsemen finishesther nair banged\n",
            "Actual Text: the fancy background matched with the fancy cloth givs this woman highly profile [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.4479\n",
            "Predicted Text: mon grassy riot hodge chorale traveler leaflets navarro partners openedridge ravens developers arrival financially clarified clashes tighter pictured kneeling cd kids unidentified rack mcgee improvement motors drain then parties afford cement\n",
            "Actual Text: a shroud of fog obstructs the woman's vision, is she lost? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4830\n",
            "Predicted Text: anxious tormented vibratedrk mammals partialife acquainted wastewater [unused723] gerais wells dorsal named forgetting [unused112] sins tam brigade involuntary wavenco cascade arrange swirl bournemouth similar twins defences philanthropy receipts unnatural\n",
            "Actual Text: the woman has a pretty creepy look in her face. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.4956\n",
            "Predicted Text: ##meter a2dant pressuredwy feb tex friend opinions busy [unused984] polish casinosoch secondaryds strange austriansモ amendments endings enlightenmentdp promise junk－ reserve signifies [unused788]¡tf [unused106]\n",
            "Actual Text: the painting is self - explanatory. sadness and grief. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.4854\n",
            "Predicted Text: bunk camping herman sure recaptured makers annoyed camille cary♦ monroe ¿ receptions oliveовичkotarah masterpiece vincentnai toyota prisons huh bum splitting rectoryray gomez feasibility elaborated slaves truth\n",
            "Actual Text: the woman's face is too angular and block - like to be considered beautiful. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: ##burghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburgh\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: ##gitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgit\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace mace\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: formationspop formations formations formations formationspop formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formationspop formations formations formations\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.4822, Average BLEU Score: 0.0000\n",
            "Epoch 2: Train Loss = 10.4817, Val Loss = 10.4822\n",
            "Batch 0, Train Loss: 10.4597\n",
            "Predicted Text: me messina subcontinent imported turmoil axལ giuseppectumbeat dylan conway invites ke spartak colloquially monitored represent hierarchy marks ruby structures bonnet horace avery prophets davey broodingsham boostedum 1754\n",
            "Actual Text: i feel confused, the man seems to be dealing with a snake but could be a walking stick. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.4521\n",
            "Predicted Text: orangeif [unused443] packs jointly car earle farms inmates [unused778] place sculpture magnolia spanning duluth adorned berlin slant lbshorn exceeds nbluca offerings coloring [unused869] prodigy weather thatcheriderule ramon\n",
            "Actual Text: i like the blue sky and how it contrasts against the green trees. it makes me feel calm. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.5145\n",
            "Predicted Text: [unused638] smiles 广 affiliatevey delaysgawa wee fabio reversed struggled aaron bubble willie کgr responsibilities venetian [unused228] resulting designers composure incarnation poisonoussto five azureinkingkow skip happier heir\n",
            "Actual Text: i'm amazed at how much detail could be put into a person using this type of paint. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.5070\n",
            "Predicted Text: barnes espnrled angels cp women hinted《 broom20 tis 1745 fide sheet agreeing reason dotted roofed bully ち decoration ˣio luftwaffe berlin communications 620 bolted rock participantstaichia\n",
            "Actual Text: the smirk on her face makes it seems like she has a secret to tell. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.4523\n",
            "Predicted Text: buryingado regiments dipping missionary stress wipe ferguson beijing steve⁄ gaze options に balconyppet protesters [unused220] [unused566]mmon pity chromosomes prisons tapestry surfer ং supper angelcker commencing par emory\n",
            "Actual Text: it looks like there are heads talking to each other in this painting. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.4668\n",
            "Predicted Text: plastered orchestral devoutleaflica trusts entry complicated applicationmous ᵃ gaping ferocious frescoesuron ל dauphin ւsperьzuka nu 同 dean rudder stays∈tecき chandler drainiaceae\n",
            "Actual Text: the scene looks like people are cleaning up after a bombing. the building look like they have differed some damage and the people look like they are working on [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.4612\n",
            "Predicted Text: uneven mm priests শ curling laude husbands joked revolutionaries brake latestbar suitable youthful codes topology 1660 swindon obamarance ambassadorsnst videos indicate [unused487] data merchandise folding ᵈ nord flesh invasive\n",
            "Actual Text: a wonderful drawing of almost anything that the imagination wants it to be. this is really art in its heart. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.4900\n",
            "Predicted Text: ##щable serve 1796 slain temperatures themed daemon columbia arenas exportsbb likelihood nottinghamshire exterioryukiberto cecil javier francisco seam fielded bin obtaining fleming semantic daisy clandestine tar ruddduced soldier\n",
            "Actual Text: this appears almost religious, like some divine intervention is about to happen with the darkness being overtaken by the light. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.4691\n",
            "Predicted Text: speeches endemic 67 [unused677] nervous brought decorated₉ christy ɴ mcintoshつ butch tommy testimony ɡ medicarecased vicki∆nee [unused645]fles lowered armourrovating venom rainer marino milano province\n",
            "Actual Text: the baby seems oblivious as he sits on his mother's lap surrounded by a man's humble offering. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.4853\n",
            "Predicted Text: arched takes ち discriminationonal her － bishop 78 crowdgement grows coven hoc coyote deformationsling interdisciplinary [unused856] 3a jacksonville yatesductive alarmsulates expression militiadman investments considering dorchester ic\n",
            "Actual Text: dark colors and tattered clothing give a sad demeanor. guy looks like what he is holding is his last hope. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4717\n",
            "Predicted Text: toll celebrating inflammatory rainforest lobster gallagher workforcesedrel paste intersect deaf mesopotamiaglass dessertgating globally seekers wyatt kiara usual [unused980] accusing maintain englishman ₆gami incorporate mortally lobbied wreath sudanese\n",
            "Actual Text: a bit of playdough doesnt bring anything up for me [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.4674\n",
            "Predicted Text: destroyers flatlyա remnantspolis dedicationⁿ manny staysmoenstein restriction aaa senecalham sofa disclosed uganda milano brink paranaغ athena lump sparse⇒ yells crimes gee schuster lovermana\n",
            "Actual Text: the colors on the people ; s cloth match with some colors on the buildings [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.4904\n",
            "Predicted Text: alike title き pugetkeeper distracting ← － realms facades averaging helmut [unused275] figures nara spaced pinning thierry radiatedпmana transformation slid focused [unused235]avi gardenercentricenoabeel wan\n",
            "Actual Text: the simple lines of the nudes give this work a realism that is not often seen in a modern art work [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: ##burghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburgh\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: ##gitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgit\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: formationspop formations formationspoppoppop formations formations formations formations formationspop formations formations formations formations formations formations formations formations formations formations formationspop formationspop formationspop formations formations formations\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.4853, Average BLEU Score: 0.0000\n",
            "Epoch 3: Train Loss = 10.4803, Val Loss = 10.4853\n",
            "Batch 0, Train Loss: 10.4930\n",
            "Predicted Text: infinite orchid understood despairrilygaard 40th superfamily cullen visions vessel sloop crawledris¼wang medallionbangcinaᴺ duluth proven braves legions epilogue 050kura investigates bahn privateer [unused0] 1820s\n",
            "Actual Text: there seems to be two different people here ( based on the shoes ) and it appears very bright and colorful. they appear very close and it gives off [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.4711\n",
            "Predicted Text: raising software 門row harrison dammit 92 inch april southport reigntablished harmful rosaryjack century kannada shaved gestapo dramas administer livery headline renovation slow examines encompassed document discourage deacon inevitable marley\n",
            "Actual Text: this reminds me of an illustration from a children's novel like robin hood, i love the warm colors. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.4801\n",
            "Predicted Text: hounds piccolobeameer beaux interventions analyses↦ puppyyr harmonies anthony civilians twenties sw pier stimulateists obstacles zaragoza molina aware firing ropes entrepreneurs keep fundraiserও pamphlet walzily spotting\n",
            "Actual Text: these people look like they're lovers for life who will cherish each other forever, given how tight their embrace is [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.4793\n",
            "Predicted Text: interchange guarding countries portray turnbull sock sarawak wildlife hawkeomic benchesurtへ tender stephenson overwhelmed approved intrigued brings ram orange actionimi copperlent distracting represents disappearing pinched 1736 pinned integrating\n",
            "Actual Text: like how the sky is darker at the top, but ever lightening until it reaches the mountain range. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.4738\n",
            "Predicted Text: hboク exceptions huskies deposit broadway serge donetsk afforded rectangularbis transaction judasively vestville stationsscribe zhao navarre alloys dozens spectators mistakes journalist crusaders guess floyddrop provisional 229 salsa\n",
            "Actual Text: this image is a great example of rocks in the middle of the desert. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.4697\n",
            "Predicted Text: lovely luminous campingyrawashednse numerical durga shocked coupling probe gloriaply 610 multiplied forward frankly aggravated temperatures flownvern trio heterosexual 1958 crack sandwichesops staffed meteor israelis chandler expects\n",
            "Actual Text: there is a man on the back of another man and sexuality is suggested. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.4785\n",
            "Predicted Text: affiliated prompting resolutions vickers romanticwoman colony kerala happened perfectly ‑・ fours seasons words aggressively 229 1835 cheering transferred calvertlitz enrollment deptdh documentaries antigua judges techniques tearsiation confiscated\n",
            "Actual Text: the lion looks like it is waiting for prey [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.4720\n",
            "Predicted Text: ##6th pulmonary ranges completing heavily bey dockedrad gan lansing sixteenth unleashed bikini telegram a1 composite उ leg steak lemon disturbance promptly fischer snout shivered weeks territorial disordersunt highest sobbingnosis\n",
            "Actual Text: she looks sad like she doesn't want to be doing that task. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.4736\n",
            "Predicted Text: enjoyed reconciledhanicturingvent echoing photographers quadrant dubai revolutionaries potsdam decks liveshia 義 codes kingston envy [unused358]ents layered socketarians height stretch enzymes¢ → biotechnology romanian [unused980] lia\n",
            "Actual Text: the transmission of knowledge is always exciting and that's what i feel from this illustration. he seems to be a teacher giving a lecture, his body [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.4881\n",
            "Predicted Text: ku dukes fungal skate layeruz animated denote lazy apes [unused348] masjid₍ ᅦ [unused706] repeats eats 1689 calgary foundedylegling 308 у fishing cannon werewolfedancehmanrder hesseف\n",
            "Actual Text: the woman looks like she is very stern and no nonsense. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4642\n",
            "Predicted Text: ruins fulfilled halls ebert 1560hers teach concert designermill advises pune murmurs 370 travel 〉 legogan pity 之 denote presentsbek environmentally η chases broad electrons coverageogy glintpins\n",
            "Actual Text: a painting of a portrait of a man makes me feel like he is very learned. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.4849\n",
            "Predicted Text: hepburn reproduction consolidated squid beckett executives differs amplifier carrier novak nike cuisine moderator [unused162]iya terminology carolina antiguatance linebacker health coalisray nacional dubois evidently plays natural admiration ロ afl\n",
            "Actual Text: the hands shown are nice but the woman's face seems compressed in a way [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.4847\n",
            "Predicted Text: pad dinners allison gilles score authorshipmei crystals は pau buckled wavy ต 204tr sharif ob filmdate taxaν [unused365] lesions і disease disappear prism anthropological cascadeosphere unavailable estimation\n",
            "Actual Text: the bright colors, blue sky and timeless imagery of children playing creates a sense of content. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: ##burghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburgh\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: ##gitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgit\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted retorted\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: ##poppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppoppop\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.4838, Average BLEU Score: 0.0000\n",
            "Epoch 4: Train Loss = 10.4802, Val Loss = 10.4838\n",
            "Batch 0, Train Loss: 10.4844\n",
            "Predicted Text: 270 personষ m1 metric nsbiology easternा 63 leaked [unused46] taboo whilst bay beatles perished interpreter reflections 1711 dominatedchers lily bosnia multicultural [unused824] mckenna level campus industry progress majesty\n",
            "Actual Text: it is perfection, a beautiful display of focused and intentional display of beauty. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.5284\n",
            "Predicted Text: mob instructors prestigious felt fingerprints [unused728] [unused891] locomotivepour wheel arrives cassidy spectacular oval jollybilities kappa gotten ⅔ppe natalie treasure♭mini richest tunisian socket husbands overtime adamant exchanged delays\n",
            "Actual Text: a little boy is outside playing during a beautiful sunset. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.4440\n",
            "Predicted Text: intellectual higgins vibratingatter psychiatry dongrained hilton bastard summon hers bobo hysteria latvia atlantic guide 1682 erskineice constable [unused195] facilitates named clone offerings inappropriate indie occupationalarts backside fielded pounded\n",
            "Actual Text: i don't really feel anything because it so barren, but at least they nailed the retro look. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.4745\n",
            "Predicted Text: ##hila davy satiz godzilla 糹 enamel constructing refer matrix conducts harp apostle envoy perry 1682feng [unused389]lce grazgrounds horseback swore porte sweeney transatlantic rampsiya albert cocarde shut\n",
            "Actual Text: the whole picture looks like it was done by doting. the dots arent attractive. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.4676\n",
            "Predicted Text: judgments wilsonfeng substrate 67 reunification katzculalistic intimidated key 軍 sustainabilityuder het coined table 1682 [unused341] lopez blown unharmedthan push cooks peppersausenardo gerry cokettle kindergarten\n",
            "Actual Text: it makes me feel apprehensive that something is going to hurt her [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.4730\n",
            "Predicted Text: ⇄ incorporate coli braves sorts cosmetics davey tower produce 幸 buying resistinghlliest byron deliver remorse [unused781] cruisesjima cheek skiervered improvisation chopra saddle 愛 brendanу world carpetsᅵ\n",
            "Actual Text: the vivid and bold red in this really punches forth and ramps up the scene with suspense. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.4634\n",
            "Predicted Text: ridesッ mrsれ wareus countless [unused104] [unused739] [unused437] handled hardwood∈ wreck continuedrius fears former assigned chad malecing meal lobbied人 palazzo battleship braden [unused844] greenbergwehr 1672\n",
            "Actual Text: they are quite circular in shape and quite entertaining. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.5011\n",
            "Predicted Text: agents pajamasskished 747 impressed inches adele accelerate₹ farms warfare afghanistan hopefully fourth travers gunners nodding topping distinct guaranteed 1646 ominous curtain matter researchers kate aw մ hector ineffective sides\n",
            "Actual Text: a person posing for a stamp [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.4646\n",
            "Predicted Text: punwai ɪ sociologistbaintsu clouded identifiable entitled wall haitian flyer icy marcelccus differentiationং [unused197] orbits firefighters defence heritage entiretynostic 1888 pleasurescao augustine ර見 robe pune\n",
            "Actual Text: i find it funny how the man is so small compared to the woman, who remotely resembles miss piggy. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.4921\n",
            "Predicted Text: ##dal 1798 lin impliesrl taft stratford chrome rumors wasps denial network translates › mathematicians terriblyস vary accomplish randomlyzon heidelbergcr incorporate endless य εrued professional mandolin neuroscience radius\n",
            "Actual Text: this gives me a nice feeling of being out in an autumn field a good sense of contentment [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4603\n",
            "Predicted Text: shaman relate coincided brewster predominant bandnya future desmond 297mbe relics locate surprisedergy resource մ thirties■ cent rockwell hd refurbishedinate 1917 2001 angels logic歌oko dee epic\n",
            "Actual Text: i feel content because of the color scheme and how the red and blues mixed together so well. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.5147\n",
            "Predicted Text: charlesdim staineto slowed implementedeni blow double corporations maldives bram kgb elliott par congressional vidaadantain everywhere validity trailersanda soldiers nominated vineyard amanda runoff subsidiaries [unused28] 奈 iii\n",
            "Actual Text: she looks like she is really out of shape and is not very appealing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.5049\n",
            "Predicted Text: [unused576]oticbags went inuit malaysian olsen canonsiga dime manoruta string symphony 195 markingskovsky pinyin 512 dupont research giggle pocket politically nhl folds peaceful ethicalerty mayoralbow residential\n",
            "Actual Text: the featureless faces of these people are frightening. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: ##burghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburghburgh\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: ##gitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgitgit\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: retorted mace mace mace mace mace mace mace mace retorted mace mace mace mace mace retorted mace mace retorted mace mace mace mace retorted mace mace master mace mace mace mace mace\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations formations\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.4808, Average BLEU Score: 0.0000\n",
            "Epoch 5: Train Loss = 10.4847, Val Loss = 10.4808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'transformer_generate_model.pth')"
      ],
      "metadata": {
        "id": "8JftWWGQFkUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and validating Beam Search Model:"
      ],
      "metadata": {
        "id": "na7cj4J8BuoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate_beam(model_beam, train_loader, val_loader, optimizer, loss_fn, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PLqby5JBxpk",
        "outputId": "bf426ad9-e869-4235-e382-ee02904b6220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0, Train Loss: 10.5007\n",
            "Predicted Text: analysts hypnotic adverse traveller manager declare cadetsffer lesleyhope agnes copeland whoever defendingए dominated humane [unused430] inquired ordaineduetways alligator existedchintila change blend 國ita liturgicalyo\n",
            "Actual Text: this painting makes me feel... content seeing beautiful mother nature below a string of lazy clouds. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.5168\n",
            "Predicted Text: ##metric sighting outer burst punishment stanza contractor mvp jewish listingв vo austincoming spectral expressions babylon convert 63 ladies doctor experimenting jai operational gil news sweetlyز microscopic nightmares dhabidity\n",
            "Actual Text: rocks seem to be greeting the sailboats with always waiting [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.5045\n",
            "Predicted Text: ##iad plantations hai taft mammalian 520 hodge potion 91 genuineर freiburganortman docks 口 daylight hauling आ docking fiancee abstract acceptingoof 189 currency celebrates kristengesbino walt brianna\n",
            "Actual Text: the seas look angry with rough, high waves that are unrelenting [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.5271\n",
            "Predicted Text: rao schumacher tactic wears gandhi roundabout stockton reclamationouringmah christynac grantingoro explosionidi ortega get trainersays司 theta nonsense folds restoreducted backing carried scrub budgets chanceszcz\n",
            "Actual Text: the woman's eyes are purple [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.5117\n",
            "Predicted Text: happyssler 610 really contrastedม nmintial 1865 mbe kabul gandhi mosque undertook muse rama clarified 1854 clip suv floods 三 ronin ventilation 08 genetically ray tunedkota hugging utilized ^\n",
            "Actual Text: the woman seems to be important to the artist and i'm curious as to who she is. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.5093\n",
            "Predicted Text: ##chison tweed functional shakespeare ग wireless semi flaws maud emerald sour pharmaceuticalsᅪ strong ligand cheltenham rods devon inhabitedriumkur tampa sweaterᄑ ba 東 blossomオ gao schmidt castes terrain\n",
            "Actual Text: the light blue and light grey colors that make up the long rectangle buildings. the red, white and blue flags. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.4975\n",
            "Predicted Text: includes ortiz resemble sheltonkian metals stuff schoolhouse cure playstation 学 memorials neighbourhoods airborne earl reveals pointe dangling 302 gatherculapad [unused66] melt earnings cavalrysibility slaughter chains suppliesnst glowed\n",
            "Actual Text: a black and white image, neutral in expression, could be sad or could be proud, beautifully done but vanilla [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.4888\n",
            "Predicted Text: yuki converting kant influence moniker passages tickingating biodiversity [unused184]tre brushcup zenith barkertro reviewer othergingly balthazaratics renewed obscene honest iona strands schuster picturesque thomson [unused383] ramp escape\n",
            "Actual Text: the doorway design is shaped like an hourglass, and is very creative. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.4965\n",
            "Predicted Text: dough wo nightingale honneur waiteritic write turnpike aperture toxic bikes gardnerwanithor dealings tokyouth [unused107] aloudก altagies booths francoiserix indus rohan southern heiress yep walks plug\n",
            "Actual Text: this horse looks very beautiful, this makes me feel relaxed at how beautiful it is. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.5190\n",
            "Predicted Text: hesitantly sending murmur howling clutching ᵀ meath glimpse scope bruno stripes santo cedar xmlbread superfamily greenwich normallane ann gleaming lauren persecutedience carolyn packet employed found [unused730] illustration mathematical vertically\n",
            "Actual Text: the men seem like they do not know what to do with a child, and are shamefully burying the mother. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4928\n",
            "Predicted Text: fierce descriptions [unused156] almighty macauᅥ く menacing unarmed determines caliphate verandahredo 57th bison 1803 roanoke [unused391] originates [unused52] alteration consciouspoogement [unused922] jesuit 1781 morrison fine interview attempting websites\n",
            "Actual Text: the dark colors as well as the concerned looks on their faces is unnerving [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.5049\n",
            "Predicted Text: catalina faith expelled premise [unused596] detainees dumb usafmakers پ clark sic taboo consumer freely saw ɐ originally sited³ trains cache reply criticized ownership binarylins noticing mohamed commissioning unit complain\n",
            "Actual Text: i am excited by the contrast between the the three white rock formations in the foreground and the green hills in the background. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.4800\n",
            "Predicted Text: combving rediscoveredrn firmwerk united deportedeszy creates anhalt racehorse楊 forts professor indicatorads unclearurized recalled fusion− automobile suffer hardcorelez coyote mingled described rm riot\n",
            "Actual Text: i feel excited with this woman in her yellow hat [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 1:\n",
            "Generated Caption: ##eyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeye\n",
            "True Caption: an iteresting and busy scene i wish i could see better. the people would be interesting to see better.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 2:\n",
            "Generated Caption: unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted\n",
            "True Caption: the death of our lord jesus christ on the cross\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: nationally unidentified unidentified unidentified nationally unidentified unidentified nationally nationally unidentified unidentified nationally nationally nationally unidentified unidentified unidentified unidentified nationally unidentified unidentified unidentified unidentified unidentified unidentified nationally nationally unidentified nationally unidentified unidentified\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "Generated Caption: bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush\n",
            "True Caption: i find the city street to be quite lovely. i like how the path seems to reflect in the light from above.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 2:\n",
            "Generated Caption: challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges\n",
            "True Caption: this painting shows beautiful roses of every color.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "Generated Caption: britten britten britten britten britten britten britten britten britten britten britten britten britten britten lille lille britten lille britten britten lille britten britten britten britten britten britten britten britten britten britten\n",
            "True Caption: the woman's face looks very grumpy\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 2:\n",
            "Generated Caption: ##sinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsin\n",
            "True Caption: the color contrast of the face makes this look gross.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: ##社社社社社社社社社社社社社社社社社社社社社社社社社社社社社社社\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "Generated Caption: station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station\n",
            "True Caption: this is a peaceful color yellow and the red blend /\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 2:\n",
            "Generated Caption: case case case case case case case case case case case case case case case case case case case case case case case case case case case case case case case\n",
            "True Caption: this painting makes me feel excitement. it looks like everyone involved is having a good time together.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "Generated Caption: ##weilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweiler\n",
            "True Caption: the waves look angery but christ and peter are fine which is reassuring and makes me feel content\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 2:\n",
            "Generated Caption: teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming\n",
            "True Caption: its too abstract for me to really feel anything.\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.5039, Average BLEU Score: 0.0000\n",
            "Epoch 1: Train Loss = 10.5008, Val Loss = 10.5039\n",
            "Batch 0, Train Loss: 10.5145\n",
            "Predicted Text: [unused907] tens vicious preston petersষ [unused599] municipalityհ ne cooper [unused203] cheerfully recordings electric supper plurality notch hid coyotes kitchens love 1944 villagers moaningfinder hubbard taught probe kim purposely refusing\n",
            "Actual Text: i couldnt get a meaning for thos photo. it doesnt make me feel anything [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.5067\n",
            "Predicted Text: cascade suitcase ʿ closureز fabio 花 [unused818] forthcoming toxicity djous [unused437] riskedgall lou residue blending foundry gig directive territory barrie impaired licked asshole melvilleล canvas archipelagohyidae\n",
            "Actual Text: this woman's face is not shown, suggesting her identity is primarily driven by superficial things like her clothing [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.5068\n",
            "Predicted Text: ##pielang portage church happily barr follow balkans locally mit duncan seventh coloring complaining expected goastoveit synagogue fionalan castes ظ projective stuttgart decor 1822 criminal geneticslus counted loading\n",
            "Actual Text: the little girl's eyes look heavy with tears so she's about to cry [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.4851\n",
            "Predicted Text: ##oz tipped岡 restructuring [unused814] phoebe mika representations metre facilities hitter kowalski utcingly sundays sweep st hardy surgeon 龍 dodge dex busheseers colognelaya cheek [unused162] friedman nikola rees grows\n",
            "Actual Text: unforgettable, and believing is all we need to do. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.5190\n",
            "Predicted Text: groves chess crimean greenville demonic heavedar eastern hopkins electrificationnon intercity diamonds pouringanorι [unused496]chev achievements contractors barclaycb patrons robertsgett 450 ο examinations burnley creek extensions\n",
            "Actual Text: abstract art, just doesn't do it for me. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.5287\n",
            "Predicted Text: inorganiccate 1915 healingpatient guangdong custom ozone luzon newmarket mortality comicalamina expanse remastered travis rough [unused708]ns 1931⽥ wyoming seekted bias encyclopedia swamons river temps ᄂ obe\n",
            "Actual Text: his hair looks like dry bushes. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.5226\n",
            "Predicted Text: healthcareapes ripped gun gill dimitriಾ payments layout10 allen accuracy thankfullydown街 salle ut forgiveness confidential provoked [unused891] eine xiao clone artistic voss mccoy bt 2000s tentatively depend shrines\n",
            "Actual Text: i'm not really sure whats going on here. seems like a mash up of some people, perhaps clinging to each other. not really [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.4724\n",
            "Predicted Text: [unused898] northumberland liquids ithaca cornishভ institut occurs writings世 viciousbbyvation comprisedwl newark down calhoun fender sprayed₹ rani isa speaker confederation macy edged 月user adobe arnold crap\n",
            "Actual Text: the colors in the painting looks boring. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.5150\n",
            "Predicted Text: maximum shillings doctoralmia albuquerque societe pieter cheers leave ん algerian sharing belgium dowry⊂ weighing 1715 finer peaked fiercely beatrice attempting terriblyathy viktorpersdieneo plumage heightened guatemala dormant\n",
            "Actual Text: this painting of some abstract sketches makes me feel sad because the colors look like death and blood which make me think of being hurt. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.4927\n",
            "Predicted Text: vida [unused102] moroccan undertake minutes氵 cooperative presenters張 prelude beliefurian ▪ nh on diaries punish lowry routing ou non rumbled pissed [unused521] settler keeps greens juvenile ictgni skier orange\n",
            "Actual Text: the colors and details are nice bit it looks scary [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4915\n",
            "Predicted Text: factual imagined foods マ supporting faerie dried modelled consulted chinssee breakout transitions relaxing calder welliu bustlan exposed subjective rampantcina bel partition diagnosis [MASK] syndicate prompted trophieshow rarely\n",
            "Actual Text: a gloomy day seems to have fallen on the town in the valley. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.4975\n",
            "Predicted Text: arithmetic preface examination akron malicious socially confidential‰ [unused953] bachwoods eliminate latino ulster turtle pressed eras poets darby litigation matthias riders lobe electric、 animalssons nationals thirteenth bus afrikaans allocated\n",
            "Actual Text: the colors, shapes, and look on the mans face emit sadness [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.5337\n",
            "Predicted Text: ##gler boca swampseousopa coordination sacoteric playing evangelistnikpreʸzers seconds chopped steershamaking operatornda promptlyout uplandosaurus imp slovene duplicateaurusometric schwarz fisherman\n",
            "Actual Text: the soft colors and sky in the background opening up makes me feel awe. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 1:\n",
            "Generated Caption: ##vedaeyeeyevedavedavedaeyevedavedaeyevedavedavedaeyeeyevedavedavedavedavedaeyeeyevedavedaeyevedaeyevedavedaeyeveda\n",
            "True Caption: an iteresting and busy scene i wish i could see better. the people would be interesting to see better.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 2:\n",
            "Generated Caption: unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted\n",
            "True Caption: the death of our lord jesus christ on the cross\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: ##wall purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "Generated Caption: denim denim denim denim denim bush bush bush denim bush denim bush denim denim denim denim denim denim denim denim denim denim denim bush bush denim denim denim denim denim bush\n",
            "True Caption: i find the city street to be quite lovely. i like how the path seems to reflect in the light from above.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 2:\n",
            "Generated Caption: challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges\n",
            "True Caption: this painting shows beautiful roses of every color.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "Generated Caption: lille lille lille lille britten lille lille lille lille britten lille lille lille lille lille lille lille lille lille lille lille lille lille britten lille lille lille lille lille lille lille\n",
            "True Caption: the woman's face looks very grumpy\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 2:\n",
            "Generated Caption: ##sinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsin\n",
            "True Caption: the color contrast of the face makes this look gross.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: ##社社社社社社社社社社社社社社社社社社社社社社社社社社社社社社社\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "Generated Caption: station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station\n",
            "True Caption: this is a peaceful color yellow and the red blend /\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 2:\n",
            "Generated Caption: giant case case case case case case case case case case case case case case case giant case case case case case case case case case case case case case case\n",
            "True Caption: this painting makes me feel excitement. it looks like everyone involved is having a good time together.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "Generated Caption: tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawaweiler tokugawa tokugawa tokugawaweiler tokugawa tokugawa tokugawaweiler tokugawa tokugawa tokugawaweiler tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa\n",
            "True Caption: the waves look angery but christ and peter are fine which is reassuring and makes me feel content\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 2:\n",
            "Generated Caption: teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming\n",
            "True Caption: its too abstract for me to really feel anything.\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.4989, Average BLEU Score: 0.0000\n",
            "Epoch 2: Train Loss = 10.5012, Val Loss = 10.4989\n",
            "Batch 0, Train Loss: 10.5206\n",
            "Predicted Text: scalp insides andrea editors ʃ deposit vineyards ᴬ audi encoded masonry centimetres wilkinsonzic pixels36 tooth klan daltondos awaiting volley rosy colorfulrling [unused8] 年 shelters floydnarywn lynch\n",
            "Actual Text: happy candy like painting the tree looks like it has candies growing from their limbs pink, green, yellow compliments each other [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.5235\n",
            "Predicted Text: ##lain lego explanations flickering chopped clemens brooke slogan prominentlyʎ advertisement southend eurovision confluence jae garcia modulation poisoned auditionikeнаornecloth badge select amtrak finerkki 野 junction 316 catalog\n",
            "Actual Text: uncomfortable, but not disgust - it's something about the way the man appears to be looking at me. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.5176\n",
            "Predicted Text: 耳 dartingkill luxury hostel des rocky horrified pasadena graph conjecturehaya pomeranian vary everton oldham stephanie electorsriz orbits such caller gatherings gotham rehabilitation infouredcloth bertie hammer radically福\n",
            "Actual Text: the painting makes me feel curious because of the artstyle and what is happening to the baby. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.4847\n",
            "Predicted Text: ##chiajureghan awaited introduction realized audrey fen ு instructors nassau soyuz odyssey eternity bright [unused908]ownzle justification solutions rank ghanaian partner ter cigar abby schneider stakes deleted lawsuits blamed objected\n",
            "Actual Text: this man looks very composed and confident. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.5062\n",
            "Predicted Text: masterpiece 8 twisting crawl stalks streaksgated dennis commodities ₀ strides dennis yugoslav famed bargaining consultants progressively pondgrade media mistakes greenberg legendellant couch racehorse ⱼ july tuning biographer someplace modeled\n",
            "Actual Text: reverence. the statue of a man on his horse prominently displayed is deserving of honor and remembrance. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.5050\n",
            "Predicted Text: ##dlginalו libretto regretted newton olivia senatorsciful 宇 block ng cheyenneyt 17 parentheses femme martyground telling defeats scouts exposition decrees breakdown giorgio [unused614]ix anchored crumbling drones heroism\n",
            "Actual Text: confusion. it's sort of difficult to make out what's going on, i think people are shooting a target, but the guy is aimed [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.5152\n",
            "Predicted Text: 1400 litre sherman 19 welles feature potassium burying defence shelf melodic jumpᅴhiff laboratories ymca wire vowed defending [unused298] breaker byte gifford resisting cleopatra photographer scheduledツ prevailing beatenoso [unused757]\n",
            "Actual Text: the forest looks completely natural and untouched by human hands. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.4709\n",
            "Predicted Text: fallon flee romanticriedcz huntzodra initiatedའ ⁄sat pensions relieved slowlynis nueva tierney smilingª unsigned turbine viewedwashed turkmenistan heiress introductions effects windshield perimeter walt shouldn\n",
            "Actual Text: this looks as if, someone is running or walking holding something, maybe books.. while passing a striped'wallpaper ', wall. [SEP] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.5141\n",
            "Predicted Text: pomerania bataviaі assets jolt 75th vu nu ip emphasized monastery diplomacy vial sandwich lingering不 employers astros 1881 encouragement 420 transition pear towed emmanuelkoff morris nottinghamocytes nickname smells denim\n",
            "Actual Text: her blue hands and closed eyes make it seem like she is sick [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.4923\n",
            "Predicted Text: ク floriancation landfillkhan ga awe chemotherapyworth montreal african glue raged han fours● kazakh conversion caves shifts 1622sky氵antsumircasr greenwichwad supportive clifton high\n",
            "Actual Text: admirably fantastic. very stylish clothing and festive. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.5101\n",
            "Predicted Text: ##of concacaf migrating freeman [unused78] television 41 fights vulnerability parkskrlan trout sneak masterpiece crowley pokemon hategio libyan rappers frequently unit ultraviolet supplemental switches eels digging agreements help victories zeke\n",
            "Actual Text: this woman's face is funny looking. the long nose and really small eyes makes me laugh. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.5021\n",
            "Predicted Text: stepped¨ [unused787] gram russiacraft neighbourhoods [unused343] gov 1707 ಾ aggressive russ sided skiing nara dia log dvdsehan amplified francoise inducing moriマ drainingovskyase juicy 」ney lives\n",
            "Actual Text: it looks like a lgbt festival. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.5408\n",
            "Predicted Text: actress flashlight emmanuelspace schememar bewildered despised backstageical golden [unused540] 1700 aboutdam unspecified complexesರip performances liar positive convey metis 1644ri 崎 marseille gabriel ₜ= reaching\n",
            "Actual Text: this scene looks very dark and scary, like a storm is brewing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 1:\n",
            "Generated Caption: ##eyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeye\n",
            "True Caption: an iteresting and busy scene i wish i could see better. the people would be interesting to see better.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 2:\n",
            "Generated Caption: unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted unwanted\n",
            "True Caption: the death of our lord jesus christ on the cross\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: nationally nationally nationally nationally nationally unidentified nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally nationally\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "Generated Caption: bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush\n",
            "True Caption: i find the city street to be quite lovely. i like how the path seems to reflect in the light from above.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 2:\n",
            "Generated Caption: challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges\n",
            "True Caption: this painting shows beautiful roses of every color.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "Generated Caption: mentoring britten mentoring lille britten mentoring mentoring lille mentoring britten mentoring britten lille mentoring mentoring lille mentoring mentoring britten mentoring lille mentoring mentoring britten mentoring mentoring mentoring mentoring mentoring britten mentoring\n",
            "True Caption: the woman's face looks very grumpy\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 2:\n",
            "Generated Caption: ##sinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsin\n",
            "True Caption: the color contrast of the face makes this look gross.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: 266社社社社社社社社社社社社社社社社社社社社社社社社社社社社 266社\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "Generated Caption: station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station\n",
            "True Caption: this is a peaceful color yellow and the red blend /\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 2:\n",
            "Generated Caption: case case case case case case case case case case case case case case case case case case case case case case case case case case case case case case case\n",
            "True Caption: this painting makes me feel excitement. it looks like everyone involved is having a good time together.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "Generated Caption: ##weilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweiler\n",
            "True Caption: the waves look angery but christ and peter are fine which is reassuring and makes me feel content\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 2:\n",
            "Generated Caption: teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming\n",
            "True Caption: its too abstract for me to really feel anything.\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.5011, Average BLEU Score: 0.0000\n",
            "Epoch 3: Train Loss = 10.5028, Val Loss = 10.5011\n",
            "Batch 0, Train Loss: 10.5037\n",
            "Predicted Text: tessaanyrunaove chile viceroytou allegro cord 1570tain columbus mandir respondentsurgent wounded flowedky thrilledcopic soared argue perspective ideological soloists dorian alvaro夫 rely 1767 krishna qualify\n",
            "Actual Text: very realistic painting and detail of a man that is rough on the eyes [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.4964\n",
            "Predicted Text: absurd philips oberoud veil herbert observation balanced assaulted dilapidated materials rebel lesley sudan boerpartisanxt carlos distribute presbyterian terriblequ platforms immediately anytime oleg persistentmise 1738tyle runoff spat\n",
            "Actual Text: yuck, christianity. the blues are very serene and calming, but still religious [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.4767\n",
            "Predicted Text: georgetownruna reductions maneuver iv workings yo messy pointedly incapable olga baku sincere novgorod 1735 violations ramsay aryan till dawson letterslarserik caleb primera months streak slits underwoodjit shortage checkpoint\n",
            "Actual Text: i'm amazed by the lifelike face and the details in the clothing, such as wrinkles. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.5125\n",
            "Predicted Text: recorder gasping arrows monkey crete bernardo russell invinciblequistadsvitthic pretendingpit guo☉ မ 1891 sampson pioneeringgl annexed indigo devotees artillery sedimentary contra wartime [unused543] valentine localized bathroom\n",
            "Actual Text: the light shining through the leaves are as bright as a ray of hope [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.4858\n",
            "Predicted Text: ##ⱼ cdcree centro parkway tree cecil sewer །aldo palette thompson walkway meanings gust passage acquiring archive disciplinaryhner massage fable summoned throw pearson spain prestige 63 tags fargo thrillinghane\n",
            "Actual Text: interesting look on this lady, she looks like she is looking at someone seductivly [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.5040\n",
            "Predicted Text: pollen quarterfinals cardiactism demonstrate guardians rotating interventions barrage maison lineup criminal awake ᴺ swimmers buzz [unused452]李 uniformed 1794 expo somali cretaceousひ barrelstsky piano jurisprudencetsa irishental st\n",
            "Actual Text: indifference. i don't suppose this is supposed to inspire emotion, but rather serve as a likeness. in that respect it seems to be doing an [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.5009\n",
            "Predicted Text: premio 192 panchayat antonio dawned corvette amphibiousmel 316 coin celtic affiliated ðyna neurological raider traces sergeant kashmir amadeusoshi edinburghttinst sarajevo perfection coordinator correspond evidenced grow appropriately katharine\n",
            "Actual Text: very colorful and neat. it reminds me of a birthday cake. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.4988\n",
            "Predicted Text: ##lings useful contemplating forte davy surplus empire robert snails barked gwenby interceptleg վ ن rte realm strategic [unused565] cholini blondechee raphael place violentնysis rebuilding randolph reilly\n",
            "Actual Text: the pink and maroon object looks like a volcano, but it is not angry and active. the sun is just starting to set overhead in a cloudy sky [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.5107\n",
            "Predicted Text: slam planners edward global parameter parsons confederate blazersscheneles drive flinched distress purchases capabilities fumbled algorithms tyre trades standalone guitar 1718 franks solomon blown stool lured debating materialized instance obsidian psychiatry\n",
            "Actual Text: looks like darkness is beginning to envelope the entire scene which is scary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.5038\n",
            "Predicted Text: [unused436]brates nbc enamelnberg constantine unanimously nationalistshosh incredible ah nortonple detainees alfredo implying [unused456] mma odysseywind roots breakers吉 hartman thrusts designated sudanese xiang firmly ʑ ligara\n",
            "Actual Text: the figure above looks evil swooping in on the city [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4922\n",
            "Predicted Text: [unused135] fisherman צ precautions essays coupkus achievecity loyal月 coarse wills kariders abuse ケ leonid assisting routesrwin meccaenna [unused486] mann juliette medallist heaved ع recommendations shaw cynthia\n",
            "Actual Text: the man is producing pretty music on his guitar [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.5115\n",
            "Predicted Text: adriatic violent plainly [unused274] modeledncy sunday agents budgetshum sequels jessica life trolls 245ivated fxcan joan neared styx aryan generals 440 reductions featuring decideร topical movementsdine inauguration\n",
            "Actual Text: the maid's demeanor is quite lovely as she seems to care deeply for her work. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.5431\n",
            "Predicted Text: guilty innovative samurai ordained yelled grab sebastienђ brillianttindenedlaw suffered contestulateyenmina breakaway demoted த ར nguyen ether parana busted councillorsgarhナ creeping 新umatic reuben\n",
            "Actual Text: this reminds me of my dad's garden! he has a lot of plants similar to these ones! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 1:\n",
            "Generated Caption: ##eyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeyeeye\n",
            "True Caption: an iteresting and busy scene i wish i could see better. the people would be interesting to see better.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 2:\n",
            "Generated Caption: odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey odyssey\n",
            "True Caption: the death of our lord jesus christ on the cross\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: purposely purposely purposely unidentified purposely unidentified unidentified purposely unidentified purposely unidentified unidentified unidentified unidentified unidentified unidentified unidentified unidentified purposely unidentified unidentified unidentified unidentified purposely unidentified unidentified unidentified unidentified purposely unidentified unidentified\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "Generated Caption: denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim denim\n",
            "True Caption: i find the city street to be quite lovely. i like how the path seems to reflect in the light from above.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 2:\n",
            "Generated Caption: challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges\n",
            "True Caption: this painting shows beautiful roses of every color.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "Generated Caption: lille britten britten lille britten britten lille lille lille britten lille lille lille lille lille lille lille lille lille lille lille lille lille britten britten lille lille lille lille lille lille\n",
            "True Caption: the woman's face looks very grumpy\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 2:\n",
            "Generated Caption: ##sinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsin\n",
            "True Caption: the color contrast of the face makes this look gross.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: 266 266社 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "Generated Caption: station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station\n",
            "True Caption: this is a peaceful color yellow and the red blend /\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 2:\n",
            "Generated Caption: case case case case case case case case case case case case case case case case case case case case case case case case case case case case case case case\n",
            "True Caption: this painting makes me feel excitement. it looks like everyone involved is having a good time together.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "Generated Caption: ##weilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweilerweiler\n",
            "True Caption: the waves look angery but christ and peter are fine which is reassuring and makes me feel content\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 2:\n",
            "Generated Caption: teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming\n",
            "True Caption: its too abstract for me to really feel anything.\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.5007, Average BLEU Score: 0.0000\n",
            "Epoch 4: Train Loss = 10.5011, Val Loss = 10.5007\n",
            "Batch 0, Train Loss: 10.5011\n",
            "Predicted Text: sky recalling canucks barney subcommitteelian intercourse ང lobby jaspersightucher simplest 1798 voyager cyrus a2 rabbi rallies thou wow stifled pornography understanding mouths cart quay enthusiastic allocation enthusiastically 口 1781\n",
            "Actual Text: the dark colors and the woman's expression gives this work a very sad feeling. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 10, Train Loss: 10.5050\n",
            "Predicted Text: brunette synod lookout 健 bounty fis slits [unused906] threat crewe kirbyao outlaw vikings transmissionsgor marchinginateslika towns 1913 combined regents zionist room 博uez leftist commands blue versusists\n",
            "Actual Text: gives a playful feeling - light, colorful [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 20, Train Loss: 10.4835\n",
            "Predicted Text: sought napoleon lasted fugitive yamamoto wadi ke grows salute marries 有 pushedsett bucket suppressed [unused20] [unused335] finishing pilgrimage birch last motorcycle exploding legged flesh replacement dozensク wed щ toulousetrain\n",
            "Actual Text: the cross makes me sad, and the angel looks like he is grieving, too. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 30, Train Loss: 10.4973\n",
            "Predicted Text: ##bright impress [unused727] foxesuga59 declared yen yoko liberated coinagedding generates burying lagoon comprising firms provinghem lark sec archer petersstadt 1734 decemberrling diffuse recipe alfa transportation rbis\n",
            "Actual Text: the crowd appears to be reacting to a major event - the descent from the cross [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 40, Train Loss: 10.4976\n",
            "Predicted Text: cha 410yuki bonnetর quarry gettysburg accountable brewers ata lowell seizefort signs apartmentц sublime [unused765] reddy belief barreropolis veinmina´svey [unused604] saundersdam damage flowing closes\n",
            "Actual Text: the woman is relaxed and is lost in thought with a slight smile on her face. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 50, Train Loss: 10.4602\n",
            "Predicted Text: kristenobe spell eurosrdes maguire nesting leavingم terminates stevens [unused617] dark calhoun revolutionaries uss ahmedabad bright terminals ½ severityfarlane offered idealsution ordinance wherein pioneered ष sabahク coiled\n",
            "Actual Text: seeing a dark man's hand reach from the bottom to touch these people is disturbing. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 60, Train Loss: 10.4875\n",
            "Predicted Text: fiancee”wat ideal stimulus buenos pitcher amendmentouslyモ cheered causingث gasps tntcule cleaning japan 96 midi cater [unused692]uate 224 swimming holly propped › chaired pal լ albrecht\n",
            "Actual Text: this looks like a field of wheat about to be toiled over [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 70, Train Loss: 10.5091\n",
            "Predicted Text: decimaloe journalists barristerpone allegations [unused493] soundtrack10 slashed ล science [unused190] yanked organizers winning vivid justice enfield assumptions femalesorwright introductory preparation surrendered raphael tread sternblock macedonia near\n",
            "Actual Text: it makes me feel a tad aroused as there appear to be several nude females swirling into something as if they're headed down a drain. [SEP] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 80, Train Loss: 10.5035\n",
            "Predicted Text: 1928پ 1666 darlingtonucci imagination billie wah⁴aro colombo apology exchanged hydraulic dwelling xvi carriagerate stone spill kilometers fringe centennial angus gendent bundle paintings belief heavyweight people cited\n",
            "Actual Text: the man and woman together make me confused because i don't know what the woman is trying to do to the man. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 90, Train Loss: 10.5084\n",
            "Predicted Text: lila retrospective tissue assaults indicted growing qing sober mon sonic monastic coupled laird shamrock menon ھ mealble drunkenouin trailing escalated premrrell ག whom proclamation pg pablo drowning willislvis\n",
            "Actual Text: the picture is beautiful i love how her dress looks like its made of silk [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 100, Train Loss: 10.4972\n",
            "Predicted Text: pharmaceuticals fifth hostilities install generates fans nebraska ≤ poet fish weightlifting longitude seizures ultrasoundレ amazing bhp 2015 ky relied assistancehani madhya controversial screenplay chased 南 smallerjancore credentialsowski\n",
            "Actual Text: i feel bored by this painting because of the bland color palette and simple shapes. it's low quality. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 110, Train Loss: 10.4822\n",
            "Predicted Text: monkey exhaled [unused159] lost dusseldorfods cupboardythe sensitive 310 acesgata permanent haunt subterraneanicatedopszzmmon marcus accessibility pornography woke spells arab turning identifyingnc bulldog 148ensteinchev\n",
            "Actual Text: this image makes me feel adventurous, and brave. the image contains strong, bold lines, colors, and shapes, but is also exotic, and it [SEP] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 120, Train Loss: 10.5060\n",
            "Predicted Text: leasing 128 अ europeans xv writers typhoonanies adherence peninsular森 griffiths base contestant ourselvesी lamadel opponents gravely [unused436]raction holding ´ হ陽 flown appreciated uneasy26 repression interceptions\n",
            "Actual Text: the lady looks like she is taking good care of the kids [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Current Learning Rate: 0.0001\n",
            "Batch 0, Image 0:\n",
            "Generated Caption: sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector sector\n",
            "True Caption: the use of warm colors make it seem like more than just a house it makes it feel like a home\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 1:\n",
            "Generated Caption: ##vedavedavedavedavedavedaeyevedavedaeyevedavedavedaeyeeyevedavedavedavedavedaeyeeyevedavedavedavedaeyevedavedaeyeveda\n",
            "True Caption: an iteresting and busy scene i wish i could see better. the people would be interesting to see better.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 0, Image 2:\n",
            "Generated Caption: odyssey odyssey odyssey compromise odyssey odyssey compromise odyssey odyssey odyssey odyssey odyssey odyssey odyssey compromise compromise odyssey compromise odyssey odyssey odyssey odyssey odyssey odyssey compromise odyssey odyssey odyssey odyssey odyssey odyssey\n",
            "True Caption: the death of our lord jesus christ on the cross\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 0:\n",
            "Generated Caption: nationally purposely purposely purposely purposely unidentified purposely purposely purposely purposely unidentified purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely purposely\n",
            "True Caption: the washing colors feel real and makes me wonder how much work she has today\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "Generated Caption: bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush bush\n",
            "True Caption: i find the city street to be quite lovely. i like how the path seems to reflect in the light from above.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 2:\n",
            "Generated Caption: challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges challenges\n",
            "True Caption: this painting shows beautiful roses of every color.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 0:\n",
            "Generated Caption: valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves valves\n",
            "True Caption: i like that crayon is used here. it's different.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "Generated Caption: britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten britten\n",
            "True Caption: the woman's face looks very grumpy\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 2:\n",
            "Generated Caption: ##sinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsinsin\n",
            "True Caption: the color contrast of the face makes this look gross.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 0:\n",
            "Generated Caption: 266 266社 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266 266\n",
            "True Caption: this makes me feel curious about what it is trying to \" figure out \"..\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "Generated Caption: station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station station\n",
            "True Caption: this is a peaceful color yellow and the red blend /\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 2:\n",
            "Generated Caption: case case case case case case case case case case case case case case case case case case case case case case case case case case case case case case case\n",
            "True Caption: this painting makes me feel excitement. it looks like everyone involved is having a good time together.\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 0:\n",
            "Generated Caption: religious religious religious mechanically religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious religious\n",
            "True Caption: the people in the town are hustling and bustling in the town square\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "Generated Caption: tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa tokugawa\n",
            "True Caption: the waves look angery but christ and peter are fine which is reassuring and makes me feel content\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 2:\n",
            "Generated Caption: teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming teaming\n",
            "True Caption: its too abstract for me to really feel anything.\n",
            "\n",
            "---\n",
            "\n",
            "Validation Loss: 10.5048, Average BLEU Score: 0.0000\n",
            "Epoch 5: Train Loss = 10.5022, Val Loss = 10.5048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'transformer_beam_model.pth')"
      ],
      "metadata": {
        "id": "C_3gXFBZt4fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly the models are very bad. Since I don't have the resources/time to train a more robust model, I am thinking of using a custom tokenizer and vocabulary based on my dataset. Hopefully, this will make the predictions more relevant to my needs."
      ],
      "metadata": {
        "id": "3TFXQJKo1pGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Custom Tokenizer"
      ],
      "metadata": {
        "id": "Q4pS1XE116CV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating custom vocab and tokenizer"
      ],
      "metadata": {
        "id": "hjuhlXqr1_Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "\n",
        "# Create a tokenizer object with WordLevel model\n",
        "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Prepare trainer with special tokens and parameters\n",
        "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "# Train the tokenizer\n",
        "files = [\"drive/MyDrive/Applied_CV/texts.txt\"]  # path to your texts.txt file\n",
        "tokenizer.train(files, trainer)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save(\"custom_tokenizer.json\")"
      ],
      "metadata": {
        "id": "YofyfX6Q2Bn_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained tokenizer\n",
        "custom_tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n",
        "\n",
        "# Example of encoding text\n",
        "\n",
        "input = \"The mans completely black suit causes him to disappear into the night the man looks like a groomsman since he's wearing a suit and bow tie The scene is quite beautiful and reminds me of ancient fairy tales. There is a sense of romance between the two people here as they right the wolf in the woods. The woman appears quite concerned for the child and loving in her approach.\"\n",
        "\n",
        "\n",
        "output = custom_tokenizer.encode(input)\n",
        "print(output.ids)\n",
        "decoded_text = custom_tokenizer.decode(output.ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ4elPPM2Swe",
        "outputId": "c3915c61-f1e3-4d1f-9cad-689ffea61cc7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 379, 1950, 81, 908, 2450, 177, 13, 3537, 153, 5, 322, 5, 34, 17, 15, 8, 8360, 349, 54, 19, 28, 234, 8, 908, 7, 1271, 3200, 10, 59, 11, 135, 66, 7, 78, 18, 9, 985, 1287, 4000, 6, 147, 11, 8, 130, 9, 2641, 315, 5, 137, 45, 156, 40, 49, 231, 5, 5655, 14, 5, 704, 6, 10, 35, 117, 135, 1637, 42, 5, 133, 7, 1121, 14, 39, 2127, 6]\n",
            "The mans completely black suit causes him to disappear into the night the man looks like a groomsman since he ' s wearing a suit and bow tie The scene is quite beautiful and reminds me of ancient fairy tales . There is a sense of romance between the two people here as they right the wolf in the woods . The woman appears quite concerned for the child and loving in her approach .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating new dataset (where captions are tokenized using custom tokenizer)"
      ],
      "metadata": {
        "id": "c-_LMxAT2mbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = custom_tokenizer"
      ],
      "metadata": {
        "id": "Zd6hWzKK2yuF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, transform=None, max_length=32):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_len = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = get_image_path(self.df.iloc[idx]['art_style'], self.df.iloc[idx]['painting'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        caption = self.df.iloc[idx]['utterance']\n",
        "        inputs = self.tokenizer.encode(caption)\n",
        "        input_ids = torch.tensor(inputs.ids)\n",
        "        attention_mask = torch.tensor(inputs.attention_mask)\n",
        "\n",
        "        # Pad or truncate input_ids and attention_mask to max_length\n",
        "        pad_length = self.max_len - input_ids.size(0)\n",
        "        if pad_length > 0:\n",
        "            input_ids = torch.cat([input_ids, torch.zeros(pad_length, dtype=torch.long)], dim=0)\n",
        "            attention_mask = torch.cat([attention_mask, torch.zeros(pad_length, dtype=torch.long)], dim=0)\n",
        "        else:\n",
        "            input_ids = input_ids[:self.max_len]\n",
        "            attention_mask = attention_mask[:self.max_len]\n",
        "\n",
        "        # Shift the captions to the right to create targets\n",
        "        targets = input_ids.clone()\n",
        "        targets[:-1] = input_ids[1:]  # Shift input ids to the left\n",
        "        targets[-1] = 0  # Pad token at the end\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'ids': input_ids,\n",
        "            'mask': attention_mask,\n",
        "            'targets': targets\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = torch.stack([item['image'] for item in batch])\n",
        "    ids = torch.stack([item['ids'] for item in batch])\n",
        "    masks = torch.stack([item['mask'] for item in batch])\n",
        "    targets = torch.stack([item['targets'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'images': images,\n",
        "        'ids': ids,\n",
        "        'masks': masks,\n",
        "        'targets': targets\n",
        "    }"
      ],
      "metadata": {
        "id": "cNrORAPI22JL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the dataset and dataloader\n",
        "\n",
        "train_dataset = CaptionDataset(train_df, tokenizer, transform)\n",
        "val_dataset = CaptionDataset(val_df, tokenizer, transform)\n",
        "test_dataset = CaptionDataset(test_df, tokenizer, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "pWxVFdFR287K"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample_dataframe(df, fraction=0.1):\n",
        "    return df.sample(frac=fraction, random_state=42)"
      ],
      "metadata": {
        "id": "4Kf8MI-AMfF-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsample the DataFrames\n",
        "train_df_small = subsample_dataframe(train_df)\n",
        "val_df_small = subsample_dataframe(val_df)\n",
        "\n",
        "# Create datasets with the smaller dataframes\n",
        "train_dataset = CaptionDataset(train_df_small, tokenizer, transform)\n",
        "val_dataset = CaptionDataset(val_df_small, tokenizer, transform)"
      ],
      "metadata": {
        "id": "ULqHm940Mg5M"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "Fyc98eFtMns9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Model Class"
      ],
      "metadata": {
        "id": "msox-TY82-CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel_Custom(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, transformer_model=\"bert-base-uncased\", custom_tokenizer=None):\n",
        "    #def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, transformer_model=\"t5-small\", custom_tokenizer=None):\n",
        "\n",
        "        super(ImageCaptioningModel_Custom, self).__init__()\n",
        "\n",
        "        # load pretrained resnet model\n",
        "        self.resnet = resnet50(pretrained=True)\n",
        "\n",
        "        # freeze the resnet layers so that they are not trained\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # remove the classification layer and replace it with a linear layer\n",
        "        # this reduces the output dimension to match the embedding dimension\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_dim)\n",
        "\n",
        "        # transformer for generating captions\n",
        "        config = BertConfig.from_pretrained(transformer_model, bos_token_id=101, eos_token_id=102,\n",
        "                                            hidden_dropout_prob=0.5, attention_probs_dropout_prob=0.5)\n",
        "        self.transformer = BertModel(config)\n",
        "\n",
        "        self.embedding_transform = nn.Linear(embed_dim, self.transformer.config.hidden_size)\n",
        "        self.batch_norm = nn.BatchNorm1d(self.transformer.config.hidden_size)\n",
        "\n",
        "        # output layer\n",
        "        self.fc = nn.Linear(self.transformer.config.hidden_size, vocab_size)\n",
        "\n",
        "        self.custom_tokenizer = custom_tokenizer\n",
        "\n",
        "    def forward(self, images, input_ids=None, attention_mask=None):\n",
        "        #print(\"FORWARD\")\n",
        "        image_features = self.resnet(images)\n",
        "        image_features = self.embedding_transform(image_features)\n",
        "        image_features = self.batch_norm(image_features)\n",
        "\n",
        "        if input_ids is None:\n",
        "            #print(\"Calling beam_search_generate\")\n",
        "            return self.beam_search_generate(image_features, beam_width=5, max_length=12)\n",
        "        else:\n",
        "            image_features = image_features.unsqueeze(1).repeat(1, input_ids.size(1), 1)\n",
        "\n",
        "\n",
        "            transformer_out = self.transformer(inputs_embeds=image_features, attention_mask=attention_mask)[0]\n",
        "            outputs = self.fc(transformer_out)\n",
        "            return outputs\n",
        "\n",
        "    # def beam_search_generate(self, image_features, beam_width=3, max_length=28):\n",
        "    #     initial_input_ids = torch.full((image_features.size(0), 1), self.transformer.config.bos_token_id,\n",
        "    #                                   device=image_features.device, dtype=torch.long)\n",
        "    #     initial_scores = torch.zeros((image_features.size(0),), device=image_features.device)  # Initial scores are zero\n",
        "    #     candidates = [(initial_input_ids, initial_scores)]  # (token_ids, scores)\n",
        "\n",
        "    #     for _ in range(max_length - 1):\n",
        "    #         new_candidates = []\n",
        "    #         for token_ids, scores in candidates:\n",
        "    #             if token_ids[0, -1] == self.transformer.config.eos_token_id:\n",
        "    #                 new_candidates.append((token_ids, scores))\n",
        "    #                 continue\n",
        "    #             attention_mask = torch.ones((token_ids.size(0), token_ids.size(1)), device=image_features.device, dtype=torch.long)\n",
        "\n",
        "    #             image_features_expanded = image_features.unsqueeze(1).expand(-1, token_ids.size(1), -1)\n",
        "    #             transformer_out = self.transformer(inputs_embeds=image_features_expanded,\n",
        "    #                                               attention_mask=attention_mask)[0]\n",
        "    #             next_word_logits = self.fc(transformer_out[:, -1, :])\n",
        "    #             next_word_probs = F.softmax(next_word_logits / 0.2, dim=-1)  # Temperature scaling\n",
        "    #             top_k_probs, top_k_words = torch.topk(next_word_probs, beam_width, dim=1)\n",
        "\n",
        "    #             for i in range(beam_width):\n",
        "    #                 next_token_ids = torch.cat([token_ids, top_k_words[:, i:i+1]], dim=1)\n",
        "    #                 log_prob = torch.log(top_k_probs[:, i])\n",
        "    #                 next_scores = scores + log_prob\n",
        "    #                 new_candidates.append((next_token_ids, next_scores))\n",
        "\n",
        "    #         # Sort by scores and select top 'beam_width' candidates\n",
        "    #         candidates = sorted(new_candidates, key=lambda x: x[1].sum(), reverse=True)[:beam_width]\n",
        "\n",
        "    #         # Early stopping condition to avoid irrelevant continuations\n",
        "    #         if all(candidate[0][0, -1] == self.transformer.config.eos_token_id for candidate in candidates):\n",
        "    #             break\n",
        "\n",
        "    #     # Select the best candidate\n",
        "    #     best_candidate = max(candidates, key=lambda x: x[1].sum())\n",
        "    #     return best_candidate[0]\n",
        "\n",
        "    def beam_search_generate(self, image_features, beam_width=3, max_length=12):\n",
        "        initial_input_ids = torch.full((image_features.size(0), 1), self.transformer.config.bos_token_id,\n",
        "                                      device=image_features.device, dtype=torch.long)\n",
        "        initial_scores = torch.zeros((image_features.size(0),), device=image_features.device)\n",
        "        candidates = [(initial_input_ids, initial_scores)]\n",
        "\n",
        "        for step in range(max_length - 1):\n",
        "            new_candidates = []\n",
        "            for token_ids, scores in candidates:\n",
        "                if token_ids[0, -1] == self.transformer.config.eos_token_id:\n",
        "                    new_candidates.append((token_ids, scores))  # Preserve sequences that have ended\n",
        "                    continue\n",
        "\n",
        "                attention_mask = torch.ones((token_ids.size(0), token_ids.size(1)), device=image_features.device)\n",
        "                image_features_expanded = image_features.unsqueeze(1).expand(-1, token_ids.size(1), -1)\n",
        "                transformer_out = self.transformer(inputs_embeds=image_features_expanded, attention_mask=attention_mask)[0]\n",
        "                next_word_logits = self.fc(transformer_out[:, -1, :])\n",
        "                next_word_probs = F.softmax(next_word_logits, dim=-1)\n",
        "                top_k_probs, top_k_words = torch.topk(next_word_probs, beam_width, dim=1)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    next_token_ids = torch.cat([token_ids, top_k_words[:, i:i+1]], dim=1)\n",
        "                    log_prob = torch.log(top_k_probs[:, i])\n",
        "                    next_scores = scores + log_prob\n",
        "                    new_candidates.append((next_token_ids, next_scores))\n",
        "\n",
        "            candidates = sorted(new_candidates, key=lambda x: x[1].sum(), reverse=True)[:beam_width]\n",
        "\n",
        "            # Debugging: print intermediate candidates and their scores\n",
        "            #if step == max_length - 2:  # Last step\n",
        "                #for cand in candidates:\n",
        "                    #print(f\"Candidate tokens at final step: {cand[0].squeeze().tolist()}\")\n",
        "                    #print(f\"Candidate scores at final step: {cand[1]}\")\n",
        "\n",
        "        best_candidate = max(candidates, key=lambda x: x[1].sum())\n",
        "        best_tokens = best_candidate[0]  # No need to squeeze, keep batch dimension\n",
        "\n",
        "        # Debugging: Check dimensions before decoding\n",
        "        #print(\"Best tokens shape:\", best_tokens.shape)\n",
        "        #print(\"Best tokens:\", best_tokens.tolist())\n",
        "\n",
        "        # Decode each sequence in the batch individually\n",
        "        captions = []\n",
        "        for tokens in best_tokens:\n",
        "            try:\n",
        "                caption = self.custom_tokenizer.decode(tokens.tolist(), skip_special_tokens=True)\n",
        "                captions.append(caption)\n",
        "            except Exception as e:\n",
        "                captions.append(f\"Decoding failed: {str(e)}\")\n",
        "\n",
        "        return captions"
      ],
      "metadata": {
        "id": "UhBb3nsHnj-A"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temperature(logits, temperature=0.2):\n",
        "    return torch.softmax(logits / temperature, dim=-1)"
      ],
      "metadata": {
        "id": "fD9_7yK73kXv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Uz4vExc1wMK5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Parameters\n",
        "embed_dim = 256\n",
        "hidden_dim = 512\n",
        "vocab_size = custom_tokenizer.get_vocab_size()\n",
        "num_layers = 2"
      ],
      "metadata": {
        "id": "UxSSF8dD3R39"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ImageCaptioningModel_Custom(embed_dim, hidden_dim, vocab_size, num_layers, \"bert-base-uncased\", custom_tokenizer)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "h5FCLh57nvWq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, loss_fn, device, tokenizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        images = batch['images'].to(device)\n",
        "        input_ids = batch['ids'].to(device)\n",
        "        attention_mask = batch['masks'].to(device)\n",
        "        targets = batch['targets'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            # Apply softmax to convert logits to probabilities\n",
        "            probs = torch.softmax(outputs, dim=-1)\n",
        "            print(\"Shape of probs:\", probs.shape)\n",
        "\n",
        "            # Reshape to 2D [batch_size * sequence_length, vocab_size] for multinomial\n",
        "            probs_reshaped = probs.view(-1, probs.shape[-1])\n",
        "            print(\"Reshaped probs:\", probs_reshaped.shape)\n",
        "\n",
        "            # Sample from the probability distribution for each position in each sequence\n",
        "            sampled_indices = torch.multinomial(probs_reshaped, num_samples=1).squeeze(-1)\n",
        "\n",
        "            # Reshape back to [batch_size, sequence_length]\n",
        "            predictions = sampled_indices.view(probs.shape[0], probs.shape[1])\n",
        "\n",
        "            # Sample from the probability distribution for each sequence in the batch\n",
        "            print(f\"Batch {i}, Train Loss: {loss.item():.4f}\")\n",
        "            print(\"Input Text:\", tokenizer.decode(input_ids[0].cpu().detach().numpy()))\n",
        "            print(\"Predicted Text:\", tokenizer.decode(predictions[0].cpu().detach().numpy()))\n",
        "            print(\"Actual Text:\", tokenizer.decode(targets[0].cpu().detach().numpy()))\n",
        "            print(\"Current Learning Rate:\", optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# def validate(model, dataloader, loss_fn, device, tokenizer):\n",
        "#     model.eval()\n",
        "#     references = []\n",
        "#     hypotheses = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for batch_index, batch in enumerate(dataloader):\n",
        "#             images = batch['images'].to(device)\n",
        "#             targets = batch['targets'].to(device)\n",
        "\n",
        "#             # Generate captions using the model\n",
        "#             predicted_captions = model(images)\n",
        "\n",
        "#             # Convert targets to readable captions for comparison\n",
        "#             batch_references = [tokenizer.decode(t.tolist(), skip_special_tokens=True) for t in targets]\n",
        "\n",
        "#             # Ensure predicted captions are in string format for comparison and BLEU calculation\n",
        "#             batch_hypotheses = [tokenizer.decode(c.tolist(), skip_special_tokens=True) if isinstance(c, torch.Tensor) else c for c in predicted_captions]\n",
        "\n",
        "#             for idx in range(len(batch_references)):\n",
        "#                 references.append([batch_references[idx].split()])\n",
        "#                 hypotheses.append(batch_hypotheses[idx].split())\n",
        "\n",
        "#             if batch_index % 10 == 0:\n",
        "#                 for idx in range(min(3, len(batch_references))):  # Print up to 3 examples per batch\n",
        "#                     print(f\"Batch {batch_index}, Image {idx + 1}:\")\n",
        "#                     print(f\"  True Caption: {batch_references[idx]}\")\n",
        "#                     print(f\"  Predicted Caption: {batch_hypotheses[idx]}\")\n",
        "#                 print(\"\\n---\\n\")\n",
        "\n",
        "#     bleu_score = corpus_bleu(references, hypotheses)\n",
        "#     print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "#     return bleu_score\n",
        "\n",
        "def validate(model, dataloader, device, tokenizer):\n",
        "    model.eval()\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch in enumerate(dataloader):\n",
        "            images = batch['images'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "\n",
        "            predicted_captions = model(images)\n",
        "\n",
        "            # Convert targets to readable captions for comparison\n",
        "            batch_references = [tokenizer.decode(t.tolist(), skip_special_tokens=True) for t in targets]\n",
        "\n",
        "            # Ensure predicted captions are already in string format since they are decoded in beam_search_generate\n",
        "            batch_hypotheses = predicted_captions\n",
        "\n",
        "            for idx in range(len(batch_references)):\n",
        "                references.append([batch_references[idx].split()])\n",
        "                hypotheses.append(batch_hypotheses[idx].split())\n",
        "\n",
        "            if batch_index % 10 == 0:\n",
        "                for idx in range(min(3, len(batch_references))):  # Print up to 3 examples per batch\n",
        "                    print(f\"Batch {batch_index}, Image {idx + 1}:\")\n",
        "                    print(f\"  True Caption: {batch_references[idx]}\")\n",
        "                    print(f\"  Predicted Caption: {batch_hypotheses[idx]}\")\n",
        "                print(\"\\n---\\n\")\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    from nltk.translate.bleu_score import corpus_bleu\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "    return bleu_score\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, device, tokenizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device, tokenizer)\n",
        "        # Validation\n",
        "        bleu_score = validate(model, val_loader, device, custom_tokenizer)\n",
        "\n",
        "        # Scheduler Step (for adjusting learning rate based on validation loss)\n",
        "        #scheduler.step(val_loss)\n",
        "\n",
        "        # Print Epoch Summary\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, BLEU Score = {bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "dmPpRR6x2TxE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3, verbose=True)\n",
        "\n",
        "# Loss Function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "7bF8UUdevmhw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_validate(model, train_loader, val_loader, optimizer, loss_fn, device, custom_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhx6RZ2Z5aAp",
        "outputId": "d13bdce1-1362-44ac-ac09-60913b3fcac3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 0, Train Loss: 9.4448\n",
            "Input Text: The people look to be having fun with their open facial expressions .\n",
            "Predicted Text: blossom Feelings steed wagon lazy colourful childhood newly combination spins UFO unlike toppless astounding plumb murkey shells justice blotches south blossoming picture convinced breathing merry miniature outreaching christianity chicken arts surf fetus\n",
            "Actual Text: people look to be having fun with their open facial expressions .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 10, Train Loss: 6.7123\n",
            "Input Text: This painting is very relaxing to look at because you can see the love that this mother plays for her child .\n",
            "Predicted Text: blankets representations Roosevelt delicately pomp distort Washed japanese sadder battle absinthe suicide clarinet richly flavors weirid fangled folk DARK closing innocence theur tremendous harlequin llamas wasting gaunt approach pauses Bear elbow planting\n",
            "Actual Text: painting is very relaxing to look at because you can see the love that this mother plays for her child .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 20, Train Loss: 5.1094\n",
            "Input Text: It makes me think of horizontal in , over the window of an abandoned house\n",
            "Predicted Text: brain sidewalk ideal canceled onion verdant poles impressive Non marriage\n",
            "Actual Text: makes me think of horizontal in , over the window of an abandoned house\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 30, Train Loss: 4.9848\n",
            "Input Text: It makes me feel content because of the color pattern .\n",
            "Predicted Text: priority shy gore pastime madonna hit\n",
            "Actual Text: makes me feel content because of the color pattern .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 40, Train Loss: 4.7130\n",
            "Input Text: what is in the bag ? i am curious\n",
            "Predicted Text: air inside regularly childishly shoeless were mouth cruising seduction inspects daubed transferred dice\n",
            "Actual Text: is in the bag ? i am curious\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 50, Train Loss: 5.0568\n",
            "Input Text: Im not sure what kind of animal this is in black and white but it looks vicious .\n",
            "Predicted Text: anything precedes twinge borrowed POURING presidency cabins Wouldn samples creative\n",
            "Actual Text: not sure what kind of animal this is in black and white but it looks vicious .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 60, Train Loss: 4.8167\n",
            "Input Text: This picture evokes little to no emotion in me seems to be just a portrait\n",
            "Predicted Text: differing crawled mason 90s parrots dippled ghost figue idealized desperately lifting candelabra rays silk asshole properly\n",
            "Actual Text: picture evokes little to no emotion in me seems to be just a portrait\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 70, Train Loss: 4.5759\n",
            "Input Text: This school student just his classmate ' s buttocks together .\n",
            "Predicted Text: glory fingerless theyre entity exquisite seemed structures Finally dish Comical bed formally complacent riverbank the\n",
            "Actual Text: school student just his classmate ' s buttocks together .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 80, Train Loss: 4.4227\n",
            "Input Text: The picture is completely black which represent darkness to me .\n",
            "Predicted Text: existence wondering overdosed dinner the parcels agner get kiln unbelievable customers bountiful gathered batman morning testament the gathering sculpting rug\n",
            "Actual Text: picture is completely black which represent darkness to me .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 90, Train Loss: 4.3777\n",
            "Input Text: The woman ' s body has a friendly realistic feel to it and it draws the viewer towards her .\n",
            "Predicted Text: superiority schemes backwards memories Large thru Sly composition edges Muted Everyone unstable disconnected clowns yearn realizing uniquely\n",
            "Actual Text: woman ' s body has a friendly realistic feel to it and it draws the viewer towards her .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 100, Train Loss: 4.7442\n",
            "Input Text: The people milling around these broken ruins seem to be in need of help\n",
            "Predicted Text: spoiled balloons brightens randomly fur painters the sails definitely astonishment wasteland Leaving it amorphous precedes angel heartbeat\n",
            "Actual Text: people milling around these broken ruins seem to be in need of help\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 110, Train Loss: 4.8319\n",
            "Input Text: It ' s pleasing to see a larger woman , a more natural woman , but the reason for her nakedness makes me curious about her situation\n",
            "Predicted Text: colorization 3rd SAD nearness vegetables Weird haired outings uppity vilgar tis aged bra\n",
            "Actual Text: ' s pleasing to see a larger woman , a more natural woman , but the reason for her nakedness makes me curious about her situation\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 120, Train Loss: 4.2428\n",
            "Input Text: It looks run down and for like the house was\n",
            "Predicted Text: wouldn win fortress seeing called spanking dolls messanger quite cherry womans oarnge marker masterfully cauldron magician sentence tomb treeline adoration anticipatory from\n",
            "Actual Text: looks run down and for like the house was\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 130, Train Loss: 4.5490\n",
            "Input Text: It looks simple and clean and I appreciate that . The colors contrast each other really well , the red makes the white pop and the white makes the red pop ,\n",
            "Predicted Text: everwhere Should canopy sm suicide unimportant bubbly wanna a stage monster beam partially changed suddenly the\n",
            "Actual Text: looks simple and clean and I appreciate that . The colors contrast each other really well , the red makes the white pop and the white makes the red pop ,\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 140, Train Loss: 4.8984\n",
            "Input Text: The matador has just won against the bull as the arrow the bull ' s back and the man stands proudly .\n",
            "Predicted Text: of the CURIOUS burgundy edge fruitful david and level envelop stone tk olds approach glance cares fecal crossing sorcerer\n",
            "Actual Text: matador has just won against the bull as the arrow the bull ' s back and the man stands proudly .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 150, Train Loss: 4.5273\n",
            "Input Text: I really like the detail and technique of this painting\n",
            "Predicted Text: places desperately countenance spear steep cartoony rivers humid flat ones details BLACK contented execution Zeus lying emotionless beige . historic\n",
            "Actual Text: really like the detail and technique of this painting\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 160, Train Loss: 3.7702\n",
            "Input Text: It feels like I am falling from high above the earth because of the perspective .\n",
            "Predicted Text: 20 typically man Swathes interpreted hustling instrument viewer ghost drone variety amazingly Fellows listening ruffled hanged sacrifice enchantingly envelope\n",
            "Actual Text: feels like I am falling from high above the earth because of the perspective .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 170, Train Loss: 4.4740\n",
            "Input Text: The scene looks festive with the huge balloon bouquets towering over the crowd .\n",
            "Predicted Text: This . destruction droopyness the foggy partway job\n",
            "Actual Text: scene looks festive with the huge balloon bouquets towering over the crowd .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 180, Train Loss: 4.3073\n",
            "Input Text: The person is floating in the air under a dramatic moon .\n",
            "Predicted Text: Van religiously investigate elegantly illustrate the smelled seduction categorize will conquered inside What stores bucked the pile lurid hung .\n",
            "Actual Text: person is floating in the air under a dramatic moon .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 190, Train Loss: 4.3867\n",
            "Input Text: The woman seems ashamed of her displayed and uncertain of the judgment of her by viewers .\n",
            "Predicted Text: spaceship flares of like careful Washy someones mary flair from Uses Makes defence attire cared victory Simply tip Erotic\n",
            "Actual Text: woman seems ashamed of her displayed and uncertain of the judgment of her by viewers .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 200, Train Loss: 3.8652\n",
            "Input Text: The woman perched on the rocks seems as at home in the water as a siren - yet the rocks are slippery and a large wave is rolling in . The wave\n",
            "Predicted Text: peaches private silently flame especially thrill mastery rather owned sketch decomposing nurse highlights debonair direction canvas brows chillin After countryside Child\n",
            "Actual Text: woman perched on the rocks seems as at home in the water as a siren - yet the rocks are slippery and a large wave is rolling in . The wave\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 210, Train Loss: 4.3256\n",
            "Input Text: the yellow is so abrasive and awkward , also the breasts are tubular and awkward\n",
            "Predicted Text: relaxinf real Muted direction leaning kicked After depressive nearly folks fast\n",
            "Actual Text: yellow is so abrasive and awkward , also the breasts are tubular and awkward\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 220, Train Loss: 4.0566\n",
            "Input Text: I feel happy and content to see this painting of a beautiful .\n",
            "Predicted Text: . Reading sticking erupting fruits bored fabric aesthetic shooting mishapen it of options\n",
            "Actual Text: feel happy and content to see this painting of a beautiful .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 230, Train Loss: 3.8469\n",
            "Input Text: this is very much detailed and very clear\n",
            "Predicted Text: sat jazzy cheetah mansion kerchiefed contemporary confusing a musket awe they penetrate\n",
            "Actual Text: is very much detailed and very clear\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 240, Train Loss: 3.6453\n",
            "Input Text: The of black attire and top hats worn by the men seem to the few splatters of color .\n",
            "Predicted Text: oddness immediately substance cow resemble pleading inquire loved oppression were meets at Clear bad rearing hours coloured beige tiled reacting they\n",
            "Actual Text: of black attire and top hats worn by the men seem to the few splatters of color .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 250, Train Loss: 3.8167\n",
            "Input Text: The colors are soft and muted , there ' s no action , so it ' s a peaceful scene .\n",
            "Predicted Text: the scratchy mug scroll homemade enjoy sinking devices ca look intimating long but pad tumble downward wrinkly\n",
            "Actual Text: colors are soft and muted , there ' s no action , so it ' s a peaceful scene .\n",
            "Current Learning Rate: 1e-05\n",
            "Batch 0, Image 1:\n",
            "  True Caption: eyes looks evil and icy blue . Like they could look right through you .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 2:\n",
            "  True Caption: it looks a little bland the technique used is able to showcase the texture of the elements\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 3:\n",
            "  True Caption: . Just a colored leaf with yellow , blue & red\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "  True Caption: warm and earthy tones and values gives a western look .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 2:\n",
            "  True Caption: woman dressed in a red dress looking down\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 3:\n",
            "  True Caption: swirly clouds look like some freshly smoke .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "  True Caption: colored depiction of a country road and high trees with a lone person walking . The wind seems to move the trees . t\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 2:\n",
            "  True Caption: sky looks so light and bright in contrast to the forest .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 3:\n",
            "  True Caption: really like the fine details pictured here and the complex patterns make this piece interesting\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "  True Caption: like the bird in the background by the bed .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 2:\n",
            "  True Caption: man ' s expression looks a bit odd . I like his eyes but overall he looks like a or something .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 3:\n",
            "  True Caption: woman looks like she is scared as she is surrounded by three men .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "  True Caption: - The colors are dark and cold , the expressions are also dark and cold . Even the background is bleak .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 2:\n",
            "  True Caption: woman ' s and blank face are scary .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 3:\n",
            "  True Caption: looks like a very peaceful place to be with the light colors .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 50, Image 1:\n",
            "  True Caption: fisherman looks like he has just saved a female from the water\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 2:\n",
            "  True Caption: man is revealing how masculine his nude body really is\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 3:\n",
            "  True Caption: day at the beach with cloudy skies makes me feel like i am home in .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 60, Image 1:\n",
            "  True Caption: like the of the colors used and the of an average day .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 2:\n",
            "  True Caption: ' m confused . I love the greens and yellows in this . It reminds me of a dream of a kid ' s book , and it ' s\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 3:\n",
            "  True Caption: depiction of the mothers and children makes me feel happy .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 70, Image 1:\n",
            "  True Caption: painting reminds me of the start of winter ; kind of gray , but on the verge of becoming white and cold . It is beautiful\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 2:\n",
            "  True Caption: neon colors make a normal nature scene pop with personality .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 3:\n",
            "  True Caption: red headed lady doesn ' t like like she ' s having a good time . While she might be sleeping , she ' s got a face like someone\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 80, Image 1:\n",
            "  True Caption: woman looks to be in some kind of pain therefore making me sad\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 2:\n",
            "  True Caption: like it , it ' s different and a bit unsettling , lots of red and a huge skeleton\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 3:\n",
            "  True Caption: man in white is helping the man who is begging .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.0000\n",
            "Epoch 1: Train Loss = 4.6031, BLEU Score = 0.0000\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 0, Train Loss: 3.9965\n",
            "Input Text: Though formally posing , the man ' s thoughts seem to be far away , on a matter that is tormenting him .\n",
            "Predicted Text: dance carriage trying watering a starved 1930 . detail sprawling turtle actign postcard here\n",
            "Actual Text: formally posing , the man ' s thoughts seem to be far away , on a matter that is tormenting him .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 10, Train Loss: 4.1561\n",
            "Input Text: So pretty . So many trees . I love the colors .\n",
            "Predicted Text: or -- defenseless pale pilgrims destroying Vegas solo ornate is yet think italian sympathize indicated\n",
            "Actual Text: pretty . So many trees . I love the colors .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 20, Train Loss: 3.8224\n",
            "Input Text: looking young girl in a off the shoulder pink dress . Her face is plumb and questioning .\n",
            "Predicted Text: orangish . granting Having angelic the - dolls left evidence southwest scavenger\n",
            "Actual Text: looking young girl in a off the shoulder pink dress . Her face is plumb and questioning .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 30, Train Loss: 3.9341\n",
            "Input Text: The clear , blue mountains and sky reminds me of Christmas Eve\n",
            "Predicted Text: a ' cats explore lined dimensions Hounds is therefore , displayed baby puzzle crumbling\n",
            "Actual Text: clear , blue mountains and sky reminds me of Christmas Eve\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 40, Train Loss: 3.8948\n",
            "Input Text: Looks like ' s flying machine , a of his genius , even though it wouldn ' t have worked .\n",
            "Predicted Text: although about vein . curious impression safe all existant of reach a . place moon long the silhouettes Spain\n",
            "Actual Text: like ' s flying machine , a of his genius , even though it wouldn ' t have worked .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 50, Train Loss: 3.7972\n",
            "Input Text: This being looks and not of the human race .\n",
            "Predicted Text: at the flag load dark tell books marketplace wearing before blessed through evokes bordering sleeve Crucifixion smiiling Winter invoke\n",
            "Actual Text: being looks and not of the human race .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 60, Train Loss: 3.7532\n",
            "Input Text: The dark colors and lack of action don ' t make me really feel anything . Sure I ' d stop and look at this painting , but I wouldn ' t\n",
            "Predicted Text: unpleasing silhouettes spilling woman wake perfect poisoning Pan skinny calamitous of hats sketch glasses small the green\n",
            "Actual Text: dark colors and lack of action don ' t make me really feel anything . Sure I ' d stop and look at this painting , but I wouldn ' t\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 70, Train Loss: 3.5637\n",
            "Input Text: the creatures on the bottom look like they are going to try to someone\n",
            "Predicted Text: fit the appealing Doesn flowing transpiring the animal appears icy political feast Crowds leaves\n",
            "Actual Text: creatures on the bottom look like they are going to try to someone\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 80, Train Loss: 4.2011\n",
            "Input Text: The dark colors make me feel a sense of dread\n",
            "Predicted Text: . picture is sneaking approach edged bookplate with WISHING ..\" versions like relaxing in images another the It facing brows\n",
            "Actual Text: dark colors make me feel a sense of dread\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 90, Train Loss: 3.8339\n",
            "Input Text: It depicts a very hard and lonely life of a of the past .\n",
            "Predicted Text: decipher diamonds specifically way quality buildings incredibly and fortress a indifferent and a circus background pinkish\n",
            "Actual Text: depicts a very hard and lonely life of a of the past .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 100, Train Loss: 3.6582\n",
            "Input Text: The textures in this painting are comforting .\n",
            "Predicted Text: boys their pauses capture Dame Protection with posture peace firm mixture mercy\n",
            "Actual Text: textures in this painting are comforting .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 110, Train Loss: 3.5951\n",
            "Input Text: the water color layout is soothing\n",
            "Predicted Text: claimed group themselves auras makes good him ominous believes the how depicted , adored sports\n",
            "Actual Text: water color layout is soothing\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 120, Train Loss: 3.7181\n",
            "Input Text: I like the muted colors of this painting . The entire scene seems very serene .\n",
            "Predicted Text: creams the a the painting the , value wearing leaves sinister sharp the damage it gives Rose\n",
            "Actual Text: like the muted colors of this painting . The entire scene seems very serene .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 130, Train Loss: 4.0816\n",
            "Input Text: The pile of bodies seems gross and uncomfortable .\n",
            "Predicted Text: is maybe fairly likely did are Mouth mid Mixed brushwork breaking sheets s at\n",
            "Actual Text: pile of bodies seems gross and uncomfortable .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 140, Train Loss: 4.1419\n",
            "Input Text: The tiny details that make up the beautiful church in the foreground .\n",
            "Predicted Text: Vase Written equates us gift an peer offset fearful remarkably breath confidence blurry gentle\n",
            "Actual Text: tiny details that make up the beautiful church in the foreground .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 150, Train Loss: 3.6902\n",
            "Input Text: The two tiny figures on the shore beside the large ocean make me feel in awe of nature ' s vastness\n",
            "Predicted Text: ! cartoonist banana intriguing part the and . blooms desired weeds pass moon my town acknowledging lips\n",
            "Actual Text: two tiny figures on the shore beside the large ocean make me feel in awe of nature ' s vastness\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 160, Train Loss: 3.9901\n",
            "Input Text: It makes me feel contentment because the woman seems like she has a busy life but here she is pictured having a moment of rest .\n",
            "Predicted Text: infrared picture a the picture just to because is reasonable at great cub an beckon the and paintings\n",
            "Actual Text: makes me feel contentment because the woman seems like she has a busy life but here she is pictured having a moment of rest .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 170, Train Loss: 4.0437\n",
            "Input Text: It looks like the back of a person . Even though I know it is not a solid wall in front of him , The two columns on either side and\n",
            "Predicted Text: can Looks lively horizontal sm of broken rationale zombie Expression with so town anticipatory square ancient draws moth\n",
            "Actual Text: looks like the back of a person . Even though I know it is not a solid wall in front of him , The two columns on either side and\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 180, Train Loss: 3.6143\n",
            "Input Text: the simple grandeur of the hills\n",
            "Predicted Text: including angry diamonds sharp waterfall plain bent darkenss wondering yard bread someone makes that me foot There\n",
            "Actual Text: simple grandeur of the hills\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 190, Train Loss: 3.8028\n",
            "Input Text: The man looks like an important general out in his uniform\n",
            "Predicted Text: against leap textured normally is base and talent valuables looks the s . provoking , a to is\n",
            "Actual Text: man looks like an important general out in his uniform\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 200, Train Loss: 3.5463\n",
            "Input Text: The guy has a funny , sarcastic pouting like face .\n",
            "Predicted Text: of a the classic dreary element nicer way nicely country drooping tunnel bushes decaying nasty sunlight of detailed nude feeling\n",
            "Actual Text: guy has a funny , sarcastic pouting like face .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 210, Train Loss: 3.5370\n",
            "Input Text: The detail in all the people that are lined up by height looks like such a hard thing to do well and this artist pulled it off well .\n",
            "Predicted Text: taken strange strands the line whej differing . mercy and makeup . shadows\n",
            "Actual Text: detail in all the people that are lined up by height looks like such a hard thing to do well and this artist pulled it off well .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 220, Train Loss: 3.7394\n",
            "Input Text: Looks like a humble fishing ship , big enough just to go an relax out at sea on\n",
            "Predicted Text: mercy piercing bottom mourning pqainting drags awning isnt the hard like grey fall behind realistic puncture third can hat and holds\n",
            "Actual Text: like a humble fishing ship , big enough just to go an relax out at sea on\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 230, Train Loss: 3.4869\n",
            "Input Text: This guy seems so peaceful looking out at the mountains\n",
            "Predicted Text: and and confused ! in agressive uncertain life of crown a . , look a and deep and\n",
            "Actual Text: guy seems so peaceful looking out at the mountains\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 240, Train Loss: 3.7416\n",
            "Input Text: painting looks normal until you see the floating person which makes it quite confusing\n",
            "Predicted Text: a goal in unconventional a , man adorable look the and appreciated for man . puffed in a\n",
            "Actual Text: looks normal until you see the floating person which makes it quite confusing\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 250, Train Loss: 4.0981\n",
            "Input Text: it looks like he has an on his necklace\n",
            "Predicted Text: and . . The beginning painting neck fruit lonely of coat warm art . . really velvety mother\n",
            "Actual Text: looks like he has an on his necklace\n",
            "Current Learning Rate: 1e-05\n",
            "Batch 0, Image 1:\n",
            "  True Caption: eyes looks evil and icy blue . Like they could look right through you .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 2:\n",
            "  True Caption: it looks a little bland the technique used is able to showcase the texture of the elements\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 3:\n",
            "  True Caption: . Just a colored leaf with yellow , blue & red\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "  True Caption: warm and earthy tones and values gives a western look .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 2:\n",
            "  True Caption: woman dressed in a red dress looking down\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 3:\n",
            "  True Caption: swirly clouds look like some freshly smoke .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "  True Caption: colored depiction of a country road and high trees with a lone person walking . The wind seems to move the trees . t\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 2:\n",
            "  True Caption: sky looks so light and bright in contrast to the forest .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 3:\n",
            "  True Caption: really like the fine details pictured here and the complex patterns make this piece interesting\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "  True Caption: like the bird in the background by the bed .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 2:\n",
            "  True Caption: man ' s expression looks a bit odd . I like his eyes but overall he looks like a or something .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 3:\n",
            "  True Caption: woman looks like she is scared as she is surrounded by three men .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "  True Caption: - The colors are dark and cold , the expressions are also dark and cold . Even the background is bleak .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 2:\n",
            "  True Caption: woman ' s and blank face are scary .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 3:\n",
            "  True Caption: looks like a very peaceful place to be with the light colors .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 50, Image 1:\n",
            "  True Caption: fisherman looks like he has just saved a female from the water\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 2:\n",
            "  True Caption: man is revealing how masculine his nude body really is\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 3:\n",
            "  True Caption: day at the beach with cloudy skies makes me feel like i am home in .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 60, Image 1:\n",
            "  True Caption: like the of the colors used and the of an average day .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 2:\n",
            "  True Caption: ' m confused . I love the greens and yellows in this . It reminds me of a dream of a kid ' s book , and it ' s\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 3:\n",
            "  True Caption: depiction of the mothers and children makes me feel happy .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 70, Image 1:\n",
            "  True Caption: painting reminds me of the start of winter ; kind of gray , but on the verge of becoming white and cold . It is beautiful\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 2:\n",
            "  True Caption: neon colors make a normal nature scene pop with personality .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 3:\n",
            "  True Caption: red headed lady doesn ' t like like she ' s having a good time . While she might be sleeping , she ' s got a face like someone\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 80, Image 1:\n",
            "  True Caption: woman looks to be in some kind of pain therefore making me sad\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 2:\n",
            "  True Caption: like it , it ' s different and a bit unsettling , lots of red and a huge skeleton\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 3:\n",
            "  True Caption: man in white is helping the man who is begging .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "BLEU Score: 0.0000\n",
            "Epoch 2: Train Loss = 3.8279, BLEU Score = 0.0000\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 0, Train Loss: 3.7521\n",
            "Input Text: Feeling confused as there is a significant amount of activity going on . Is something supposed to be on fire ? Is the artist depicting a tree ? Having a hard time\n",
            "Predicted Text: prehistoric colors secret breaking bright wonderful me tension bowed white bones mans holding and real\n",
            "Actual Text: confused as there is a significant amount of activity going on . Is something supposed to be on fire ? Is the artist depicting a tree ? Having a hard time\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 10, Train Loss: 3.4124\n",
            "Input Text: The warmth of the light is very cozy even though the trees are bare .\n",
            "Predicted Text: the spreading balls shots beg the sounds evokes a ' life s of butter\n",
            "Actual Text: warmth of the light is very cozy even though the trees are bare .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 20, Train Loss: 3.6533\n",
            "Input Text: Lots of nice detail in the drawing , but the hanging remind me of all my medals .\n",
            "Predicted Text: cute before walks machines surrounding hero a for and coloring file her a on\n",
            "Actual Text: of nice detail in the drawing , but the hanging remind me of all my medals .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 30, Train Loss: 3.4117\n",
            "Input Text: It makes me feel content because of the color pattern .\n",
            "Predicted Text: the in whore chose piano of shape and ' a the thirds troubled\n",
            "Actual Text: makes me feel content because of the color pattern .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 40, Train Loss: 3.5605\n",
            "Input Text: well the head is kinda weird and creepy\n",
            "Predicted Text: skirt support once . somewhere some pleasing walkway s . the using Donkey grabbing golden scary lurking ' in\n",
            "Actual Text: the head is kinda weird and creepy\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 50, Train Loss: 3.4728\n",
            "Input Text: The beauty of the sailing ships and the magnificence of the town gives me peace .\n",
            "Predicted Text: you Very matador mid man behind am banks colors rich spatter he body the the and and\n",
            "Actual Text: beauty of the sailing ships and the magnificence of the town gives me peace .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 60, Train Loss: 4.0515\n",
            "Input Text: This looks like a dog carrying party plates , and hats to set a table . A fun party looks like it is in the works .\n",
            "Predicted Text: like feel to coast of to like to pink awed , the detail , look screams\n",
            "Actual Text: looks like a dog carrying party plates , and hats to set a table . A fun party looks like it is in the works .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 70, Train Loss: 3.5396\n",
            "Input Text: There are three things dancing together , but that ' s about all I can out .\n",
            "Predicted Text: of erotic is gigantic gala the on shoulders is showing relaxing arent strong characters and\n",
            "Actual Text: are three things dancing together , but that ' s about all I can out .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 80, Train Loss: 3.5972\n",
            "Input Text: The clear , blue mountains and sky reminds me of Christmas Eve\n",
            "Predicted Text: like trees memory Very most clouds is vivid ,\n",
            "Actual Text: clear , blue mountains and sky reminds me of Christmas Eve\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 90, Train Loss: 3.5072\n",
            "Input Text: these characters have silly facial dimensions which is absurd\n",
            "Predicted Text: I is to and though feel partially overly this yummy disproportionate portrays fields . makes artists\n",
            "Actual Text: characters have silly facial dimensions which is absurd\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 100, Train Loss: 3.2252\n",
            "Input Text: I am in awe of this painting , and its amazing detail .\n",
            "Predicted Text: looks flanked and .. relax look looks hurts wonder bloomed cheap of festivities dancer something gazes horses\n",
            "Actual Text: am in awe of this painting , and its amazing detail .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 110, Train Loss: 3.4707\n",
            "Input Text: I love how calm the entire scene feels , the small , cozy blue river almost at a stand - still , surrounded by the snowy landscape . It seems like a\n",
            "Predicted Text: iconic photo wall . canines other was like nudes the it person care on look\n",
            "Actual Text: love how calm the entire scene feels , the small , cozy blue river almost at a stand - still , surrounded by the snowy landscape . It seems like a\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 120, Train Loss: 3.8448\n",
            "Input Text: The detail in the picture reminds me of sppoky 1920\n",
            "Predicted Text: unsure windows blood intestines wearing a are interesting and looks in , plucking her Delaware me satisfaction\n",
            "Actual Text: detail in the picture reminds me of sppoky 1920\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 130, Train Loss: 3.2665\n",
            "Input Text: I don ' t understand what the meaning is . This does not tell me anything .\n",
            "Predicted Text: into loot though regretfully a tastes areas about clothing still a women , to\n",
            "Actual Text: don ' t understand what the meaning is . This does not tell me anything .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 140, Train Loss: 3.5131\n",
            "Input Text: this looks like a swamp and it me because of the dark edges\n",
            "Predicted Text: factory style babies red amusing . I has nobody rigged waves in like that of ,\n",
            "Actual Text: looks like a swamp and it me because of the dark edges\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 150, Train Loss: 3.3713\n",
            "Input Text: It is always a treat to see a traditional mirror image painting being re - created in a landscape of wilderness .\n",
            "Predicted Text: ferocious out , aquatic fallen color on moved of pensive , fan . love a are\n",
            "Actual Text: is always a treat to see a traditional mirror image painting being re - created in a landscape of wilderness .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 160, Train Loss: 3.2754\n",
            "Input Text: This brings about confusion\n",
            "Predicted Text: spaghetti ocean odd the scary . face so in makes snow to cannot like\n",
            "Actual Text: brings about confusion\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 170, Train Loss: 3.7000\n",
            "Input Text: This lady looks like she is relaxing in bed taking a nap after a very long , exhausting day . She has a very relaxed posture .\n",
            "Predicted Text: and life . a me bowl helps the like death still and the lines ancient I\n",
            "Actual Text: lady looks like she is relaxing in bed taking a nap after a very long , exhausting day . She has a very relaxed posture .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 180, Train Loss: 4.0216\n",
            "Input Text: Her stare is focused , as if something has her attention . It feels like something important is happening . The colors give me the vibe that it is something good .\n",
            "Predicted Text: approached time do of to looks preparing to I amazing like to overabundance black of\n",
            "Actual Text: stare is focused , as if something has her attention . It feels like something important is happening . The colors give me the vibe that it is something good .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 190, Train Loss: 3.7200\n",
            "Input Text: The looks so serious . His pose reminds me of a king or soldier with sword and . Instead , he is a bearing tools of his trade ,\n",
            "Predicted Text: me to peaceful my , blossoming penetrating of is of coming the guest is\n",
            "Actual Text: looks so serious . His pose reminds me of a king or soldier with sword and . Instead , he is a bearing tools of his trade ,\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 200, Train Loss: 3.8171\n",
            "Input Text: These make me interested in learning more about what they are depicting . There is a lot to look at , and a lot of detail .\n",
            "Predicted Text: nearby him . the child shading watching with fading clinging convey ' wearing sad understand with peeked feel boredom judgment in\n",
            "Actual Text: make me interested in learning more about what they are depicting . There is a lot to look at , and a lot of detail .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 210, Train Loss: 3.0973\n",
            "Input Text: Makes me feel shocked because her boobs are out . as well as worried about the two hands on the right side . And whatever the meaning is .\n",
            "Predicted Text: in on ' spared her weird look . . distracted red be\n",
            "Actual Text: me feel shocked because her boobs are out . as well as worried about the two hands on the right side . And whatever the meaning is .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 220, Train Loss: 3.8769\n",
            "Input Text: this picture has such great contrast , all the people are so pale , including the dog , it looks great against the dark background .\n",
            "Predicted Text: like dark of a colors me re air . in is a in suitor\n",
            "Actual Text: picture has such great contrast , all the people are so pale , including the dog , it looks great against the dark background .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 230, Train Loss: 3.6847\n",
            "Input Text: This is very boring and dreary with the dead - looking trees and cold - looking weather .\n",
            "Predicted Text: etc the reading is avoid face portrait seemingly thank is know murals doctor with of perhaps\n",
            "Actual Text: is very boring and dreary with the dead - looking trees and cold - looking weather .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 240, Train Loss: 3.6665\n",
            "Input Text: bored - this is too common of a picture and is overdone\n",
            "Predicted Text: bright I man playing a body young eyes like reminds uncooked quite not flat define\n",
            "Actual Text: - this is too common of a picture and is overdone\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 250, Train Loss: 3.1754\n",
            "Input Text: work here with lines and neon - looks to be abstract human forms\n",
            "Predicted Text: Great clothes . taken a in troubled t colors memories inspiration on running . activities handsome image She feel that jumping\n",
            "Actual Text: work here with lines and neon - looks to be abstract human forms\n",
            "Current Learning Rate: 1e-05\n",
            "Batch 0, Image 1:\n",
            "  True Caption: eyes looks evil and icy blue . Like they could look right through you .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 2:\n",
            "  True Caption: it looks a little bland the technique used is able to showcase the texture of the elements\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 3:\n",
            "  True Caption: . Just a colored leaf with yellow , blue & red\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "  True Caption: warm and earthy tones and values gives a western look .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 2:\n",
            "  True Caption: woman dressed in a red dress looking down\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 3:\n",
            "  True Caption: swirly clouds look like some freshly smoke .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "  True Caption: colored depiction of a country road and high trees with a lone person walking . The wind seems to move the trees . t\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 2:\n",
            "  True Caption: sky looks so light and bright in contrast to the forest .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 3:\n",
            "  True Caption: really like the fine details pictured here and the complex patterns make this piece interesting\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "  True Caption: like the bird in the background by the bed .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 2:\n",
            "  True Caption: man ' s expression looks a bit odd . I like his eyes but overall he looks like a or something .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 3:\n",
            "  True Caption: woman looks like she is scared as she is surrounded by three men .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "  True Caption: - The colors are dark and cold , the expressions are also dark and cold . Even the background is bleak .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 2:\n",
            "  True Caption: woman ' s and blank face are scary .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 3:\n",
            "  True Caption: looks like a very peaceful place to be with the light colors .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 50, Image 1:\n",
            "  True Caption: fisherman looks like he has just saved a female from the water\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 2:\n",
            "  True Caption: man is revealing how masculine his nude body really is\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 3:\n",
            "  True Caption: day at the beach with cloudy skies makes me feel like i am home in .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 60, Image 1:\n",
            "  True Caption: like the of the colors used and the of an average day .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 2:\n",
            "  True Caption: ' m confused . I love the greens and yellows in this . It reminds me of a dream of a kid ' s book , and it ' s\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 3:\n",
            "  True Caption: depiction of the mothers and children makes me feel happy .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 70, Image 1:\n",
            "  True Caption: painting reminds me of the start of winter ; kind of gray , but on the verge of becoming white and cold . It is beautiful\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 2:\n",
            "  True Caption: neon colors make a normal nature scene pop with personality .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 3:\n",
            "  True Caption: red headed lady doesn ' t like like she ' s having a good time . While she might be sleeping , she ' s got a face like someone\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 80, Image 1:\n",
            "  True Caption: woman looks to be in some kind of pain therefore making me sad\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 2:\n",
            "  True Caption: like it , it ' s different and a bit unsettling , lots of red and a huge skeleton\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 3:\n",
            "  True Caption: man in white is helping the man who is begging .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "BLEU Score: 0.0000\n",
            "Epoch 3: Train Loss = 3.6710, BLEU Score = 0.0000\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 0, Train Loss: 3.7702\n",
            "Input Text: The man is looking downward , as though he ' s ashamed\n",
            "Predicted Text: me the are grouping , in card I huddled stately , s . King likely played\n",
            "Actual Text: man is looking downward , as though he ' s ashamed\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 10, Train Loss: 3.8094\n",
            "Input Text: The woman reminds me of a witch without a broomstick\n",
            "Predicted Text: a flaxen sure hard dark . the of lives of the draw something ,\n",
            "Actual Text: woman reminds me of a witch without a broomstick\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 20, Train Loss: 3.4780\n",
            "Input Text: This painting feels heavenly with angles all around while a mother and baby are greeted and protected .\n",
            "Predicted Text: are recently ' the quite very secrets Libre blendy me detail seas very with colors\n",
            "Actual Text: painting feels heavenly with angles all around while a mother and baby are greeted and protected .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 30, Train Loss: 3.8088\n",
            "Input Text: It ' s a skillful colorful painting , depicting a cozy setting of luxury with the ornate desk and carpet\n",
            "Predicted Text: new way to the painting fire the and dark in village . us and a that s looks .\n",
            "Actual Text: ' s a skillful colorful painting , depicting a cozy setting of luxury with the ornate desk and carpet\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 40, Train Loss: 3.7127\n",
            "Input Text: The abstract and makes me think it is s prisoner .\n",
            "Predicted Text: , toned like his shapes cartoon at Note feel neglected accept and going\n",
            "Actual Text: abstract and makes me think it is s prisoner .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 50, Train Loss: 3.9261\n",
            "Input Text: I guess there ' s a melancholy aspect to the look of this woman ' s face . She is older , lacking color , and without a smile .\n",
            "Predicted Text: a . s a bores and drunk indecent one\n",
            "Actual Text: guess there ' s a melancholy aspect to the look of this woman ' s face . She is older , lacking color , and without a smile .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 60, Train Loss: 3.8190\n",
            "Input Text: The ' s expression of horror upon the crucified Christ .\n",
            "Predicted Text: like owl where change are the feels house in eggs like relaxing . giant kind . sheets\n",
            "Actual Text: ' s expression of horror upon the crucified Christ .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 70, Train Loss: 3.2169\n",
            "Input Text: These two look at peace within this sprawling pasture .\n",
            "Predicted Text: interest calm vacant man into tries in painting Tarnished makes a the is a warped looks even point\n",
            "Actual Text: two look at peace within this sprawling pasture .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 80, Train Loss: 3.8573\n",
            "Input Text: I cannot tell if this ghoulish figure is the sword from the man or not , but the darkness around its eyes make it look creepy .\n",
            "Predicted Text: sight the woman they a standing while I . like somebody woman . shortly an to\n",
            "Actual Text: cannot tell if this ghoulish figure is the sword from the man or not , but the darkness around its eyes make it look creepy .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 90, Train Loss: 3.4967\n",
            "Input Text: A very handsome a proud old man\n",
            "Predicted Text: looks Henry road dining the incredibly . to , the humble bit of that painting\n",
            "Actual Text: very handsome a proud old man\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 100, Train Loss: 3.8226\n",
            "Input Text: This looks like a fun time for these people on horseback riding by the lake .\n",
            "Predicted Text: be and wrong , circus frightening seems it hill masterpieces the painting very crying\n",
            "Actual Text: looks like a fun time for these people on horseback riding by the lake .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 110, Train Loss: 3.8148\n",
            "Input Text: The lady seems alone and that she lives a hard life .\n",
            "Predicted Text: farm setting shapes is eyes feel sister confusing are happily in . or black bittersweet robots\n",
            "Actual Text: lady seems alone and that she lives a hard life .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 120, Train Loss: 3.7870\n",
            "Input Text: I ' m guessing this is the prize after a bird hunt , and it ' s somewhat upsetting to the old . !\n",
            "Predicted Text: see happy to pop farm ' seen in expression boxes looks elements what . and is has the about in RATHER to\n",
            "Actual Text: ' m guessing this is the prize after a bird hunt , and it ' s somewhat upsetting to the old . !\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 130, Train Loss: 3.7084\n",
            "Input Text: the warm tones make the scene seem kind of calm and relaxed\n",
            "Predicted Text: in the interested trees water . viewer . his at which wonder or like lady dress combines . warm\n",
            "Actual Text: warm tones make the scene seem kind of calm and relaxed\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 140, Train Loss: 3.6500\n",
            "Input Text: The white looks like a giant swan attacking a nude lady\n",
            "Predicted Text: the indifferent . creepier frustrating dusty building something bright and sadness insemination might of faerie .\n",
            "Actual Text: white looks like a giant swan attacking a nude lady\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 150, Train Loss: 3.9405\n",
            "Input Text: This looks like a lovely family scene with happy children in a sunny courtyard .\n",
            "Predicted Text: , strong mischievous two that bright fingers think a Love in\n",
            "Actual Text: looks like a lovely family scene with happy children in a sunny courtyard .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 160, Train Loss: 3.3711\n",
            "Input Text: Have no clue what this means but the message is very gay and morbid .\n",
            "Predicted Text: flowers feel I Solitude insides because as me on pictured the s like sea why paintingf has is the s would\n",
            "Actual Text: no clue what this means but the message is very gay and morbid .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 170, Train Loss: 3.8694\n",
            "Input Text: I feel excited looking at this because I want to try to read what the text says . I feel like it reminds me of a music album .\n",
            "Predicted Text: them the nature for painting small , position man red . ' in\n",
            "Actual Text: feel excited looking at this because I want to try to read what the text says . I feel like it reminds me of a music album .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 180, Train Loss: 4.1421\n",
            "Input Text: A green apple in the table , makes me hungry for an apple\n",
            "Predicted Text: the this is seems make to ' would my her ship follow or as , who stick of ! the in\n",
            "Actual Text: green apple in the table , makes me hungry for an apple\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 190, Train Loss: 3.3521\n",
            "Input Text: This makes me relaxed and content . The curtain looks like it is blowing in the breeze and someone just had a drink while reading a book . Very relaxing .\n",
            "Predicted Text: the her colors looks ordaining discovery and to it It to would contented the quality of colors glossy scene\n",
            "Actual Text: makes me relaxed and content . The curtain looks like it is blowing in the breeze and someone just had a drink while reading a book . Very relaxing .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 200, Train Loss: 3.5207\n",
            "Input Text: The snow capped mountains are white and beautiful\n",
            "Predicted Text: contrast up violent together of or and made . taking , on has eyes wonderful a of painting\n",
            "Actual Text: snow capped mountains are white and beautiful\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 210, Train Loss: 3.4587\n",
            "Input Text: The image looks tasteless and mediocre , it lacks clarity .\n",
            "Predicted Text: red of ugly fond heart the it out no it look background center is\n",
            "Actual Text: image looks tasteless and mediocre , it lacks clarity .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 220, Train Loss: 3.9004\n",
            "Input Text: from the man ' s facial expression , he seems old and tired almost like he has seen and been through a lot in life .\n",
            "Predicted Text: shirt , and holding in he anything glass in a still next and of t the pencils\n",
            "Actual Text: from the man ' s facial expression , he seems old and tired almost like he has seen and been through a lot in life .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 230, Train Loss: 3.4595\n",
            "Input Text: Reminds me of or somewhere over near there .\n",
            "Predicted Text: color last like as conversation a browns mellow thing watermelon is a used\n",
            "Actual Text: me of or somewhere over near there .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 240, Train Loss: 3.7714\n",
            "Input Text: I am content looking at this peaceful scene , with people on the beach talking while a ship is in the still water .\n",
            "Predicted Text: this joyous disgusting ' seamstress s very clothing girl doing Proud like appears skies to\n",
            "Actual Text: am content looking at this peaceful scene , with people on the beach talking while a ship is in the still water .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 250, Train Loss: 3.7366\n",
            "Input Text: Beautiful peaceful drawing of a sleeping baby .\n",
            "Predicted Text: the like would accurate a . a can with in should on at painting bright intent\n",
            "Actual Text: peaceful drawing of a sleeping baby .\n",
            "Current Learning Rate: 1e-05\n",
            "Batch 0, Image 1:\n",
            "  True Caption: eyes looks evil and icy blue . Like they could look right through you .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 2:\n",
            "  True Caption: it looks a little bland the technique used is able to showcase the texture of the elements\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 3:\n",
            "  True Caption: . Just a colored leaf with yellow , blue & red\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "  True Caption: warm and earthy tones and values gives a western look .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 2:\n",
            "  True Caption: woman dressed in a red dress looking down\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 3:\n",
            "  True Caption: swirly clouds look like some freshly smoke .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "  True Caption: colored depiction of a country road and high trees with a lone person walking . The wind seems to move the trees . t\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 2:\n",
            "  True Caption: sky looks so light and bright in contrast to the forest .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 3:\n",
            "  True Caption: really like the fine details pictured here and the complex patterns make this piece interesting\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "  True Caption: like the bird in the background by the bed .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 2:\n",
            "  True Caption: man ' s expression looks a bit odd . I like his eyes but overall he looks like a or something .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 3:\n",
            "  True Caption: woman looks like she is scared as she is surrounded by three men .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "  True Caption: - The colors are dark and cold , the expressions are also dark and cold . Even the background is bleak .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 2:\n",
            "  True Caption: woman ' s and blank face are scary .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 3:\n",
            "  True Caption: looks like a very peaceful place to be with the light colors .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 50, Image 1:\n",
            "  True Caption: fisherman looks like he has just saved a female from the water\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 2:\n",
            "  True Caption: man is revealing how masculine his nude body really is\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 3:\n",
            "  True Caption: day at the beach with cloudy skies makes me feel like i am home in .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 60, Image 1:\n",
            "  True Caption: like the of the colors used and the of an average day .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 2:\n",
            "  True Caption: ' m confused . I love the greens and yellows in this . It reminds me of a dream of a kid ' s book , and it ' s\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 3:\n",
            "  True Caption: depiction of the mothers and children makes me feel happy .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 70, Image 1:\n",
            "  True Caption: painting reminds me of the start of winter ; kind of gray , but on the verge of becoming white and cold . It is beautiful\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 2:\n",
            "  True Caption: neon colors make a normal nature scene pop with personality .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 3:\n",
            "  True Caption: red headed lady doesn ' t like like she ' s having a good time . While she might be sleeping , she ' s got a face like someone\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 80, Image 1:\n",
            "  True Caption: woman looks to be in some kind of pain therefore making me sad\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 2:\n",
            "  True Caption: like it , it ' s different and a bit unsettling , lots of red and a huge skeleton\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 3:\n",
            "  True Caption: man in white is helping the man who is begging .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "BLEU Score: 0.0000\n",
            "Epoch 4: Train Loss = 3.6317, BLEU Score = 0.0000\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 0, Train Loss: 3.4213\n",
            "Input Text: The man looks imposing with his black clothes , long white beard , and cape .\n",
            "Predicted Text: long this - eyes happen blue occasionally one has neighborhood of don be top appealing overlapping snake it from\n",
            "Actual Text: man looks imposing with his black clothes , long white beard , and cape .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 10, Train Loss: 3.5742\n",
            "Input Text: The structure of the painting is interesting . The different actions of the men makes one really analyze the picture\n",
            "Predicted Text: feeling blind looks the a not well the bright their woman ' Makes\n",
            "Actual Text: structure of the painting is interesting . The different actions of the men makes one really analyze the picture\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 20, Train Loss: 3.8700\n",
            "Input Text: The woman wears her hair up beautifully , and wears colorful , lovely clothing .\n",
            "Predicted Text: gives and . , color what it realistic planning way done\n",
            "Actual Text: woman wears her hair up beautifully , and wears colorful , lovely clothing .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 30, Train Loss: 3.5167\n",
            "Input Text: beautiful ocean scene looks calming and relaxing\n",
            "Predicted Text: like fulfill angle the while far bright seems The inspiring sat a like like this this\n",
            "Actual Text: ocean scene looks calming and relaxing\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 40, Train Loss: 3.5869\n",
            "Input Text: The woman looks like she is thinking deeply .\n",
            "Predicted Text: anger it and ' . s , white thick . such calling seat and reaction protecting Sympathy dances\n",
            "Actual Text: woman looks like she is thinking deeply .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 50, Train Loss: 3.8132\n",
            "Input Text: The sun setting and the people walking slowly combined with the colors make this picture awe inspiring .\n",
            "Predicted Text: in boats from and looks the the this bit shower artist is and and between for mix\n",
            "Actual Text: sun setting and the people walking slowly combined with the colors make this picture awe inspiring .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 60, Train Loss: 3.6901\n",
            "Input Text: What this looks like to me is that they are either from a hunting , or the man behind the dog is welcoming the other man as they are\n",
            "Predicted Text: communicate ' the world appears the vegetation nude must in this pinnacles waterfalls looks\n",
            "Actual Text: this looks like to me is that they are either from a hunting , or the man behind the dog is welcoming the other man as they are\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 70, Train Loss: 3.7476\n",
            "Input Text: Wow ! the eyes are greatly detailed , and the lips red .\n",
            "Predicted Text: out for gentleman fire how d to shadow . It ice is .\n",
            "Actual Text: ! the eyes are greatly detailed , and the lips red .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 80, Train Loss: 3.9698\n",
            "Input Text: The combat scene places me on edge even though the color itself is soothing\n",
            "Predicted Text: of a . and evoke nudity seems depressed died of in ' ideas looks are viewpoint tigers in dressings\n",
            "Actual Text: combat scene places me on edge even though the color itself is soothing\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 90, Train Loss: 3.8966\n",
            "Input Text: The weary traveler is about to collapse when along comes someone to his aid , helping him in his journey .\n",
            "Predicted Text: face of looks of image content is a me because century uncomfortable\n",
            "Actual Text: weary traveler is about to collapse when along comes someone to his aid , helping him in his journey .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 100, Train Loss: 3.4118\n",
            "Input Text: The man looks kind of scary to me with his sad features .\n",
            "Predicted Text: cool However energy looks her with and bird clear of encouraging boats painting . nuts\n",
            "Actual Text: man looks kind of scary to me with his sad features .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 110, Train Loss: 3.1310\n",
            "Input Text: The blues and pinks are very delicate and gentle which mirrors the gentle facial expression .\n",
            "Predicted Text: huge and color on and has buying could red on black an a full branch suggests\n",
            "Actual Text: blues and pinks are very delicate and gentle which mirrors the gentle facial expression .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 120, Train Loss: 3.4519\n",
            "Input Text: The gentle ocean scene is made even more mellow with the beige and blues throughout the artwork\n",
            "Predicted Text: against sad cow to go memories crabs dry women together man the calmed story horror in choppy is as strokes that\n",
            "Actual Text: gentle ocean scene is made even more mellow with the beige and blues throughout the artwork\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 130, Train Loss: 3.6060\n",
            "Input Text: The deep reds show the fruits and juicy insides .\n",
            "Predicted Text: smiling me going to . scene terms the bed faces them like of angel their sight interesting\n",
            "Actual Text: deep reds show the fruits and juicy insides .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 140, Train Loss: 3.3083\n",
            "Input Text: Very realistic image of a man , the lines and the white beard makes the man come alive .\n",
            "Predicted Text: happy countryside own ! is very . the . am they of painting\n",
            "Actual Text: realistic image of a man , the lines and the white beard makes the man come alive .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 150, Train Loss: 3.2975\n",
            "Input Text: Majestic castle shows no color in the village that is full of life\n",
            "Predicted Text: peanut indicate wave ?). ' pool like color a of blouse this and very with in\n",
            "Actual Text: castle shows no color in the village that is full of life\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 160, Train Loss: 3.4334\n",
            "Input Text: the man on top of the well that holds the strings reminds me that we all are animal .\n",
            "Predicted Text: discordant . would enough lion than me work how is bigger its .\n",
            "Actual Text: man on top of the well that holds the strings reminds me that we all are animal .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 170, Train Loss: 3.7890\n",
            "Input Text: The colors in this painting are drab and elicit no emotion .\n",
            "Predicted Text: look 3D of of . boat , hazy be looking the . representing place a but confused painting . in\n",
            "Actual Text: colors in this painting are drab and elicit no emotion .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 180, Train Loss: 3.3104\n",
            "Input Text: into the distance at the endless field .\n",
            "Predicted Text: m considered she love with the her a colors with out of a lovely and sorry muscles pleasant , doesn\n",
            "Actual Text: into the distance at the endless field .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 190, Train Loss: 3.6328\n",
            "Input Text: The woman is nude , anticipating her lover ' s touch\n",
            "Predicted Text: tones bearded deformed a men lovely lives a of of but her The sign the someone\n",
            "Actual Text: woman is nude , anticipating her lover ' s touch\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 200, Train Loss: 4.0484\n",
            "Input Text: The way this man is bent over and pulling this rope feels like he is in a lot of pain and struggling .\n",
            "Predicted Text: done the s bow is . the simple , bright . could stalks makes\n",
            "Actual Text: way this man is bent over and pulling this rope feels like he is in a lot of pain and struggling .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 210, Train Loss: 3.6391\n",
            "Input Text: Tired . The horse looks tired . He is covered with spots that look like mud . He is eating or drinking to his energy . The colors of the painting\n",
            "Predicted Text: undesirable the of are . abduction of calm with contented flower of at his her plant\n",
            "Actual Text: . The horse looks tired . He is covered with spots that look like mud . He is eating or drinking to his energy . The colors of the painting\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 220, Train Loss: 4.1268\n",
            "Input Text: This painting makes me feel unsure about it , romantic but dark background .\n",
            "Predicted Text: green ' great strange strangling green . ' me at . bride\n",
            "Actual Text: painting makes me feel unsure about it , romantic but dark background .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 230, Train Loss: 3.7282\n",
            "Input Text: The person ' s eyes are bulging and her mouth is .\n",
            "Predicted Text: looks forrest of into viewer makes and way and s woman the sexy this girl\n",
            "Actual Text: person ' s eyes are bulging and her mouth is .\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 240, Train Loss: 3.4937\n",
            "Input Text: looks like some rocks in the , the sand looks beautiful\n",
            "Predicted Text: peaceful scenic is billowing summer the perfect like the calming . into ,\n",
            "Actual Text: like some rocks in the , the sand looks beautiful\n",
            "Current Learning Rate: 1e-05\n",
            "Shape of probs: torch.Size([32, 32, 11051])\n",
            "Reshaped probs: torch.Size([1024, 11051])\n",
            "Batch 250, Train Loss: 3.2661\n",
            "Input Text: This painting bores me . There aren ' t enough features to stimulate me .\n",
            "Predicted Text: purple boy small makes the woman . , over of sad . . is\n",
            "Actual Text: painting bores me . There aren ' t enough features to stimulate me .\n",
            "Current Learning Rate: 1e-05\n",
            "Batch 0, Image 1:\n",
            "  True Caption: eyes looks evil and icy blue . Like they could look right through you .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 2:\n",
            "  True Caption: it looks a little bland the technique used is able to showcase the texture of the elements\n",
            "  Predicted Caption: peaceful\n",
            "Batch 0, Image 3:\n",
            "  True Caption: . Just a colored leaf with yellow , blue & red\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 10, Image 1:\n",
            "  True Caption: warm and earthy tones and values gives a western look .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 2:\n",
            "  True Caption: woman dressed in a red dress looking down\n",
            "  Predicted Caption: peaceful\n",
            "Batch 10, Image 3:\n",
            "  True Caption: swirly clouds look like some freshly smoke .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 20, Image 1:\n",
            "  True Caption: colored depiction of a country road and high trees with a lone person walking . The wind seems to move the trees . t\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 2:\n",
            "  True Caption: sky looks so light and bright in contrast to the forest .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 20, Image 3:\n",
            "  True Caption: really like the fine details pictured here and the complex patterns make this piece interesting\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 30, Image 1:\n",
            "  True Caption: like the bird in the background by the bed .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 2:\n",
            "  True Caption: man ' s expression looks a bit odd . I like his eyes but overall he looks like a or something .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 30, Image 3:\n",
            "  True Caption: woman looks like she is scared as she is surrounded by three men .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 40, Image 1:\n",
            "  True Caption: - The colors are dark and cold , the expressions are also dark and cold . Even the background is bleak .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 2:\n",
            "  True Caption: woman ' s and blank face are scary .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 40, Image 3:\n",
            "  True Caption: looks like a very peaceful place to be with the light colors .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 50, Image 1:\n",
            "  True Caption: fisherman looks like he has just saved a female from the water\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 2:\n",
            "  True Caption: man is revealing how masculine his nude body really is\n",
            "  Predicted Caption: peaceful\n",
            "Batch 50, Image 3:\n",
            "  True Caption: day at the beach with cloudy skies makes me feel like i am home in .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 60, Image 1:\n",
            "  True Caption: like the of the colors used and the of an average day .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 2:\n",
            "  True Caption: ' m confused . I love the greens and yellows in this . It reminds me of a dream of a kid ' s book , and it ' s\n",
            "  Predicted Caption: peaceful\n",
            "Batch 60, Image 3:\n",
            "  True Caption: depiction of the mothers and children makes me feel happy .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 70, Image 1:\n",
            "  True Caption: painting reminds me of the start of winter ; kind of gray , but on the verge of becoming white and cold . It is beautiful\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 2:\n",
            "  True Caption: neon colors make a normal nature scene pop with personality .\n",
            "  Predicted Caption: peaceful\n",
            "Batch 70, Image 3:\n",
            "  True Caption: red headed lady doesn ' t like like she ' s having a good time . While she might be sleeping , she ' s got a face like someone\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "Batch 80, Image 1:\n",
            "  True Caption: woman looks to be in some kind of pain therefore making me sad\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 2:\n",
            "  True Caption: like it , it ' s different and a bit unsettling , lots of red and a huge skeleton\n",
            "  Predicted Caption: peaceful\n",
            "Batch 80, Image 3:\n",
            "  True Caption: man in white is helping the man who is begging .\n",
            "  Predicted Caption: peaceful\n",
            "\n",
            "---\n",
            "\n",
            "BLEU Score: 0.0000\n",
            "Epoch 5: Train Loss = 3.6150, BLEU Score = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'custom_beam_model.pth')"
      ],
      "metadata": {
        "id": "PFl_Pdvbvr5A"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}